// Vitest Snapshot v1, https://vitest.dev/guide/snapshot.html

exports[`template snapshots > AI templates > generateChatRoute 1`] = `
"import { convertToModelMessages, streamText, type UIMessage } from "ai";

export async function POST(req: Request) {
	const { messages }: { messages: UIMessage[] } = await req.json();

	const result = streamText({
		model: "openai/gpt-5",
		messages: await convertToModelMessages(messages),
		system: "You are a helpful assistant.",
	});

	return result.toUIMessageStreamResponse();
}
"
`;

exports[`template snapshots > Claude templates > generateDbMigrateSkill 1`] = `
"---
name: db-migrate
description: Handle Drizzle ORM migrations - generate, apply, and resolve merge conflicts. Use when schema changes, migrations fail, or conflicts occur during git merge.
allowed-tools: Bash(pnpm:*), Bash(git:*), Read, Grep
---

# Drizzle Migration Management

## Commands

Generate migration from schema changes:
\`\`\`bash
pnpm db:generate
\`\`\`

Apply migrations:
\`\`\`bash
pnpm db:migrate
\`\`\`

Open Drizzle Studio:
\`\`\`bash
pnpm db:studio
\`\`\`

Start PostgreSQL:
\`\`\`bash
pnpm docker:up
\`\`\`

## Key Files

- \`apps/web/db/schema.ts\` - Database schema definitions
- \`apps/web/db/index.ts\` - Database client
- \`apps/web/drizzle.config.ts\` - Drizzle configuration
- \`apps/web/drizzle/\` - Generated migration files

## Workflow

1. Modify schema in \`apps/web/db/schema.ts\`
2. Generate migration: \`pnpm db:generate\`
3. Review generated SQL in \`apps/web/drizzle/\`
4. Apply migration: \`pnpm db:migrate\`
5. Commit migration files

## Merge Conflict Resolution

When migrations conflict during git merge:

1. **Identify conflicts**: Check for conflicting files in \`apps/web/drizzle/\`
2. **Keep both migrations**: Don't merge migration content - keep as separate files
3. **Fix timestamps**: Ensure migration filenames have unique timestamps
4. **Regenerate metadata**: Run \`pnpm db:generate\` to update snapshot
5. **Test locally**: Apply migrations to dev database before committing
6. **Verify schema**: Compare schema.ts with actual database state

## Instructions

1. Ask what migration task is needed (generate, apply, or conflict resolution)
2. For conflicts, identify the conflicting files
3. Guide through resolution steps
4. Verify with \`pnpm db:generate\` after resolution
5. Test migrations work before committing

## Common Issues

### Schema out of sync
If database doesn't match code, introspect current state:
\`\`\`bash
pnpm db:studio
\`\`\`

### Migration already applied
Check migration status in \`drizzle/__drizzle_migrations\` table.

### Type errors after schema change
Run \`pnpm typecheck\` to find consumers that need updating.
"
`;

exports[`template snapshots > Claude templates > generateSettingsLocal 1`] = `
"{
	"permissions": {
		"disableBypassPermissionsMode": "disable",
		"allow": [
			"Bash(pnpm lint:*)",
			"Bash(ls:*)",
			"Bash(pnpm test:*)",
			"Bash(pnpm typecheck:*)",
			"Bash(cat:*)",
			"Bash(pnpm --filter web typecheck:*)",
			"Bash(pnpm --filter web lint:*)",
			"Bash(pnpm --filter web test)",
			"Bash(nvm use)",
			"Bash(pnpm db:generate:*)",
			"Bash(find:*)",
			"Bash(xargs cat:*)",
			"Bash(pnpm --filter web test:*)",
			"Bash(node -e:*)",
			"Bash(node --input-type=module -e:*)",
			"Bash(grep:*)",
			"Bash(wc:*)",
			"Bash(paste:*)",
			"Bash(sort:*)",
			"Bash(git status:*)",
			"Bash(git diff:*)",
			"Bash(git log:*)",
			"Bash(git branch:*)",
			"Bash(git show:*)",
			"WebSearch"
		],
		"deny": [],
		"ask": [
			"Bash(git commit:*)",
			"Bash(git push:*)",
			"Bash(git merge:*)",
			"Bash(git rebase:*)",
			"Bash(git reset:*)",
			"Bash(git stash:*)",
			"Bash(git cherry-pick:*)"
		]
	}
}"
`;

exports[`template snapshots > Claude templates > generateTestSkill 1`] = `
"---
name: test
description: Run Vitest tests in the monorepo. Use when running tests, debugging test failures, checking coverage, or before commits.
allowed-tools: Bash(pnpm:*), Read, Grep
---

# Vitest Testing

## Commands

Run all tests:
\`\`\`bash
pnpm test
\`\`\`

Run with coverage:
\`\`\`bash
pnpm test:coverage
\`\`\`

Run specific package:
\`\`\`bash
pnpm --filter web test
\`\`\`

Watch mode:
\`\`\`bash
pnpm --filter web test:watch
\`\`\`

Run specific test file:
\`\`\`bash
pnpm --filter web test -- --run path/to/test.test.ts
\`\`\`

Run tests by pattern:
\`\`\`bash
pnpm --filter web test -- --run --testNamePattern "pattern"
\`\`\`

## Instructions

1. Clarify which tests to run (all, specific package, pattern)
2. Run the appropriate command
3. Show results and any failure details
4. For failures, offer to help debug

## Test Database

For integration tests, ensure the test database is running:
\`\`\`bash
pnpm docker:up:test
\`\`\`

## Test Structure

- \`apps/web/__tests__/\` - Main test directory
- \`apps/web/__tests__/utils/\` - Test utilities (test-db.ts, mocks.ts)
- \`apps/web/__tests__/factories/\` - Data factories
- \`apps/web/vitest.config.mts\` - Vitest configuration
- \`apps/web/vitest.setup.ts\` - Test setup file
"
`;

exports[`template snapshots > Claude templates > generateTypecheckSkill 1`] = `
"---
name: typecheck
description: Run TypeScript type checking across the Turborepo monorepo. Use when checking for type errors, validating TypeScript code, or before committing changes.
allowed-tools: Bash(pnpm:*), Read, Grep
---

# TypeScript Type Checking

## Commands

Check all packages:
\`\`\`bash
pnpm typecheck
\`\`\`

Check specific package:
\`\`\`bash
pnpm --filter web typecheck
\`\`\`

## Instructions

1. Run \`pnpm typecheck\` to check all packages
2. If errors occur, show full error output with file locations
3. Group errors by package for clarity
4. Report total error count at the end

## Common Type Patterns

### Drizzle ORM Types
- Use \`typeof schema.tableName.$inferSelect\` for select types
- Use \`typeof schema.tableName.$inferInsert\` for insert types

### Next.js App Router
- Server Components are async by default
- Client Components must have \`"use client"\` directive
- Use \`use()\` hook in client components for promises passed from server

### Workspace Packages
- Internal packages use \`@repo/\` prefix
- Path aliases: \`@/*\` maps to app root in \`apps/web\`
"
`;

exports[`template snapshots > GitHub workflow templates > generateChecksWorkflow 1`] = `
"name: Checks

on:
  pull_request:
    branches: ["*"]

jobs:
  lint:
    runs-on: ubuntu-latest
    name: lint
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 22

      - uses: pnpm/action-setup@v4
        name: Install pnpm
        id: pnpm-install
        with:
          version: 10.4.1
          run_install: false

      - name: Get pnpm cache directory
        id: pnpm-cache
        run: |
          echo "pnpm_cache_dir=$(pnpm store path)" >> $GITHUB_OUTPUT
      - uses: actions/cache@v4
        name: Setup pnpm cache
        with:
          path: \${{ steps.pnpm-cache.outputs.pnpm_cache_dir }}
          key: \${{ runner.os }}-pnpm-store-\${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            \${{ runner.os }}-pnpm-store-
      - name: Install dependencies
        run: pnpm install

      - run: pnpm --filter web lint

  typecheck:
    runs-on: ubuntu-latest
    name: typecheck
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 22

      - uses: pnpm/action-setup@v4
        name: Install pnpm
        id: pnpm-install
        with:
          version: 10.4.1
          run_install: false

      - name: Get pnpm cache directory
        id: pnpm-cache
        run: |
          echo "pnpm_cache_dir=$(pnpm store path)" >> $GITHUB_OUTPUT
      - uses: actions/cache@v4
        name: Setup pnpm cache
        with:
          path: \${{ steps.pnpm-cache.outputs.pnpm_cache_dir }}
          key: \${{ runner.os }}-pnpm-store-\${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            \${{ runner.os }}-pnpm-store-
      - name: Install dependencies
        run: pnpm install

      - run: pnpm --filter web typecheck
"
`;

exports[`template snapshots > GitHub workflow templates > generateClaudeCodeReviewWorkflow 1`] = `
"name: Claude Code Review

on:
  pull_request:
    types: [opened, synchronize]
    # Optional: Only run on specific file changes
    # paths:
    #   - "src/**/*.ts"
    #   - "src/**/*.tsx"
    #   - "src/**/*.js"
    #   - "src/**/*.jsx"

jobs:
  claude-review:
    # Optional: Filter by PR author
    # if: |
    #   github.event.pull_request.user.login == 'external-contributor' ||
    #   github.event.pull_request.user.login == 'new-developer' ||
    #   github.event.pull_request.author_association == 'FIRST_TIME_CONTRIBUTOR'

    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude Code Review
        id: claude-review
        uses: anthropics/claude-code-action@v1
        with:
          claude_code_oauth_token: \${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}
          prompt: |
            REPO: \${{ github.repository }}
            PR NUMBER: \${{ github.event.pull_request.number }}

            Please review this pull request and provide feedback on:
            - Code quality and best practices
            - Potential bugs or issues
            - Performance considerations
            - Security concerns
            - Test coverage

            Use the repository's CLAUDE.md for guidance on style and conventions. Be constructive and helpful in your feedback.

            Use \\\`gh pr comment\\\` with your Bash tool to leave your review as a comment on the PR.

          # See https://github.com/anthropics/claude-code-action/blob/main/docs/usage.md
          # or https://docs.claude.com/en/docs/claude-code/sdk#command-line for available options
          claude_args: '--allowed-tools "Bash(gh issue view:*),Bash(gh search:*),Bash(gh issue list:*),Bash(gh pr comment:*),Bash(gh pr diff:*),Bash(gh pr view:*),Bash(gh pr list:*)"'
"
`;

exports[`template snapshots > GitHub workflow templates > generateClaudeWorkflow 1`] = `
"name: Claude Code

on:
  issue_comment:
    types: [created]
  pull_request_review_comment:
    types: [created]
  issues:
    types: [opened, assigned]
  pull_request_review:
    types: [submitted]

jobs:
  claude:
    if: |
      (github.event_name == 'issue_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review_comment' && contains(github.event.comment.body, '@claude')) ||
      (github.event_name == 'pull_request_review' && contains(github.event.review.body, '@claude')) ||
      (github.event_name == 'issues' && (contains(github.event.issue.body, '@claude') || contains(github.event.issue.title, '@claude')))
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: read
      issues: read
      id-token: write
      actions: read # Required for Claude to read CI results on PRs
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Run Claude Code
        id: claude
        uses: anthropics/claude-code-action@v1
        with:
          claude_code_oauth_token: \${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

          # This is an optional setting that allows Claude to read CI results on PRs
          additional_permissions: |
            actions: read

          # Optional: Give a custom prompt to Claude. If this is not specified, Claude will perform the instructions specified in the comment that tagged it.
          # prompt: 'Update the pull request description to include a summary of changes.'

          # Optional: Add claude_args to customize behavior and configuration
          # See https://github.com/anthropics/claude-code-action/blob/main/docs/usage.md
          # or https://docs.claude.com/en/docs/claude-code/sdk#command-line for available options
          # claude_args: '--model claude-opus-4-1-20250805 --allowed-tools Bash(gh pr:*)'
"
`;

exports[`template snapshots > GitHub workflow templates > generateTestWorkflow 1`] = `
"name: Test

on:
  pull_request:
    branches: ["*"]

jobs:
  test:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: pgvector/pgvector:pg18
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: undefined_test
        ports:
          - 5434:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v4

      - uses: pnpm/action-setup@v4
        name: Install pnpm
        id: pnpm-install
        with:
          version: 10.4.1
          run_install: false

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "22"
          cache: "pnpm"

      - name: Install dependencies
        run: pnpm install

      - name: Setup test database
        run: cd apps/web && DATABASE_URL=postgresql://postgres:postgres@localhost:5434/undefined_test pnpm db:push
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5434/undefined_test

      - name: Run tests
        run: pnpm --filter web test
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5434/undefined_test
          TEST_DATABASE_URL: postgresql://postgres:postgres@localhost:5434/undefined_test
"
`;

exports[`template snapshots > Supabase templates > generateSupabaseBranchScript 1`] = `
"#!/usr/bin/env bash
set -e

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
NC='\\033[0m' # No Color

usage() {
  echo "Supabase Branch Management"
  echo ""
  echo "Usage: ./scripts/supabase-branch <command> <name>"
  echo ""
  echo "Commands:"
  echo "  create <name>  - Create dev and test branches"
  echo "  delete <name>  - Delete dev and test branches"
  echo "  list           - List all branches"
  echo ""
  echo "Examples:"
  echo "  ./scripts/supabase-branch create feature-auth"
  echo "  ./scripts/supabase-branch delete feature-auth"
  echo "  ./scripts/supabase-branch list"
  exit 1
}

# Check for project ref
if [[ ! -f ".supabase/.project-ref" ]]; then
  echo -e "\${RED}Error: Supabase project not configured.\${NC}"
  echo "Run: pnpm supabase:setup"
  exit 1
fi

PROJECT_REF=$(cat .supabase/.project-ref)

[[ -z "$1" ]] && usage

case "$1" in
  create)
    [[ -z "$2" ]] && usage
    BRANCH_NAME="$2"
    TEST_BRANCH_NAME="\${2}-test"

    echo "Creating Supabase branches..."
    echo ""

    echo "  Creating '$BRANCH_NAME' branch..."
    if supabase branches create "$BRANCH_NAME" --persistent --project-ref "$PROJECT_REF"; then
      echo -e "  \${GREEN}✓ $BRANCH_NAME created\${NC}"
    else
      echo -e "  \${RED}✗ Failed to create $BRANCH_NAME\${NC}"
      exit 1
    fi

    echo "  Creating '$TEST_BRANCH_NAME' branch..."
    if supabase branches create "$TEST_BRANCH_NAME" --persistent --project-ref "$PROJECT_REF"; then
      echo -e "  \${GREEN}✓ $TEST_BRANCH_NAME created\${NC}"
    else
      echo -e "  \${RED}✗ Failed to create $TEST_BRANCH_NAME\${NC}"
      # Clean up the first branch
      supabase branches delete "$BRANCH_NAME" --project-ref "$PROJECT_REF" --confirm 2>/dev/null || true
      exit 1
    fi

    echo ""
    echo "Waiting for branches to be provisioned..."
    sleep 15

    echo ""
    echo "Applying migrations..."

    # Apply migrations to main branch
    echo "  Pushing to $BRANCH_NAME..."
    eval "$(supabase branches get "$BRANCH_NAME" --project-ref "$PROJECT_REF" -o env 2>/dev/null)" || true
    if [[ -n "$POSTGRES_URL" ]]; then
      DATABASE_URL="$POSTGRES_URL" pnpm --filter web db:push 2>/dev/null && echo -e "  \${GREEN}✓ Migrations applied\${NC}" || echo -e "  \${YELLOW}⚠ Migration failed\${NC}"
    fi

    # Apply migrations to test branch
    echo "  Pushing to $TEST_BRANCH_NAME..."
    eval "$(supabase branches get "$TEST_BRANCH_NAME" --project-ref "$PROJECT_REF" -o env 2>/dev/null)" || true
    if [[ -n "$POSTGRES_URL" ]]; then
      DATABASE_URL="$POSTGRES_URL" pnpm --filter web db:push 2>/dev/null && echo -e "  \${GREEN}✓ Migrations applied\${NC}" || echo -e "  \${YELLOW}⚠ Migration failed\${NC}"
    fi

    echo ""
    echo -e "\${GREEN}Branches created successfully!\${NC}"
    echo ""
    echo "To use these branches, run:"
    echo "  ./scripts/supabase-env $BRANCH_NAME"
    ;;

  delete)
    [[ -z "$2" ]] && usage
    BRANCH_NAME="$2"
    TEST_BRANCH_NAME="\${2}-test"

    echo "Deleting Supabase branches..."
    echo ""

    echo "  Deleting '$BRANCH_NAME'..."
    if supabase branches delete "$BRANCH_NAME" --project-ref "$PROJECT_REF" --confirm 2>/dev/null; then
      echo -e "  \${GREEN}✓ $BRANCH_NAME deleted\${NC}"
    else
      echo -e "  \${YELLOW}⚠ $BRANCH_NAME may not exist or failed to delete\${NC}"
    fi

    echo "  Deleting '$TEST_BRANCH_NAME'..."
    if supabase branches delete "$TEST_BRANCH_NAME" --project-ref "$PROJECT_REF" --confirm 2>/dev/null; then
      echo -e "  \${GREEN}✓ $TEST_BRANCH_NAME deleted\${NC}"
    else
      echo -e "  \${YELLOW}⚠ $TEST_BRANCH_NAME may not exist or failed to delete\${NC}"
    fi

    echo ""
    echo -e "\${GREEN}Branch cleanup complete!\${NC}"
    ;;

  list)
    echo "Supabase branches for project: $PROJECT_REF"
    echo ""
    supabase branches list --project-ref "$PROJECT_REF"
    ;;

  *)
    usage
    ;;
esac
"
`;

exports[`template snapshots > Supabase templates > generateSupabaseConfig 1`] = `
"# Supabase configuration
# See: https://supabase.com/docs/guides/cli/config

[api]
enabled = true
port = 54321
schemas = ["public", "graphql_public"]
extra_search_path = ["public", "extensions"]
max_rows = 1000

[db]
port = 54322
shadow_port = 54320
major_version = 17

[db.pooler]
enabled = true
port = 54329
pool_mode = "session"
default_pool_size = 20
max_client_conn = 100

[db.seed]
enabled = true
sql_paths = ["./seed.sql"]

[studio]
enabled = true
port = 54323
api_url = "http://127.0.0.1"

[auth]
enabled = true
site_url = "http://127.0.0.1:3000"

[storage]
enabled = true
file_size_limit = "50MiB"

# Enable pgvector extension for vector operations
# Extension is available in Supabase by default
"
`;

exports[`template snapshots > Supabase templates > generateSupabaseEnvScript 1`] = `
"#!/usr/bin/env bash
set -e

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
NC='\\033[0m' # No Color

BRANCH="\${1:-dev}"
ENV_FILE="apps/web/.env.local"

# Check for project ref
if [[ ! -f ".supabase/.project-ref" ]]; then
  echo -e "\${RED}Error: Supabase project not configured.\${NC}"
  echo "Run: pnpm supabase:setup"
  exit 1
fi

PROJECT_REF=$(cat .supabase/.project-ref)

echo "Fetching credentials for branch: $BRANCH"
echo ""

# Get main branch credentials
echo "  Fetching $BRANCH credentials..."
if ! eval "$(supabase branches get "$BRANCH" --project-ref "$PROJECT_REF" -o env 2>/dev/null)"; then
  echo -e "\${RED}Error: Could not fetch credentials for branch '$BRANCH'\${NC}"
  echo "Make sure the branch exists. List branches with:"
  echo "  ./scripts/supabase-branch list"
  exit 1
fi

if [[ -z "$POSTGRES_URL" ]]; then
  echo -e "\${RED}Error: No POSTGRES_URL returned for branch '$BRANCH'\${NC}"
  echo "The branch may still be provisioning. Try again in a minute."
  exit 1
fi

DB_URL="$POSTGRES_URL"
echo -e "  \${GREEN}✓ Got DATABASE_URL\${NC}"

# Get test branch credentials
TEST_BRANCH="\${BRANCH}-test"
echo "  Fetching $TEST_BRANCH credentials..."
if eval "$(supabase branches get "$TEST_BRANCH" --project-ref "$PROJECT_REF" -o env 2>/dev/null)"; then
  if [[ -n "$POSTGRES_URL" ]]; then
    TEST_DB_URL="$POSTGRES_URL"
    echo -e "  \${GREEN}✓ Got TEST_DATABASE_URL\${NC}"
  else
    echo -e "  \${YELLOW}⚠ No POSTGRES_URL for test branch\${NC}"
    TEST_DB_URL=""
  fi
else
  echo -e "  \${YELLOW}⚠ Test branch '$TEST_BRANCH' not found\${NC}"
  TEST_DB_URL=""
fi

# Ensure directory exists
mkdir -p "$(dirname "$ENV_FILE")"

# Update or create .env.local
echo ""
if [[ -f "$ENV_FILE" ]]; then
  echo "Updating $ENV_FILE..."

  # Update DATABASE_URL
  if grep -q "^DATABASE_URL=" "$ENV_FILE"; then
    # Use a different delimiter for sed since URLs contain slashes
    sed -i '' "s|^DATABASE_URL=.*|DATABASE_URL=\\"$DB_URL\\"|" "$ENV_FILE"
  else
    echo "DATABASE_URL=\\"$DB_URL\\"" >> "$ENV_FILE"
  fi

  # Update TEST_DATABASE_URL if we have one
  if [[ -n "$TEST_DB_URL" ]]; then
    if grep -q "^TEST_DATABASE_URL=" "$ENV_FILE"; then
      sed -i '' "s|^TEST_DATABASE_URL=.*|TEST_DATABASE_URL=\\"$TEST_DB_URL\\"|" "$ENV_FILE"
    else
      echo "TEST_DATABASE_URL=\\"$TEST_DB_URL\\"" >> "$ENV_FILE"
    fi
  fi
else
  echo "Creating $ENV_FILE..."
  echo "DATABASE_URL=\\"$DB_URL\\"" > "$ENV_FILE"
  if [[ -n "$TEST_DB_URL" ]]; then
    echo "TEST_DATABASE_URL=\\"$TEST_DB_URL\\"" >> "$ENV_FILE"
  fi
fi

echo ""
echo -e "\${GREEN}✓ Updated $ENV_FILE with '$BRANCH' branch credentials\${NC}"
echo ""
echo "You can now run: pnpm dev"
"
`;

exports[`template snapshots > Supabase templates > generateSupabaseSeedSql 1`] = `
"-- Seed data for development and testing (Better Auth schema)
-- This file is applied when branches are created

-- Insert test user
INSERT INTO "user" (id, email, name, created_at, updated_at)
VALUES (
  'test-user-id-123',
  'test@example.com',
  'Test User',
  NOW(),
  NOW()
) ON CONFLICT (email) DO NOTHING;

-- Insert test session (expires in 30 days)
INSERT INTO session (id, user_id, expires_at, created_at, updated_at)
SELECT
  'test-session-id-123',
  id,
  NOW() + INTERVAL '30 days',
  NOW(),
  NOW()
FROM "user"
WHERE email = 'test@example.com'
ON CONFLICT (id) DO NOTHING;
"
`;

exports[`template snapshots > Supabase templates > generateSupabaseSetupScript 1`] = `
"#!/usr/bin/env bash
set -e

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
NC='\\033[0m' # No Color

echo ""
echo "========================================"
echo "       Supabase Project Setup"
echo "========================================"
echo ""

# Check if supabase CLI is installed
if ! command -v supabase &> /dev/null; then
  echo -e "\${RED}Error: Supabase CLI is not installed.\${NC}"
  echo ""
  echo "Install it with:"
  echo "  brew install supabase/tap/supabase"
  echo ""
  echo "Or see: https://supabase.com/docs/guides/cli/getting-started"
  exit 1
fi

# Check if already linked
if [[ -f ".supabase/.project-ref" ]]; then
  existing_ref=$(cat .supabase/.project-ref)
  echo -e "\${YELLOW}Already linked to project: $existing_ref\${NC}"
  read -p "Re-link to a different project? (y/N) " relink
  if [[ "$relink" != [yY] ]]; then
    echo "Keeping existing configuration."
    exit 0
  fi
fi

echo "Prerequisites:"
echo "  1. Create a Supabase project at https://supabase.com/dashboard"
echo "  2. Enable branching in Project Settings > Branching (requires Pro plan)"
echo "  3. Copy the project reference ID from Project Settings > General"
echo ""

read -p "Enter your Supabase project reference: " PROJECT_REF

if [[ -z "$PROJECT_REF" ]]; then
  echo -e "\${RED}Error: Project reference is required.\${NC}"
  exit 1
fi

# Ensure .supabase directory exists
mkdir -p .supabase

# Link to project
echo ""
echo "Linking to Supabase project..."
supabase link --project-ref "$PROJECT_REF"

# Store project ref for other scripts
echo "$PROJECT_REF" > .supabase/.project-ref
echo -e "\${GREEN}Project reference saved to .supabase/.project-ref\${NC}"

# Check if branching is enabled
echo ""
echo "Checking branching status..."
if ! supabase branches list --project-ref "$PROJECT_REF" &> /dev/null; then
  echo -e "\${YELLOW}Warning: Branching may not be enabled for this project.\${NC}"
  echo "Enable it at: https://supabase.com/dashboard/project/$PROJECT_REF/settings/branching"
  echo ""
  read -p "Continue anyway? (y/N) " continue_setup
  if [[ "$continue_setup" != [yY] ]]; then
    exit 1
  fi
fi

# Create persistent development branches
echo ""
echo "Creating development branches..."

echo "  Creating 'dev' branch..."
if supabase branches create dev --persistent --project-ref "$PROJECT_REF" 2>/dev/null; then
  echo -e "  \${GREEN}✓ dev branch created\${NC}"
else
  echo -e "  \${YELLOW}⚠ dev branch may already exist\${NC}"
fi

echo "  Creating 'dev-test' branch..."
if supabase branches create dev-test --persistent --project-ref "$PROJECT_REF" 2>/dev/null; then
  echo -e "  \${GREEN}✓ dev-test branch created\${NC}"
else
  echo -e "  \${YELLOW}⚠ dev-test branch may already exist\${NC}"
fi

# Wait for branches to be ready
echo ""
echo "Waiting for branches to be provisioned (this may take 1-2 minutes)..."
sleep 10

# Apply migrations to dev branches
echo ""
echo "Applying migrations to development branches..."

echo "  Pushing to dev branch..."
eval "$(supabase branches get dev --project-ref "$PROJECT_REF" -o env 2>/dev/null)" || true
if [[ -n "$POSTGRES_URL" ]]; then
  DATABASE_URL="$POSTGRES_URL" pnpm --filter web db:push 2>/dev/null || echo -e "  \${YELLOW}⚠ Migration push to dev failed (branch may still be provisioning)\${NC}"
fi

echo "  Pushing to dev-test branch..."
eval "$(supabase branches get dev-test --project-ref "$PROJECT_REF" -o env 2>/dev/null)" || true
if [[ -n "$POSTGRES_URL" ]]; then
  DATABASE_URL="$POSTGRES_URL" pnpm --filter web db:push 2>/dev/null || echo -e "  \${YELLOW}⚠ Migration push to dev-test failed (branch may still be provisioning)\${NC}"
fi

# Fetch credentials for local development
echo ""
echo "Fetching credentials for local development..."
./scripts/supabase-env dev 2>/dev/null || echo -e "\${YELLOW}⚠ Could not fetch credentials yet. Run './scripts/supabase-env dev' later.\${NC}"

echo ""
echo "========================================"
echo -e "\${GREEN}       Setup Complete!\${NC}"
echo "========================================"
echo ""
echo "Next steps:"
echo ""
echo "  1. For local development:"
echo "     Your .env.local should now have DATABASE_URL configured."
echo "     Run: pnpm dev"
echo ""
echo "  2. For production (Vercel):"
echo "     Add DATABASE_URL to your Vercel environment variables."
echo "     Get it from: https://supabase.com/dashboard/project/$PROJECT_REF/settings/database"
echo "     Use the 'Transaction' pooler connection string for serverless."
echo ""
echo "  3. To create a new sandbox:"
echo "     Run: ./scripts/wts <branch-name>"
echo ""
"
`;

exports[`template snapshots > UI package templates > generateUIComponentsJson 1`] = `
"{
  "$schema": "https://ui.shadcn.com/schema.json",
  "style": "default",
  "rsc": true,
  "tsx": true,
  "tailwind": {
    "config": "",
    "css": "../../apps/web/app/globals.css",
    "baseColor": "slate",
    "cssVariables": true
  },
  "iconLibrary": "lucide",
  "aliases": {
    "components": "@workspace/ui/components",
    "utils": "@workspace/ui/lib/utils",
    "ui": "@workspace/ui/components",
    "hooks": "@workspace/ui/hooks",
    "lib": "@workspace/ui/lib"
  }
}
"
`;

exports[`template snapshots > UI package templates > generateUIIndex 1`] = `
"// UI components are exported via package.json exports field
// Import components like: import { Button } from "@workspace/ui/components/button"
export {};
"
`;

exports[`template snapshots > UI package templates > generateUILibUtils 1`] = `
"import { clsx, type ClassValue } from "clsx";
import { twMerge } from "tailwind-merge";

export function cn(...inputs: ClassValue[]) {
	return twMerge(clsx(inputs));
}
"
`;

exports[`template snapshots > UI package templates > generateUIPackageJson 1`] = `
"{
  "name": "@workspace/ui",
  "version": "0.0.1",
  "private": true,
  "exports": {
    "./lib/*": "./src/lib/*.ts",
    "./components/*": "./src/components/*.tsx",
    "./hooks/*": "./src/hooks/*.ts"
  },
  "scripts": {
    "lint": "biome check ."
  },
  "dependencies": {
    "@radix-ui/react-slot": "^1.2.4",
    "class-variance-authority": "^0.7.1",
    "clsx": "^2.1.1",
    "lucide-react": "^0.562.0",
    "tailwind-merge": "^3.4.0"
  },
  "devDependencies": {
    "@types/react": "^19.2.8",
    "@types/react-dom": "^19.2.3",
    "typescript": "^5.9.3"
  },
  "peerDependencies": {
    "react": "^19.0.0"
  }
}
"
`;

exports[`template snapshots > UI package templates > generateUITsconfig 1`] = `
"{
  "compilerOptions": {
    "target": "ES2017",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "baseUrl": ".",
    "paths": {
      "@workspace/ui/*": [
        "./src/*"
      ],
      "@/*": [
        "./src/*"
      ]
    }
  },
  "include": [
    "src/**/*"
  ],
  "exclude": [
    "node_modules"
  ]
}
"
`;

exports[`template snapshots > VS Code templates > generateVSCodeExtensions 1`] = `
"{
	"recommendations": [
		"biomejs.biome"
	]
}"
`;

exports[`template snapshots > VS Code templates > generateVSCodeSettings 1`] = `
"{
	"editor.defaultFormatter": "biomejs.biome",
	"editor.formatOnSave": true,
	"editor.tabSize": 2,
	"editor.insertSpaces": true,
	"eslint.enable": false,
	"prettier.enable": false,
	"typescript.tsdk": "node_modules/typescript/lib",
	"editor.codeActionsOnSave": {
		"source.organizeImports.biome": "explicit",
		"source.fixAll.biome": "explicit"
	},
	"biome.lsp.bin": "./node_modules/@biomejs/biome/bin/biome",
	"[javascript]": {
		"editor.defaultFormatter": "biomejs.biome"
	},
	"[typescript]": {
		"editor.defaultFormatter": "biomejs.biome"
	},
	"[typescriptreact]": {
		"editor.defaultFormatter": "biomejs.biome"
	},
	"[json]": {
		"editor.defaultFormatter": "biomejs.biome"
	},
	"[jsonc]": {
		"editor.defaultFormatter": "biomejs.biome"
	}
}"
`;

exports[`template snapshots > analytics templates > generatePostHogProvider 1`] = `
""use client";

import posthog from "posthog-js";
import { PostHogProvider as PHProvider } from "posthog-js/react";

// Initialize PostHog at module load (client-side only)
if (typeof window !== "undefined" && process.env.NEXT_PUBLIC_POSTHOG_KEY) {
	posthog.init(process.env.NEXT_PUBLIC_POSTHOG_KEY, {
		api_host: process.env.NEXT_PUBLIC_POSTHOG_HOST || "https://us.i.posthog.com",
		capture_pageview: "history_change",
		capture_exceptions: true,
	});
}

export function PostHogProvider({ children }: { children: React.ReactNode }) {
	return <PHProvider client={posthog}>{children}</PHProvider>;
}
"
`;

exports[`template snapshots > analytics templates > generatePostHogServer 1`] = `
"import { PostHog } from "posthog-node";

let posthogClient: PostHog | null = null;

function getPostHogClient(): PostHog | null {
	if (posthogClient) return posthogClient;

	const apiKey = process.env.POSTHOG_API_KEY;
	if (!apiKey) return null;

	posthogClient = new PostHog(apiKey, {
		host: process.env.NEXT_PUBLIC_POSTHOG_HOST || "https://us.i.posthog.com",
		flushAt: 1,
		flushInterval: 0,
	});

	return posthogClient;
}

export default getPostHogClient;

export async function trackServerEvent(
	distinctId: string,
	event: string,
	properties?: Record<string, unknown>,
) {
	const client = getPostHogClient();
	if (!client) return;

	client.capture({
		distinctId,
		event,
		properties,
	});
	await client.flush();
}
"
`;

exports[`template snapshots > app templates > generateAppLayout 1`] = `
"export default function AppLayout({
	children,
}: {
	children: React.ReactNode;
}) {
	return <div className="min-h-screen">{children}</div>;
}
"
`;

exports[`template snapshots > auth templates - Better Auth > generateAuthSkeleton 1`] = `
"export function AuthSkeleton() {
	return (
		<div className="flex min-h-screen items-center justify-center p-4 animate-pulse">
			<div className="w-full max-w-md border rounded-lg p-6 space-y-6">
				<div className="text-center space-y-2">
					<div className="h-7 w-24 bg-muted rounded mx-auto" />
					<div className="h-4 w-56 bg-muted rounded mx-auto" />
				</div>
				<div className="space-y-4">
					<div className="space-y-2">
						<div className="h-4 w-12 bg-muted rounded" />
						<div className="h-10 w-full bg-muted rounded" />
					</div>
					<div className="h-10 w-full bg-muted rounded" />
				</div>
			</div>
		</div>
	);
}
"
`;

exports[`template snapshots > auth templates - Better Auth > generateBetterAuthClient 1`] = `
"import { createAuthClient } from "better-auth/react";
import { emailOTPClient } from "better-auth/client/plugins";

export const authClient = createAuthClient({
	baseURL: process.env.NEXT_PUBLIC_APP_URL || "http://localhost:3000",
	plugins: [emailOTPClient()],
});

export const { signIn, signOut, useSession } = authClient;
"
`;

exports[`template snapshots > auth templates - Better Auth > generateBetterAuthConfig 1`] = `
"import { cache } from "react";
import { headers } from "next/headers";
import { betterAuth } from "better-auth";
import { emailOTP } from "better-auth/plugins";
import { drizzleAdapter } from "better-auth/adapters/drizzle";
import { db } from "@/db";
import { Resend } from "resend";

// Lazily create Resend client to avoid build-time initialization
let resendClient: Resend | null = null;
function getResend(): Resend {
	if (!resendClient) {
		resendClient = new Resend(process.env.RESEND_API_KEY);
	}
	return resendClient;
}

export const auth = betterAuth({
	database: drizzleAdapter(db, { provider: "pg" }),
	emailAndPassword: {
		enabled: false, // Using OTP only
	},
	plugins: [
		emailOTP({
			async sendVerificationOTP({ email, otp, type }) {
				// In development, also log to console
				if (process.env.NODE_ENV === "development") {
					console.log(\`[DEV] OTP for \${email}: \${otp} (type: \${type})\`);
				}

				// Send email via Resend
				try {
					await getResend().emails.send({
						from: "noreply@yourdomain.com", // Update with your domain
						to: email,
						subject: type === "sign-in"
							? "Your sign-in code"
							: type === "email-verification"
								? "Verify your email"
								: "Reset your password",
						html: \`
							<div style="font-family: sans-serif; max-width: 400px; margin: 0 auto;">
								<h2>Your verification code</h2>
								<p style="font-size: 32px; font-weight: bold; letter-spacing: 8px; text-align: center; padding: 20px; background: #f5f5f5; border-radius: 8px;">
									\${otp}
								</p>
								<p style="color: #666; font-size: 14px;">
									This code expires in 5 minutes. If you didn't request this, please ignore this email.
								</p>
							</div>
						\`,
					});
				} catch (error) {
					console.error("Failed to send OTP email:", error);
					// In development, don't throw so we can still use console OTP
					if (process.env.NODE_ENV !== "development") {
						throw error;
					}
				}
			},
			otpLength: 6,
			expiresIn: 300, // 5 minutes
		}),
	],
});

export type Session = typeof auth.$Infer.Session;

/**
 * Per-request cached session getter.
 * Uses React.cache() to deduplicate session fetches within the same request.
 * Multiple calls to getSession() in the same request will only fetch once.
 */
export const getSession = cache(async () => {
	return auth.api.getSession({
		headers: await headers(),
	});
});
"
`;

exports[`template snapshots > auth templates - Better Auth > generateBetterAuthProxy 1`] = `
"import { NextResponse } from "next/server";
import type { NextRequest } from "next/server";

export async function proxy(request: NextRequest) {
	// Get session from cookie
	const sessionCookie = request.cookies.get("better-auth.session_token");
	const hasSession = !!sessionCookie?.value;

	const { pathname } = request.nextUrl;

	// Protect dashboard routes
	if (pathname.startsWith("/dashboard")) {
		if (!hasSession) {
			return NextResponse.redirect(new URL("/login", request.url));
		}
	}

	// Redirect authenticated users away from auth pages
	if (hasSession && (pathname === "/login" || pathname === "/verify-otp")) {
		return NextResponse.redirect(new URL("/dashboard", request.url));
	}

	return NextResponse.next();
}

export const config = {
	matcher: [
		"/dashboard/:path*",
		"/login",
		"/verify-otp",
		// Exclude workflow endpoints
		"/((?!.well-known/workflow|api/workflow).*)",
	],
};
"
`;

exports[`template snapshots > auth templates - Better Auth > generateBetterAuthRouteHandler 1`] = `
"import { toNextJsHandler } from "better-auth/next-js";
import { auth } from "@/lib/auth";

export const { POST, GET } = toNextJsHandler(auth);
"
`;

exports[`template snapshots > auth templates - Better Auth > generateLoginPage 1`] = `
""use client";

import { useState, startTransition } from "react";
import { useRouter } from "next/navigation";
import { authClient } from "@/lib/auth-client";
import { Button } from "@workspace/ui/components/button";
import { Input } from "@workspace/ui/components/input";
import { Label } from "@workspace/ui/components/label";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@workspace/ui/components/card";

export default function LoginPage() {
	const router = useRouter();
	// Lazy initialization - restore email if user navigated back
	const [email, setEmail] = useState(() =>
		typeof window !== "undefined"
			? sessionStorage.getItem("pendingEmail") || ""
			: ""
	);
	const [loading, setLoading] = useState(false);
	const [error, setError] = useState("");

	const handleSendOTP = async (e: React.FormEvent) => {
		e.preventDefault();
		setLoading(true);
		setError("");

		try {
			const { error } = await authClient.emailOtp.sendVerificationOtp({
				email,
				type: "sign-in",
			});

			if (error) {
				setError(error.message || "Failed to send code");
				return;
			}

			// Store email for verification page
			sessionStorage.setItem("pendingEmail", email);
			router.push("/verify-otp");
		} catch {
			setError("An unexpected error occurred");
		} finally {
			setLoading(false);
		}
	};

	const handleEmailChange = (e: React.ChangeEvent<HTMLInputElement>) => {
		// Use transition for non-urgent input updates
		startTransition(() => {
			setEmail(e.target.value);
		});
	};

	return (
		<div className="flex min-h-screen items-center justify-center p-4">
			<Card className="w-full max-w-md">
				<CardHeader className="text-center">
					<CardTitle className="text-2xl">Sign In</CardTitle>
					<CardDescription>
						Enter your email to receive a one-time code
					</CardDescription>
				</CardHeader>
				<CardContent>
					<form onSubmit={handleSendOTP} className="space-y-4">
						<div className="space-y-2">
							<Label htmlFor="email">Email</Label>
							<Input
								id="email"
								type="email"
								value={email}
								onChange={handleEmailChange}
								required
								placeholder="you@example.com"
							/>
						</div>

						{error ? (
							<p className="text-sm text-destructive" role="alert">{error}</p>
						) : null}

						<Button type="submit" className="w-full" disabled={loading}>
							{loading ? "Sending..." : "Send Code"}
						</Button>
					</form>
				</CardContent>
			</Card>
		</div>
	);
}
"
`;

exports[`template snapshots > auth templates - Better Auth > generateVerifyOTPPage 1`] = `
""use client";

import { useState, useEffect, startTransition } from "react";
import { useRouter } from "next/navigation";
import { authClient } from "@/lib/auth-client";
import { Button } from "@workspace/ui/components/button";
import { Input } from "@workspace/ui/components/input";
import { Label } from "@workspace/ui/components/label";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@workspace/ui/components/card";

export default function VerifyOTPPage() {
	const router = useRouter();
	const [otp, setOtp] = useState("");
	// Lazy initialization - read email from sessionStorage immediately
	const [email] = useState(() =>
		typeof window !== "undefined"
			? sessionStorage.getItem("pendingEmail") || ""
			: ""
	);
	const [loading, setLoading] = useState(false);
	const [error, setError] = useState("");

	// Redirect if no pending email (after hydration)
	useEffect(() => {
		if (!email) {
			router.push("/login");
		}
	}, [email, router]);

	const handleVerify = async (e: React.FormEvent) => {
		e.preventDefault();
		setLoading(true);
		setError("");

		try {
			const { error } = await authClient.signIn.emailOtp({
				email,
				otp,
			});

			if (error) {
				setError(error.message || "Invalid code");
				return;
			}

			sessionStorage.removeItem("pendingEmail");
			router.push("/dashboard");
		} catch {
			setError("An unexpected error occurred");
		} finally {
			setLoading(false);
		}
	};

	const handleResend = async () => {
		setLoading(true);
		setError("");

		try {
			const { error } = await authClient.emailOtp.sendVerificationOtp({
				email,
				type: "sign-in",
			});

			if (error) {
				setError(error.message || "Failed to resend code");
			}
		} catch {
			setError("Failed to resend code");
		} finally {
			setLoading(false);
		}
	};

	const handleOtpChange = (e: React.ChangeEvent<HTMLInputElement>) => {
		// Use transition for non-urgent input updates
		startTransition(() => {
			setOtp(e.target.value);
		});
	};

	// Don't render form if no email (will redirect)
	if (!email) {
		return null;
	}

	return (
		<div className="flex min-h-screen items-center justify-center p-4">
			<Card className="w-full max-w-md">
				<CardHeader className="text-center">
					<CardTitle className="text-2xl">Enter Code</CardTitle>
					<CardDescription>
						We sent a code to {email}
					</CardDescription>
				</CardHeader>
				<CardContent>
					<form onSubmit={handleVerify} className="space-y-4">
						<div className="space-y-2">
							<Label htmlFor="otp">Verification Code</Label>
							<Input
								id="otp"
								type="text"
								value={otp}
								onChange={handleOtpChange}
								required
								maxLength={6}
								className="text-center text-2xl tracking-widest"
								placeholder="000000"
							/>
						</div>

						{error ? (
							<p className="text-sm text-destructive" role="alert">{error}</p>
						) : null}

						<Button type="submit" className="w-full" disabled={loading}>
							{loading ? "Verifying..." : "Verify"}
						</Button>

						<button
							type="button"
							onClick={handleResend}
							className="w-full text-sm text-muted-foreground hover:underline"
							disabled={loading}
						>
							Resend code
						</button>
					</form>
				</CardContent>
			</Card>
		</div>
	);
}
"
`;

exports[`template snapshots > auth templates - WorkOS > generateWorkOSCallback 1`] = `
"import { handleAuth } from "@workos-inc/authkit-nextjs";

export const GET = handleAuth();
"
`;

exports[`template snapshots > auth templates - WorkOS > generateWorkOSLoginPage 1`] = `
"import { getSignInUrl } from "@workos-inc/authkit-nextjs";
import { redirect } from "next/navigation";
import { Button } from "@workspace/ui/components/button";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@workspace/ui/components/card";

export default function LoginPage() {
	return (
		<div className="flex min-h-screen items-center justify-center p-4">
			<Card className="w-full max-w-md">
				<CardHeader className="text-center">
					<CardTitle className="text-2xl">Sign In</CardTitle>
					<CardDescription>
						Sign in to access your account
					</CardDescription>
				</CardHeader>
				<CardContent>
					<form
						action={async () => {
							"use server";
							const signInUrl = await getSignInUrl();
							redirect(signInUrl);
						}}
					>
						<Button type="submit" className="w-full">
							Sign In with WorkOS
						</Button>
					</form>
				</CardContent>
			</Card>
		</div>
	);
}
"
`;

exports[`template snapshots > auth templates - WorkOS > generateWorkOSProxy 1`] = `
"import { authkitMiddleware } from "@workos-inc/authkit-nextjs";

export default authkitMiddleware();

export const config = {
	matcher: [
		"/dashboard/:path*",
		// Exclude workflow endpoints from auth
		"/((?!.well-known/workflow|api/workflow|callback).*)",
	],
};
"
`;

exports[`template snapshots > dashboard templates > generateAITriggerButton 1`] = `
""use client";

import { useState, useCallback, startTransition } from "react";
import { Button } from "@workspace/ui/components/button";
import { Input } from "@workspace/ui/components/input";
import { Label } from "@workspace/ui/components/label";
import { useWorkflowProgress } from "@/hooks/use-workflow-progress";
import { useLatest } from "@/hooks/use-latest";

export function AITriggerButton() {
	const [prompt, setPrompt] = useState("What are 3 interesting facts about TypeScript?");
	const [isPending, setIsPending] = useState(false);
	const { state, startWithRunId, setError, reset } = useWorkflowProgress();
	const promptRef = useLatest(prompt);

	const handleTrigger = useCallback(async () => {
		reset();
		setIsPending(true);

		try {
			const response = await fetch("/api/workflow", {
				method: "POST",
				headers: { "Content-Type": "application/json" },
				body: JSON.stringify({ prompt: promptRef.current }),
			});

			const data = await response.json();

			if (!response.ok) {
				setError(data.error || "Failed to start workflow");
				return;
			}

			startWithRunId(data.runId);
		} catch (error) {
			setError(error instanceof Error ? error.message : "Failed to start workflow");
		} finally {
			setIsPending(false);
		}
	}, [promptRef, reset, setError, startWithRunId]);

	const handlePromptChange = useCallback((e: React.ChangeEvent<HTMLInputElement>) => {
		startTransition(() => {
			setPrompt(e.target.value);
		});
	}, []);

	const isRunning = isPending || state.status === "generating";

	return (
		<div className="space-y-4">
			<div className="space-y-2">
				<Label htmlFor="prompt">Prompt</Label>
				<Input
					id="prompt"
					value={prompt}
					onChange={handlePromptChange}
					placeholder="Enter a prompt for the AI agent..."
					disabled={isRunning}
				/>
			</div>

			<Button onClick={handleTrigger} disabled={isRunning || !prompt}>
				{isRunning ? "Running..." : "Trigger AI Workflow"}
			</Button>

			{/* Progress indicator */}
			{state.status === "generating" && (
				<div className="p-4 bg-blue-50 text-blue-800 rounded border border-blue-200 dark:bg-blue-950 dark:text-blue-200 dark:border-blue-800">
					<div className="font-medium mb-1">Workflow started! Run ID: {state.runId}</div>
					<div className="flex items-center justify-between mb-2">
						<span className="text-sm">Processing</span>
						<span className="text-sm">
							Step {state.step} of {state.totalSteps}
						</span>
					</div>
					<div className="w-full bg-blue-200 dark:bg-blue-800 rounded-full h-2 mb-2">
						<div
							className="bg-blue-600 dark:bg-blue-400 h-2 rounded-full transition-all duration-300"
							style={{ width: \`\${(state.step / state.totalSteps) * 100}%\` }}
						/>
					</div>
					<p className="text-sm">{state.message}</p>
				</div>
			)}

			{/* Success state */}
			{state.status === "completed" && (
				<div className="p-4 bg-green-50 text-green-800 rounded border border-green-200 dark:bg-green-950 dark:text-green-200 dark:border-green-800">
					<div className="font-medium mb-2">Workflow completed!</div>
					<div className="text-sm whitespace-pre-wrap">{state.result}</div>
				</div>
			)}

			{/* Error state */}
			{state.status === "error" && (
				<div className="p-4 bg-red-50 text-red-800 rounded border border-red-200 dark:bg-red-950 dark:text-red-200 dark:border-red-800">
					<div className="font-medium mb-1">Error</div>
					<div className="text-sm">{state.error}</div>
					<Button
						variant="outline"
						size="sm"
						className="mt-2"
						onClick={reset}
					>
						Try again
					</Button>
				</div>
			)}

			<p className="text-sm text-muted-foreground">
				View workflow runs with: <code className="bg-muted px-1 rounded">npx workflow web</code>
			</p>
		</div>
	);
}
"
`;

exports[`template snapshots > dashboard templates > generateDashboardActions 1`] = `
""use server";

import { z } from "zod";
import { authActionClient } from "@/lib/safe-action";

/**
 * Example authenticated server action
 *
 * This demonstrates using next-safe-action with:
 * - Zod schema validation
 * - Authentication middleware (ctx.userId available)
 * - Type-safe input/output
 */
export const exampleAction = authActionClient
	.schema(
		z.object({
			message: z.string().min(1, "Message is required"),
		}),
	)
	.action(async ({ parsedInput, ctx }) => {
		// ctx.userId and ctx.user are available from the auth middleware
		console.log(\`User \${ctx.userId} sent: \${parsedInput.message}\`);

		return {
			success: true,
			userId: ctx.userId,
			echo: parsedInput.message,
		};
	});

/**
 * Example action with more complex input
 */
export const updateSettingsAction = authActionClient
	.schema(
		z.object({
			notifications: z.boolean().optional(),
			theme: z.enum(["light", "dark", "system"]).optional(),
		}),
	)
	.action(async ({ parsedInput }) => {
		// In a real app, you'd update the database here using ctx.userId
		// await db.update(userSettings).set(parsedInput).where(eq(userSettings.userId, ctx.userId));

		return {
			success: true,
			updatedSettings: parsedInput,
		};
	});
"
`;

exports[`template snapshots > dashboard templates > generateDashboardPage (Better Auth) 1`] = `
"import { Suspense } from "react";
import { redirect } from "next/navigation";
import { AITriggerButton } from "./_components/ai-trigger";
import { SignOutButton } from "./_components/sign-out-button";
import { DashboardSkeleton } from "./_components/skeleton";
import { Avatar, AvatarFallback, AvatarImage } from "@workspace/ui/components/avatar";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@workspace/ui/components/card";
import { Separator } from "@workspace/ui/components/separator";
import { getSession } from "@/lib/auth";

async function DashboardContent() {
	const session = await getSession();

	if (!session?.user) {
		redirect("/login");
	}

	const user = session.user;

	// Extract only needed fields to minimize serialization
	const displayName = user?.name || user?.email || "User";
	const avatarUrl = user?.image || undefined;
	const initials = user?.name?.[0] || user?.email?.[0]?.toUpperCase() || "U";
	const userInfo = {
		name: user?.name,
		email: user?.email,
	};

	return (
		<div className="container mx-auto p-8">
			<div className="flex items-center justify-between mb-8">
				<div className="flex items-center gap-4">
					<Avatar className="h-12 w-12">
						<AvatarImage src={avatarUrl} />
						<AvatarFallback>{initials}</AvatarFallback>
					</Avatar>
					<div>
						<h1 className="text-2xl font-bold">Dashboard</h1>
						<p className="text-muted-foreground">
							Welcome back, {displayName}
						</p>
					</div>
				</div>
				<SignOutButton />
			</div>

			<Separator className="mb-8" />

			<div className="grid gap-6">
				<Card className="defer-render">
					<CardHeader>
						<CardTitle>AI Workflow Demo</CardTitle>
						<CardDescription>
							Click the button below to trigger an AI workflow that processes
							your request using Vercel Workflow DevKit.
						</CardDescription>
					</CardHeader>
					<CardContent>
						<AITriggerButton />
					</CardContent>
				</Card>

				<Card className="defer-render">
					<CardHeader>
						<CardTitle>User Info</CardTitle>
						<CardDescription>Your account details</CardDescription>
					</CardHeader>
					<CardContent>
						<pre className="bg-muted p-4 rounded text-sm overflow-auto">
							{JSON.stringify(userInfo, null, 2)}
						</pre>
					</CardContent>
				</Card>
			</div>
		</div>
	);
}

export default function DashboardPage() {
	return (
		<Suspense fallback={<DashboardSkeleton />}>
			<DashboardContent />
		</Suspense>
	);
}
"
`;

exports[`template snapshots > dashboard templates > generateDashboardPage (WorkOS) 1`] = `
"import { Suspense } from "react";
import { signOut, withAuth } from "@workos-inc/authkit-nextjs";
import { AITriggerButton } from "./_components/ai-trigger";
import { DashboardSkeleton } from "./_components/skeleton";
import { Avatar, AvatarFallback, AvatarImage } from "@workspace/ui/components/avatar";
import { Button } from "@workspace/ui/components/button";
import { Card, CardContent, CardDescription, CardHeader, CardTitle } from "@workspace/ui/components/card";
import { Separator } from "@workspace/ui/components/separator";

async function DashboardContent() {
	const { user } = await withAuth({ ensureSignedIn: true });

	// Extract only needed fields to minimize serialization
	const displayName = user?.firstName || user?.email || "User";
	const avatarUrl = user?.profilePictureUrl || undefined;
	const initials = user?.firstName?.[0] || user?.email?.[0]?.toUpperCase() || "U";
	const userInfo = {
		email: user?.email,
		firstName: user?.firstName,
		lastName: user?.lastName,
	};

	return (
		<div className="container mx-auto p-8">
			<div className="flex items-center justify-between mb-8">
				<div className="flex items-center gap-4">
					<Avatar className="h-12 w-12">
						<AvatarImage src={avatarUrl} />
						<AvatarFallback>{initials}</AvatarFallback>
					</Avatar>
					<div>
						<h1 className="text-2xl font-bold">Dashboard</h1>
						<p className="text-muted-foreground">
							Welcome back, {displayName}
						</p>
					</div>
				</div>
				<form
					action={async () => {
						"use server";
						await signOut();
					}}
				>
					<Button variant="outline" type="submit">
						Sign Out
					</Button>
				</form>
			</div>

			<Separator className="mb-8" />

			<div className="grid gap-6">
				<Card className="defer-render">
					<CardHeader>
						<CardTitle>AI Workflow Demo</CardTitle>
						<CardDescription>
							Click the button below to trigger an AI workflow that processes
							your request using Vercel Workflow DevKit.
						</CardDescription>
					</CardHeader>
					<CardContent>
						<AITriggerButton />
					</CardContent>
				</Card>

				<Card className="defer-render">
					<CardHeader>
						<CardTitle>User Info</CardTitle>
						<CardDescription>Your account details from WorkOS</CardDescription>
					</CardHeader>
					<CardContent>
						<pre className="bg-muted p-4 rounded text-sm overflow-auto">
							{JSON.stringify(userInfo, null, 2)}
						</pre>
					</CardContent>
				</Card>
			</div>
		</div>
	);
}

export default function DashboardPage() {
	return (
		<Suspense fallback={<DashboardSkeleton />}>
			<DashboardContent />
		</Suspense>
	);
}
"
`;

exports[`template snapshots > dashboard templates > generateDashboardSkeleton 1`] = `
"export function DashboardSkeleton() {
	return (
		<div className="container mx-auto p-8 animate-pulse">
			<div className="flex items-center justify-between mb-8">
				<div className="flex items-center gap-4">
					<div className="h-12 w-12 bg-muted rounded-full" />
					<div className="space-y-2">
						<div className="h-7 w-32 bg-muted rounded" />
						<div className="h-4 w-48 bg-muted rounded" />
					</div>
				</div>
				<div className="h-9 w-24 bg-muted rounded" />
			</div>

			<div className="h-px w-full bg-muted mb-8" />

			<div className="grid gap-6">
				<div className="border rounded-lg p-6 space-y-4">
					<div className="space-y-2">
						<div className="h-6 w-40 bg-muted rounded" />
						<div className="h-4 w-72 bg-muted rounded" />
					</div>
					<div className="h-10 w-32 bg-muted rounded" />
				</div>

				<div className="border rounded-lg p-6 space-y-4">
					<div className="space-y-2">
						<div className="h-6 w-24 bg-muted rounded" />
						<div className="h-4 w-48 bg-muted rounded" />
					</div>
					<div className="h-32 w-full bg-muted rounded" />
				</div>
			</div>
		</div>
	);
}
"
`;

exports[`template snapshots > dashboard templates > generateSignOutButton 1`] = `
""use client";

import { useRouter } from "next/navigation";
import { authClient } from "@/lib/auth-client";
import { Button } from "@workspace/ui/components/button";

export function SignOutButton() {
	const router = useRouter();

	const handleSignOut = async () => {
		await authClient.signOut({
			fetchOptions: {
				onSuccess: () => {
					router.push("/login");
				},
			},
		});
	};

	return (
		<Button variant="outline" onClick={handleSignOut}>
			Sign Out
		</Button>
	);
}
"
`;

exports[`template snapshots > database templates > generateBetterAuthSchema 1`] = `
"import { relations } from "drizzle-orm";
import { pgTable, text, timestamp, boolean, index } from "drizzle-orm/pg-core";

// Better Auth tables
// Generated to match @better-auth/cli output

export const user = pgTable("user", {
	id: text("id").primaryKey(),
	name: text("name").notNull(),
	email: text("email").notNull().unique(),
	emailVerified: boolean("email_verified").default(false).notNull(),
	image: text("image"),
	createdAt: timestamp("created_at").defaultNow().notNull(),
	updatedAt: timestamp("updated_at")
		.defaultNow()
		.$onUpdate(() => new Date())
		.notNull(),
});

export const session = pgTable(
	"session",
	{
		id: text("id").primaryKey(),
		expiresAt: timestamp("expires_at").notNull(),
		token: text("token").notNull().unique(),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at")
			.$onUpdate(() => new Date())
			.notNull(),
		ipAddress: text("ip_address"),
		userAgent: text("user_agent"),
		userId: text("user_id")
			.notNull()
			.references(() => user.id, { onDelete: "cascade" }),
	},
	(table) => [index("session_userId_idx").on(table.userId)],
);

export const account = pgTable(
	"account",
	{
		id: text("id").primaryKey(),
		accountId: text("account_id").notNull(),
		providerId: text("provider_id").notNull(),
		userId: text("user_id")
			.notNull()
			.references(() => user.id, { onDelete: "cascade" }),
		accessToken: text("access_token"),
		refreshToken: text("refresh_token"),
		idToken: text("id_token"),
		accessTokenExpiresAt: timestamp("access_token_expires_at"),
		refreshTokenExpiresAt: timestamp("refresh_token_expires_at"),
		scope: text("scope"),
		password: text("password"),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at")
			.$onUpdate(() => new Date())
			.notNull(),
	},
	(table) => [index("account_userId_idx").on(table.userId)],
);

export const verification = pgTable(
	"verification",
	{
		id: text("id").primaryKey(),
		identifier: text("identifier").notNull(),
		value: text("value").notNull(),
		expiresAt: timestamp("expires_at").notNull(),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at")
			.defaultNow()
			.$onUpdate(() => new Date())
			.notNull(),
	},
	(table) => [index("verification_identifier_idx").on(table.identifier)],
);

export const userRelations = relations(user, ({ many }) => ({
	sessions: many(session),
	accounts: many(account),
}));

export const sessionRelations = relations(session, ({ one }) => ({
	user: one(user, {
		fields: [session.userId],
		references: [user.id],
	}),
}));

export const accountRelations = relations(account, ({ one }) => ({
	user: one(user, {
		fields: [account.userId],
		references: [user.id],
	}),
}));

// Add your custom tables below
"
`;

exports[`template snapshots > database templates > generateDbIndex 1`] = `
"import { drizzle, type NodePgDatabase } from "drizzle-orm/node-postgres";
import * as schema from "./schema";

let dbInstance: NodePgDatabase<typeof schema> | null = null;

export function getDb(): NodePgDatabase<typeof schema> {
	if (dbInstance) return dbInstance;

	if (!process.env.DATABASE_URL) {
		throw new Error("DATABASE_URL environment variable is required");
	}

	dbInstance = drizzle(process.env.DATABASE_URL, { schema });
	return dbInstance;
}

// Export a proxy that lazily initializes the db
// This allows imports without throwing at build time
export const db = new Proxy({} as NodePgDatabase<typeof schema>, {
	get(_, prop) {
		return getDb()[prop as keyof NodePgDatabase<typeof schema>];
	},
});
"
`;

exports[`template snapshots > database templates > generateDbSchema (Better Auth) 1`] = `
"import { relations } from "drizzle-orm";
import { pgTable, text, timestamp, boolean, index } from "drizzle-orm/pg-core";

// Better Auth tables
// Generated to match @better-auth/cli output

export const user = pgTable("user", {
	id: text("id").primaryKey(),
	name: text("name").notNull(),
	email: text("email").notNull().unique(),
	emailVerified: boolean("email_verified").default(false).notNull(),
	image: text("image"),
	createdAt: timestamp("created_at").defaultNow().notNull(),
	updatedAt: timestamp("updated_at")
		.defaultNow()
		.$onUpdate(() => new Date())
		.notNull(),
});

export const session = pgTable(
	"session",
	{
		id: text("id").primaryKey(),
		expiresAt: timestamp("expires_at").notNull(),
		token: text("token").notNull().unique(),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at")
			.$onUpdate(() => new Date())
			.notNull(),
		ipAddress: text("ip_address"),
		userAgent: text("user_agent"),
		userId: text("user_id")
			.notNull()
			.references(() => user.id, { onDelete: "cascade" }),
	},
	(table) => [index("session_userId_idx").on(table.userId)],
);

export const account = pgTable(
	"account",
	{
		id: text("id").primaryKey(),
		accountId: text("account_id").notNull(),
		providerId: text("provider_id").notNull(),
		userId: text("user_id")
			.notNull()
			.references(() => user.id, { onDelete: "cascade" }),
		accessToken: text("access_token"),
		refreshToken: text("refresh_token"),
		idToken: text("id_token"),
		accessTokenExpiresAt: timestamp("access_token_expires_at"),
		refreshTokenExpiresAt: timestamp("refresh_token_expires_at"),
		scope: text("scope"),
		password: text("password"),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at")
			.$onUpdate(() => new Date())
			.notNull(),
	},
	(table) => [index("account_userId_idx").on(table.userId)],
);

export const verification = pgTable(
	"verification",
	{
		id: text("id").primaryKey(),
		identifier: text("identifier").notNull(),
		value: text("value").notNull(),
		expiresAt: timestamp("expires_at").notNull(),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at")
			.defaultNow()
			.$onUpdate(() => new Date())
			.notNull(),
	},
	(table) => [index("verification_identifier_idx").on(table.identifier)],
);

export const userRelations = relations(user, ({ many }) => ({
	sessions: many(session),
	accounts: many(account),
}));

export const sessionRelations = relations(session, ({ one }) => ({
	user: one(user, {
		fields: [session.userId],
		references: [user.id],
	}),
}));

export const accountRelations = relations(account, ({ one }) => ({
	user: one(user, {
		fields: [account.userId],
		references: [user.id],
	}),
}));

// Add your custom tables below
"
`;

exports[`template snapshots > database templates > generateDbSchema (WorkOS) 1`] = `
"import {
	pgTable,
	text,
	timestamp,
	uuid,
	boolean,
	index,
	unique,
} from "drizzle-orm/pg-core";

// Users table - synced from WorkOS
export const users = pgTable(
	"users",
	{
		id: uuid("id").primaryKey().defaultRandom(),
		workosId: text("workos_id").notNull().unique(),
		email: text("email").notNull().unique(),
		firstName: text("first_name"),
		lastName: text("last_name"),
		profilePictureUrl: text("profile_picture_url"),
		emailVerified: boolean("email_verified").default(false).notNull(),
		lastSignInAt: timestamp("last_sign_in_at"),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at").defaultNow().notNull(),
		deletedAt: timestamp("deleted_at"),
	},
	(table) => [
		index("users_workos_id_idx").on(table.workosId),
		index("users_email_idx").on(table.email),
	],
);

// Organizations table - synced from WorkOS
export const organizations = pgTable(
	"organizations",
	{
		id: uuid("id").primaryKey().defaultRandom(),
		workosId: text("workos_id").notNull().unique(),
		name: text("name").notNull(),
		allowProfilesOutsideOrganization: boolean(
			"allow_profiles_outside_organization",
		)
			.default(false)
			.notNull(),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at").defaultNow().notNull(),
		deletedAt: timestamp("deleted_at"),
	},
	(table) => [index("organizations_workos_id_idx").on(table.workosId)],
);

// Organization memberships - junction table linking users to organizations
export const organizationMemberships = pgTable(
	"organization_memberships",
	{
		id: uuid("id").primaryKey().defaultRandom(),
		workosId: text("workos_id").notNull().unique(),
		organizationId: uuid("organization_id")
			.notNull()
			.references(() => organizations.id, { onDelete: "cascade" }),
		userId: uuid("user_id")
			.notNull()
			.references(() => users.id, { onDelete: "cascade" }),
		roleSlug: text("role_slug"),
		roleName: text("role_name"),
		status: text("status").default("active"),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at").defaultNow().notNull(),
		deletedAt: timestamp("deleted_at"),
	},
	(table) => [
		index("org_memberships_org_id_idx").on(table.organizationId),
		index("org_memberships_user_id_idx").on(table.userId),
		unique("org_memberships_org_user_unique").on(
			table.organizationId,
			table.userId,
		),
	],
);

"
`;

exports[`template snapshots > database templates > generateDrizzleConfig 1`] = `
"import { defineConfig } from "drizzle-kit";

export default defineConfig({
	schema: "./db/schema.ts",
	out: "./drizzle",
	dialect: "postgresql",
	...(process.env.DATABASE_URL && {
		dbCredentials: {
			url: process.env.DATABASE_URL,
		},
	}),
});
"
`;

exports[`template snapshots > database templates > generateWorkOSSchema 1`] = `
"import {
	pgTable,
	text,
	timestamp,
	uuid,
	boolean,
	index,
	unique,
} from "drizzle-orm/pg-core";

// Users table - synced from WorkOS
export const users = pgTable(
	"users",
	{
		id: uuid("id").primaryKey().defaultRandom(),
		workosId: text("workos_id").notNull().unique(),
		email: text("email").notNull().unique(),
		firstName: text("first_name"),
		lastName: text("last_name"),
		profilePictureUrl: text("profile_picture_url"),
		emailVerified: boolean("email_verified").default(false).notNull(),
		lastSignInAt: timestamp("last_sign_in_at"),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at").defaultNow().notNull(),
		deletedAt: timestamp("deleted_at"),
	},
	(table) => [
		index("users_workos_id_idx").on(table.workosId),
		index("users_email_idx").on(table.email),
	],
);

// Organizations table - synced from WorkOS
export const organizations = pgTable(
	"organizations",
	{
		id: uuid("id").primaryKey().defaultRandom(),
		workosId: text("workos_id").notNull().unique(),
		name: text("name").notNull(),
		allowProfilesOutsideOrganization: boolean(
			"allow_profiles_outside_organization",
		)
			.default(false)
			.notNull(),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at").defaultNow().notNull(),
		deletedAt: timestamp("deleted_at"),
	},
	(table) => [index("organizations_workos_id_idx").on(table.workosId)],
);

// Organization memberships - junction table linking users to organizations
export const organizationMemberships = pgTable(
	"organization_memberships",
	{
		id: uuid("id").primaryKey().defaultRandom(),
		workosId: text("workos_id").notNull().unique(),
		organizationId: uuid("organization_id")
			.notNull()
			.references(() => organizations.id, { onDelete: "cascade" }),
		userId: uuid("user_id")
			.notNull()
			.references(() => users.id, { onDelete: "cascade" }),
		roleSlug: text("role_slug"),
		roleName: text("role_name"),
		status: text("status").default("active"),
		createdAt: timestamp("created_at").defaultNow().notNull(),
		updatedAt: timestamp("updated_at").defaultNow().notNull(),
		deletedAt: timestamp("deleted_at"),
	},
	(table) => [
		index("org_memberships_org_id_idx").on(table.organizationId),
		index("org_memberships_user_id_idx").on(table.userId),
		unique("org_memberships_org_user_unique").on(
			table.organizationId,
			table.userId,
		),
	],
);

"
`;

exports[`template snapshots > docker templates > generateDockerCompose 1`] = `
"services:
  postgres:
    image: pgvector/pgvector:pg18
    container_name: undefined-postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: undefined
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-test:
    image: pgvector/pgvector:pg18
    container_name: undefined-postgres-test
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: undefined_test
    ports:
      - '5434:5432'
    volumes:
      - postgres_test_data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U postgres']
      interval: 10s
      timeout: 5s
      retries: 5

volumes:
  postgres_data:
  postgres_test_data:
"
`;

exports[`template snapshots > hooks templates > generateUseLatest 1`] = `
""use client";

import { useRef, useEffect } from "react";

/**
 * Returns a ref that always contains the latest value.
 * Useful for accessing values in callbacks without adding them to dependency arrays.
 */
export function useLatest<T>(value: T) {
	const ref = useRef(value);
	useEffect(() => {
		ref.current = value;
	}, [value]);
	return ref;
}
"
`;

exports[`template snapshots > hooks templates > generateUseWorkflowProgress 1`] = `
""use client";

import { useCallback, useEffect, useRef, useState } from "react";
import { WORKFLOW_TOTAL_STEPS } from "@/lib/workflow-progress/types";

/**
 * Progress event structure from the workflow SSE stream
 */
interface ProgressEvent {
	type: "progress" | "completed" | "error";
	step: number;
	totalSteps: number;
	message: string;
	data?: {
		result?: string;
		error?: string;
	};
}

/**
 * State machine for workflow progress
 */
export type WorkflowState =
	| { status: "idle" }
	| {
			status: "generating";
			runId: string;
			step: number;
			totalSteps: number;
			message: string;
	  }
	| { status: "completed"; result: string }
	| { status: "error"; error: string };

/**
 * Return type for the workflow progress hook
 */
export interface UseWorkflowProgressReturn {
	/** Current state of the workflow */
	state: WorkflowState;
	/** Start tracking a workflow - pass runId from your action result */
	startWithRunId: (runId: string) => void;
	/** Set error state (e.g., when action fails before workflow starts) */
	setError: (error: string) => void;
	/** Reset state to idle */
	reset: () => void;
}

/**
 * Hook for consuming workflow progress events via SSE
 *
 * Tracks progress for the current page session only. If the user refreshes,
 * progress is lost but the workflow continues server-side.
 *
 * @example
 * \`\`\`tsx
 * const { state, startWithRunId } = useWorkflowProgress();
 *
 * const handleStart = async () => {
 *   const response = await fetch("/api/workflow", {
 *     method: "POST",
 *     body: JSON.stringify({ prompt }),
 *   });
 *   const { runId } = await response.json();
 *   startWithRunId(runId);
 * };
 * \`\`\`
 */
export function useWorkflowProgress(): UseWorkflowProgressReturn {
	const [state, setState] = useState<WorkflowState>({ status: "idle" });

	const abortControllerRef = useRef<AbortController | null>(null);
	const isMountedRef = useRef(true);
	const cleanupTimeoutRef = useRef<NodeJS.Timeout | null>(null);

	// Reset mounted ref on every render (handles Strict Mode remounts)
	isMountedRef.current = true;

	// Connect to the workflow progress SSE stream
	const connectToStream = useCallback(async (runId: string, retryCount = 0) => {
		const abortController = new AbortController();
		abortControllerRef.current = abortController;

		try {
			const response = await fetch(\`/api/workflow-progress/\${runId}\`, {
				signal: abortController.signal,
			});

			if (!response.ok) {
				if (response.status === 404) {
					// Workflow not found - may not be ready yet after start()
					const maxRetries = 3;
					if (retryCount < maxRetries) {
						const delay = Math.min(500 * 2 ** retryCount, 2000);
						await new Promise((resolve) => setTimeout(resolve, delay));
						return connectToStream(runId, retryCount + 1);
					}
					setState({ status: "idle" });
					return;
				}
				throw new Error(\`Failed to connect to stream: \${response.status}\`);
			}

			const reader = response.body?.getReader();
			if (!reader) {
				throw new Error("No response body");
			}

			const decoder = new TextDecoder();
			let buffer = "";

			while (true) {
				const { done, value } = await reader.read();
				if (done) break;

				buffer += decoder.decode(value, { stream: true });
				const lines = buffer.split("\\n\\n");
				buffer = lines.pop() || "";

				for (const line of lines) {
					if (line.startsWith("data: ")) {
						let event: ProgressEvent;
						try {
							event = JSON.parse(line.slice(6)) as ProgressEvent;
						} catch {
							continue;
						}

						if (event.type === "progress") {
							setState({
								status: "generating",
								runId,
								step: event.step,
								totalSteps: event.totalSteps,
								message: event.message,
							});
						} else if (event.type === "completed" && event.data?.result) {
							setState({ status: "completed", result: event.data.result });
						} else if (event.type === "error") {
							setState({
								status: "error",
								error: event.data?.error || "An error occurred",
							});
						}
					}
				}
			}
		} catch (error) {
			if (error instanceof Error && error.name === "AbortError") {
				return;
			}
			setState({
				status: "error",
				error: error instanceof Error ? error.message : "Connection lost",
			});
		}
	}, []);

	// Start tracking a workflow with a given runId
	const startWithRunId = useCallback(
		async (runId: string) => {
			setState({
				status: "generating",
				runId,
				step: 0,
				totalSteps: WORKFLOW_TOTAL_STEPS,
				message: "Starting workflow...",
			});

			// Small delay to allow workflow to initialize on Vercel infrastructure
			await new Promise((resolve) => setTimeout(resolve, 500));

			connectToStream(runId);
		},
		[connectToStream],
	);

	// Set error state
	const setError = useCallback((error: string) => {
		setState({ status: "error", error });
	}, []);

	// Reset state to idle
	const reset = useCallback(() => {
		abortControllerRef.current?.abort();
		setState({ status: "idle" });
	}, []);

	// Cleanup on unmount
	useEffect(() => {
		return () => {
			isMountedRef.current = false;

			if (cleanupTimeoutRef.current) {
				clearTimeout(cleanupTimeoutRef.current);
			}

			const currentController = abortControllerRef.current;
			cleanupTimeoutRef.current = setTimeout(() => {
				if (!isMountedRef.current && currentController) {
					currentController.abort();
				}
			}, 200);
		};
	}, []);

	return {
		state,
		startWithRunId,
		setError,
		reset,
	};
}
"
`;

exports[`template snapshots > lib templates > generateClientLogger 1`] = `
"import posthog from "posthog-js";

type LogLevel = "debug" | "info" | "warn" | "error";

interface LogContext {
	[key: string]: unknown;
}

class ClientLogger {
	/**
	 * Log a debug message
	 */
	debug(message: string, context?: LogContext): void {
		this.log("debug", message, context);
	}

	/**
	 * Log an info message
	 */
	info(message: string, context?: LogContext): void {
		this.log("info", message, context);
	}

	/**
	 * Log a warning message
	 */
	warn(message: string, context?: LogContext): void {
		this.log("warn", message, context);
	}

	/**
	 * Log an error and send it to PostHog error tracking
	 */
	error(message: string, error?: Error | unknown, context?: LogContext): void {
		this.log("error", message, context);

		// Send to PostHog error tracking
		this.captureError(message, error, context);
	}

	/**
	 * Capture error to PostHog client-side
	 */
	private captureError(
		message: string,
		error?: Error | unknown,
		context?: LogContext,
	): void {
		try {
			if (!posthog.__loaded) return;

			if (error instanceof Error) {
				posthog.captureException(error, {
					...context,
					message,
				});
			} else if (error) {
				const errorObj = new Error(message);
				posthog.captureException(errorObj, {
					...context,
					originalError: error,
				});
			} else {
				const errorObj = new Error(message);
				posthog.captureException(errorObj, context);
			}
		} catch (err) {
			// Fail silently if PostHog tracking fails
			console.error("Failed to send error to PostHog:", err);
		}
	}

	/**
	 * Internal logging method
	 */
	private log(level: LogLevel, message: string, context?: LogContext): void {
		const timestamp = new Date().toISOString();
		const prefix = \`[\${timestamp}] [\${level.toUpperCase()}]\`;

		if (context && Object.keys(context).length > 0) {
			console[level](\`\${prefix} \${message}\`, context);
		} else {
			console[level](\`\${prefix} \${message}\`);
		}
	}
}

// Export a singleton instance
export const logger = new ClientLogger();
"
`;

exports[`template snapshots > lib templates > generateSafeAction 1`] = `
""use server";

import { createSafeActionClient } from "next-safe-action";
import { auth } from "@/lib/auth";
import { headers } from "next/headers";

/**
 * Base action client without authentication
 * Use this for public actions that don't require auth
 */
export const actionClient = createSafeActionClient();

/**
 * Authenticated action client
 * Automatically validates the user is logged in and provides userId in context
 *
 * Usage:
 * \`\`\`ts
 * export const myAction = authActionClient
 *   .inputSchema(z.object({ ... }))
 *   .action(async ({ parsedInput, ctx }) => {
 *     // ctx.userId is available here
 *     return { success: true };
 *   });
 * \`\`\`
 */
export const authActionClient = actionClient.use(async ({ next }) => {
	const session = await auth.api.getSession({
		headers: await headers(),
	});

	if (!session?.user) {
		throw new Error("Unauthorized: Authentication required");
	}

	return next({
		ctx: {
			userId: session.user.id,
			user: {
				id: session.user.id,
				email: session.user.email,
				name: session.user.name,
			},
		},
	});
});
"
`;

exports[`template snapshots > lib templates > generateServerLogger 1`] = `
"import "server-only";
import getPostHogClient from "@/lib/posthog";

type LogLevel = "debug" | "info" | "warn" | "error";

interface LogContext {
	[key: string]: unknown;
}

class ServerLogger {
	/**
	 * Log a debug message
	 */
	debug(message: string, context?: LogContext): void {
		this.log("debug", message, context);
	}

	/**
	 * Log an info message
	 */
	info(message: string, context?: LogContext): void {
		this.log("info", message, context);
	}

	/**
	 * Log a warning message
	 */
	warn(message: string, context?: LogContext): void {
		this.log("warn", message, context);
	}

	/**
	 * Log an error and send it to PostHog
	 * @param message - Error message
	 * @param error - Error object or unknown error
	 * @param context - Additional context to attach to the error
	 * @param distinctId - Optional PostHog distinct_id to associate the error with a user
	 */
	error(
		message: string,
		error?: Error | unknown,
		context?: LogContext,
		distinctId?: string,
	): void {
		this.log("error", message, context);

		// Send to PostHog (fire-and-forget)
		this.captureError(message, error, context, distinctId).catch((err) => {
			console.error("Error in captureError:", err);
		});
	}

	/**
	 * Capture error to PostHog server-side
	 */
	private async captureError(
		message: string,
		error?: Error | unknown,
		context?: LogContext,
		distinctId?: string,
	): Promise<void> {
		const posthog = getPostHogClient();
		if (!posthog) return;

		try {
			const errorMessage = error instanceof Error ? error.message : message;
			const errorStack = error instanceof Error ? error.stack : undefined;

			posthog.capture({
				distinctId: distinctId || "server",
				event: "error",
				properties: {
					...context,
					message,
					errorMessage,
					errorStack,
					source: "logger",
				},
			});

			await posthog.flush();
		} catch (err) {
			// Fail silently if PostHog tracking fails
			console.error("Failed to send error to PostHog:", err);
		}
	}

	/**
	 * Internal logging method
	 */
	private log(level: LogLevel, message: string, context?: LogContext): void {
		const timestamp = new Date().toISOString();
		const prefix = \`[\${timestamp}] [\${level.toUpperCase()}]\`;

		if (context && Object.keys(context).length > 0) {
			console[level](\`\${prefix} \${message}\`, context);
		} else {
			console[level](\`\${prefix} \${message}\`);
		}
	}
}

// Export a singleton instance
export const logger = new ServerLogger();
"
`;

exports[`template snapshots > marketing templates > generateFooter 1`] = `
"import { cacheLife, cacheTag } from "next/cache";

export async function Footer() {
	"use cache";
	cacheLife("weeks");
	cacheTag("marketing-footer");

	return (
		<footer className="border-t py-6 text-center text-sm text-muted-foreground">
			<p>&copy; {new Date().getFullYear()} Your Company. All rights reserved.</p>
		</footer>
	);
}
"
`;

exports[`template snapshots > marketing templates > generateHero 1`] = `
"import Link from "next/link";
import { cacheLife, cacheTag } from "next/cache";
import { Button } from "@workspace/ui/components/button";

export async function Hero() {
	"use cache";
	cacheLife("days");
	cacheTag("marketing-hero");

	return (
		<section className="flex-1 flex flex-col items-center justify-center gap-6 p-8 text-center">
			<h1 className="text-4xl font-bold tracking-tight sm:text-5xl">
				Welcome to Your App
			</h1>
			<p className="text-lg text-muted-foreground max-w-md">
				Built with Next.js, Turborepo, and Hatch
			</p>
			<Button asChild size="lg">
				<Link href="/login">Sign In</Link>
			</Button>
		</section>
	);
}
"
`;

exports[`template snapshots > marketing templates > generateMarketingPage 1`] = `
"import type { Metadata } from "next";
import { cacheLife, cacheTag } from "next/cache";
import { Hero } from "./_components/hero";
import { Footer } from "./_components/footer";

export const metadata: Metadata = {
	title: "Home",
	description: "Welcome to undefined",
};

export default async function HomePage() {
	"use cache";
	cacheLife("days");
	cacheTag("marketing-home");

	const jsonLd = {
		"@context": "https://schema.org",
		"@type": "WebSite",
		name: "undefined",
		url: process.env.NEXT_PUBLIC_APP_URL || "http://localhost:3000",
	};

	return (
		<>
			<script
				type="application/ld+json"
				// biome-ignore lint/security/noDangerouslySetInnerHtml: JSON-LD structured data for SEO
				dangerouslySetInnerHTML={{ __html: JSON.stringify(jsonLd) }}
			/>
			<main className="min-h-screen flex flex-col">
				<Hero />
				<Footer />
			</main>
		</>
	);
}
"
`;

exports[`template snapshots > root templates > generateBiomeJson 1`] = `
"{
  "$schema": "https://biomejs.dev/schemas/2.3.11/schema.json",
  "vcs": {
    "enabled": true,
    "clientKind": "git",
    "useIgnoreFile": true
  },
  "files": {
    "ignoreUnknown": false,
    "includes": [
      "**/*.ts",
      "**/*.tsx",
      "**/*.js",
      "**/*.jsx",
      "**/*.json",
      "!**/node_modules/**",
      "!**/.next/**",
      "!**/dist/**",
      "!**/drizzle/**",
      "!**/packages/ui/src/components/**"
    ]
  },
  "formatter": {
    "enabled": true,
    "indentStyle": "space",
    "indentWidth": 2
  },
  "assist": {
    "enabled": true,
    "actions": {
      "source": {
        "organizeImports": {
          "level": "off"
        }
      }
    }
  },
  "linter": {
    "enabled": true,
    "rules": {
      "recommended": true
    }
  },
  "javascript": {
    "formatter": {
      "quoteStyle": "double",
      "semicolons": "always"
    }
  }
}
"
`;

exports[`template snapshots > root templates > generateClaudeMd 1`] = `
"# CLAUDE.md

This file provides guidance to Claude Code when working with code in this repository.

## Important: Node Version Management

**ALWAYS run \`nvm use\` before executing any commands.** This project requires Node.js >=22.

## Git Operations

**ALWAYS ask for explicit user confirmation before running any git commands that modify history or remote state**, including:
- \`git commit\`
- \`git push\`
- \`git merge\`
- \`git rebase\`
- \`git reset\`
- \`git stash\`
- \`git cherry-pick\`

This applies even in dangerous/bypass permissions mode. Never auto-approve these operations.

## Commands

### Development
- \`pnpm dev\` - Start all apps with Turbopack
- \`pnpm build\` - Build all apps and packages
- \`pnpm start\` - Start production server

### Code Quality
- \`pnpm lint\` - Biome linting across workspaces
- \`pnpm format\` - Biome formatting

### Testing
- \`pnpm test\` - Run all tests
- \`pnpm test:watch\` - Watch mode (from apps/web)
- \`pnpm test:ui\` - Interactive Vitest UI

### Database
- \`pnpm docker:up\` / \`docker:down\` - Start/stop PostgreSQL
- \`pnpm db:generate\` - Generate migrations from schema
- \`pnpm db:migrate\` - Apply migrations
- \`pnpm db:studio\` - Open Drizzle Studio

### Test Database
- \`pnpm docker:up:test\` / \`docker:down:test\` - Start/stop test DB

## Architecture

### Project Overview
test-app is a Next.js application built as a pnpm monorepo with Turborepo.

### Monorepo Structure
\`\`\`
test-app/
├── apps/web/                    # Main Next.js application
│   ├── app/
│   │   ├── (marketing)/         # Public marketing pages
│   │   ├── (auth)/              # Login/authentication
│   │   ├── (app)/               # Authenticated user pages
│   │   └── api/                 # API routes
│   ├── components/              # React components
│   ├── db/                      # Drizzle ORM schema and client
│   ├── lib/                     # Shared utilities
│   ├── services/                # Data access layer
│   ├── workflows/               # Vercel Workflows
│   └── evals/                   # LLM evaluation framework
├── packages/
│   └── ui/                      # Shared UI components
└── docker-compose.yml           # PostgreSQL database
\`\`\`

### Tech Stack
- **Framework**: Next.js 16 with App Router, Turbopack
- **UI**: React 19, shadcn/ui, Tailwind CSS v4
- **Database**: PostgreSQL, Drizzle ORM
- **Auth**: Better Auth (Email OTP)
- **AI**: Vercel AI SDK with OpenAI
- **Workflows**: Vercel Workflow DevKit
- **Testing**: Vitest

## Key Conventions

### Server/Client Data Fetching Pattern
In server components, create promises but DO NOT await them. Pass promises to client components wrapped in Suspense. Client components unwrap with React's \`use\` hook. Always use service layer for database access.

### Route Groups
- \`(marketing)/\` - Public pages
- \`(auth)/\` - Login/authentication
- \`(app)/\` - Authenticated pages (protected by middleware)

### Workspace Dependencies
- \`@workspace/*\` protocol for internal packages
- Path aliases: \`@/*\` for app root

### Vercel Workflows
All I/O operations inside a workflow MUST be wrapped in functions marked with \`"use step"\`. The workflow engine needs this to properly track, retry, and resume operations.

\`\`\`typescript
// CORRECT - wrapped in step function
async function fetchData(id: string) {
  "use step";
  return getDataFromDb(id);
}

export async function myWorkflow(input) {
  "use workflow";
  const data = await fetchData(id);
}
\`\`\`

## Environment Variables

Required (stored in \`apps/web/.env.local\`):
- \`DATABASE_URL\` - PostgreSQL connection string
- \`OPENAI_API_KEY\` - For AI features
- \`BETTER_AUTH_SECRET\` - Auth secret key
- \`RESEND_API_KEY\` - For email OTP
- \`NEXT_PUBLIC_POSTHOG_KEY\` - PostHog analytics (optional)

## Development Workflow

### First-time Setup
1. \`pnpm install\`
2. \`pnpm docker:up\`
3. \`cp apps/web/.env.local.example apps/web/.env.local\`
4. Fill in environment variables
5. \`pnpm db:generate && pnpm db:migrate\`

### Daily Development
1. \`pnpm docker:up\` (if not running)
2. \`nvm use && pnpm dev\`

### Schema Changes
1. Edit \`apps/web/db/schema.ts\`
2. \`pnpm db:generate\`
3. \`pnpm db:migrate\`

## Service Layer

All database access goes through service files in \`apps/web/services/\`. Never call \`db\` directly from components or server actions.

## Browser Automation

Use \`agent-browser\` for web automation. Run \`agent-browser --help\` for all commands.

Core workflow:
1. \`agent-browser open <url>\` - Navigate to page
2. \`agent-browser snapshot -i\` - Get interactive elements with refs (@e1, @e2)
3. \`agent-browser click @e1\` / \`fill @e2 "text"\` - Interact using refs
4. Re-snapshot after page changes
"
`;

exports[`template snapshots > root templates > generateGitignore 1`] = `
"# Dependencies
node_modules/
.pnpm-store/

# Build
.next/
dist/
.turbo/
*.tsbuildinfo

# Environment
.env
.env.local
.env.*.local

# IDE
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Logs
*.log
npm-debug.log*

# Testing
coverage/

# Setup script state (idempotency tracking)
.setup-state

# Supabase
.supabase/
supabase/.temp/
supabase/.branches/

# Vercel
.vercel/
"
`;

exports[`template snapshots > root templates > generateMcpJson 1`] = `
"{
  "mcpServers": {
    "next-devtools": {
      "command": "npx",
      "args": ["-y", "next-devtools-mcp@latest"]
    }
  }
}
"
`;

exports[`template snapshots > root templates > generateNvmrc 1`] = `
"lts/jod
"
`;

exports[`template snapshots > root templates > generatePnpmWorkspace 1`] = `
"packages:
  - "apps/*"
  - "packages/*"
"
`;

exports[`template snapshots > root templates > generateReadme 1`] = `
"# test-app

A full-stack monorepo built with [Hatch](https://github.com/collinschaafsma/hatch).

## Quick Start

### Prerequisites

- [Node.js 22+](https://nodejs.org/) (see \`.nvmrc\`)
- [pnpm](https://pnpm.io/) (\`corepack enable\`)
- [Docker](https://www.docker.com/) (for local PostgreSQL)
- [Supabase CLI](https://supabase.com/docs/guides/cli) (for cloud database)

### Automated Setup (Recommended)

Run the interactive setup script to configure GitHub, Vercel, and Supabase:

\`\`\`bash
pnpm app:setup
\`\`\`

This will:
- Create a GitHub repository (or link to existing)
- Set up a Vercel project
- Create a Supabase project with dev branches
- Pull environment variables

### Manual Setup

1. Copy the environment template:
   \`\`\`bash
   cp apps/web/.env.local.example apps/web/.env.local
   \`\`\`

2. Fill in your environment variables in \`apps/web/.env.local\`

3. Set up Supabase:
   \`\`\`bash
   pnpm supabase:setup
   \`\`\`

4. Start the development server:
   \`\`\`bash
   pnpm dev
   \`\`\`

---

## Project Structure

\`\`\`
test-app/
├── apps/
│   └── web/              # Next.js application
│       ├── app/          # App router pages
│       ├── components/   # React components
│       ├── db/           # Drizzle schema and client
│       ├── hooks/        # Custom React hooks
│       ├── lib/          # Utilities and auth
│       ├── services/     # Business logic layer
│       ├── workflows/    # Vercel Workflow DevKit
│       ├── evals/        # LLM evaluation tests
│       └── __tests__/    # Vitest tests
├── packages/
│   └── ui/               # Shared UI components
├── scripts/              # Setup and worktree scripts
└── supabase/             # Supabase configuration
└── .github/workflows/   # CI/CD workflows
\`\`\`

---

## Development Commands

| Command | Description |
|---------|-------------|
| \`pnpm dev\` | Start Next.js development server (with Turbopack) |
| \`pnpm build\` | Build all packages for production |
| \`pnpm lint\` | Run Biome linting |
| \`pnpm typecheck\` | Run TypeScript type checking |
| \`pnpm format\` | Auto-format code with Biome |
| \`pnpm check\` | Run all Biome checks |
| \`pnpm test\` | Run Vitest tests |
| \`pnpm test:ui\` | Run tests with Vitest UI |

---

## Database Commands

| Command | Description |
|---------|-------------|
| \`pnpm db:generate\` | Generate Drizzle migration files |
| \`pnpm db:migrate\` | Apply pending migrations |
| \`pnpm db:push\` | Push schema directly (dev only) |
| \`pnpm db:studio\` | Open Drizzle Studio GUI |

---

## Docker Commands

Local PostgreSQL runs in Docker for development and testing.

| Command | Description |
|---------|-------------|
| \`pnpm docker:up\` | Start PostgreSQL container (port 5432) |
| \`pnpm docker:down\` | Stop PostgreSQL container |
| \`pnpm docker:logs\` | Stream container logs |
| \`pnpm docker:up:test\` | Start test database (port 5434) |
| \`pnpm docker:down:all\` | Stop all containers and delete volumes |
| \`pnpm docker:reset\` | Reset database (delete all data and restart) |

### Supabase Commands

| Command | Description |
|---------|-------------|
| \`pnpm supabase:setup\` | Link or create Supabase project with dev branches |
| \`pnpm supabase:branch <cmd> <name>\` | Manage database branches (create/delete/list) |
| \`pnpm supabase:env [branch]\` | Fetch credentials for a branch (default: dev) |

---

## Running Tests

Tests use a separate PostgreSQL container to avoid affecting development data.

\`\`\`bash
# Start the test database
pnpm docker:up:test

# Run all tests
pnpm test

# Run tests in watch mode
pnpm --filter web test:watch

# Run tests with UI
pnpm test:ui

# Run tests with coverage
pnpm --filter web test:coverage
\`\`\`

---

## Agent Worktree (Claude Code)

Create isolated development environments with separate databases, perfect for feature development with Claude Code.

| Command | Description |
|---------|-------------|
| \`pnpm agent <branch>\` | Create worktree with isolated database (runs Claude directly) |
| \`pnpm agent:sandbox <branch>\` | Create worktree with Docker sandbox isolation |
| \`pnpm agent:clean\` | Clean up worktree (run from within worktree) |
| \`pnpm agent:clean:sandbox\` | Clean up sandbox worktree (run from within worktree) |

### Create a Worktree

\`\`\`bash
# Default: Run Claude Code directly
pnpm agent <branch-name>

# Optional: Run Claude Code in Docker sandbox with isolated node_modules
pnpm agent:sandbox <branch-name>
\`\`\`

This will:
1. Create a git worktree for the branch
2. Creates Supabase database branches for isolated development
3. Copy environment files and Vercel config
4. Install dependencies and run migrations
5. Copy data from the main database
6. Push branch to remote
7. Open iTerm2 with 3 panes:
   - **Pane 1:** Claude Code (directly or in Docker sandbox)
   - **Pane 2:** Development terminal
   - **Pane 3:** Additional terminal

### Clean Up

From within the worktree directory:

\`\`\`bash
# For worktrees created with pnpm agent
pnpm agent:clean

# For worktrees created with pnpm agent:sandbox
pnpm agent:clean:sandbox
\`\`\`

This will:
1. Deletes Supabase branches and stops any Docker containers
2. Remove the git worktree
3. Delete the local branch
4. (Sandbox only) Stop and remove the Docker sandbox and node_modules volumes

---

## Environment Variables

Copy \`apps/web/.env.local.example\` to \`apps/web/.env.local\` and configure:

### Database
- \`DATABASE_URL\` - PostgreSQL connection string
- \`TEST_DATABASE_URL\` - Test database connection string

### Authentication
Better Auth (Email OTP) or WorkOS:
- \`BETTER_AUTH_SECRET\` - Auth encryption secret
- \`BETTER_AUTH_URL\` - Auth callback URL
- \`RESEND_API_KEY\` - Email service for OTP (get your key at [resend.com](https://resend.com))

### AI
- \`AI_GATEWAY_API_KEY\` - Vercel AI Gateway key (get your key at [vercel.com/ai-gateway](https://vercel.com/dashboard/~/ai))

### Analytics
- \`NEXT_PUBLIC_POSTHOG_KEY\` - PostHog public key
- \`NEXT_PUBLIC_POSTHOG_HOST\` - PostHog host
- \`POSTHOG_API_KEY\` - PostHog server-side key

---

## CI/CD (GitHub Actions)

| Workflow | Trigger | Description |
|----------|---------|-------------|
| \`checks.yml\` | Pull request | Runs linting and type checking |
| \`test.yml\` | Pull request | Runs Vitest tests with PostgreSQL |
| \`claude-code-review.yml\` | Pull request | AI-powered code review |
| \`claude.yml\` | \`@claude\` mention | Interactive Claude in issues/PRs |

### Claude Integration

Mention \`@claude\` in any issue or PR comment to get AI assistance:
- Code explanations
- Bug analysis
- Implementation suggestions
- Review feedback

---

## Workflows

This project includes [Vercel Workflow DevKit](https://vercel.com/docs/workflow-kit) for durable, long-running AI workflows.

### Example Workflow

The included example workflow (\`workflows/example.ts\`) demonstrates:
- Multi-step AI processing with OpenAI
- Real-time progress streaming via SSE
- Error handling and retry logic

### Progress Streaming

Workflows emit real-time progress events that the UI consumes via Server-Sent Events:

\`\`\`
Client                    Server
  │                         │
  ├─ POST /api/workflow ───►│  Start workflow, get runId
  │◄── { runId } ───────────┤
  │                         │
  ├─ GET /api/workflow-progress/[runId] ──►│
  │◄── SSE: step 1/5 ───────┤
  │◄── SSE: step 2/5 ───────┤
  │◄── SSE: completed ──────┤
\`\`\`

Key files:
- \`workflows/example.ts\` - Workflow definition with progress emits
- \`app/api/workflow/route.ts\` - Starts workflow runs
- \`app/api/workflow-progress/[runId]/route.ts\` - SSE progress stream
- \`hooks/use-workflow-progress.ts\` - React hook for consuming progress
- \`lib/workflow-progress/types.ts\` - Shared progress types

### Monitoring

View workflow runs in the browser:

\`\`\`bash
npx workflow web
\`\`\`

---

## Database Environments

This project uses Supabase with database branching for isolated environments:

| Environment | Database | Purpose |
|-------------|----------|---------|
| **Production** | Main Supabase database | Live application |
| **Preview** | Auto-created per PR | Vercel preview deployments (via Supabase Integration) |
| **Development** | \`dev\` branch | Local development (\`.env.local\`) |
| **Tests** | \`dev-test\` branch | Local test runs |
| **Worktrees** | Per-branch databases | Created by \`./scripts/wts\` |

### Preview Deployments

The Supabase Vercel Integration automatically:
1. Creates a database branch when Vercel builds a preview
2. Injects the correct \`DATABASE_URL\` into that deployment
3. Cleans up the branch when the preview is deleted

This means each PR gets its own isolated database - no conflicts between concurrent feature development.

To set up the integration (if not done during setup):
\`\`\`bash
supabase integrations create vercel
\`\`\`

Or configure it at: \`https://supabase.com/dashboard/project/<ref>/settings/integrations\`

---

## Deployment

### Vercel

The project is configured for Vercel deployment:

1. Link your project:
   \`\`\`bash
   vercel link
   \`\`\`

2. Set environment variables in Vercel dashboard (copy DATABASE_URL from Supabase)

3. Deploy:
   \`\`\`bash
   vercel --prod
   \`\`\`

Database migrations run automatically during the build process via \`vercel.json\`:
\`\`\`json
{
  "buildCommand": "pnpm db:migrate:deploy && pnpm build"
}
\`\`\`

---

## Tech Stack

- **Framework:** [Next.js 16](https://nextjs.org/) with React 19
- **Database:** [Drizzle ORM](https://orm.drizzle.team/) with PostgreSQL
- **Auth:** [Better Auth](https://www.better-auth.com/) (Email OTP via Resend)
- **AI:** [Vercel AI SDK](https://sdk.vercel.ai/) with OpenAI
- **Workflows:** [Vercel Workflow DevKit](https://vercel.com/docs/workflow-kit)
- **Styling:** [Tailwind CSS 4](https://tailwindcss.com/) + [shadcn/ui](https://ui.shadcn.com/)
- **Testing:** [Vitest](https://vitest.dev/)
- **Monorepo:** [Turborepo](https://turbo.build/repo)
- **Linting:** [Biome](https://biomejs.dev/)

---

## License

Private
"
`;

exports[`template snapshots > root templates > generateRootPackageJson 1`] = `
"{
  "name": "test-app",
  "private": true,
  "scripts": {
    "build": "turbo run build",
    "dev": "turbo run dev",
    "lint": "turbo run lint",
    "typecheck": "turbo run typecheck",
    "test": "turbo run test",
    "test:ui": "pnpm --filter web test:ui",
    "format": "biome format --write .",
    "check": "biome check .",
    "db:generate": "pnpm --filter web db:generate",
    "db:migrate": "pnpm --filter web db:migrate",
    "db:push": "pnpm --filter web db:push",
    "db:studio": "pnpm --filter web db:studio",
    "supabase:setup": "./scripts/supabase-setup",
    "supabase:branch": "./scripts/supabase-branch",
    "supabase:env": "./scripts/supabase-env",
    "docker:up": "docker compose up -d postgres",
    "docker:down": "docker compose down",
    "docker:logs": "docker compose logs -f",
    "docker:up:test": "docker compose up -d postgres-test",
    "docker:down:all": "docker compose down -v",
    "docker:reset": "docker compose down -v && docker compose up -d postgres",
    "app:setup": "./scripts/setup",
    "agent": "./scripts/wts",
    "agent:sandbox": "./scripts/wts --sandbox",
    "agent:clean": "./scripts/wtcs",
    "agent:clean:sandbox": "./scripts/wtcs --sandbox"
  },
  "devDependencies": {
    "@biomejs/biome": "^2.3.11",
    "turbo": "^2.7.4",
    "typescript": "^5.9.3"
  },
  "packageManager": "pnpm@10.28.0"
}
"
`;

exports[`template snapshots > root templates > generateTurboJson 1`] = `
"{
  "$schema": "https://turbo.build/schema.json",
  "tasks": {
    "build": {
      "dependsOn": [
        "^build"
      ],
      "outputs": [
        ".next/**",
        "!.next/cache/**",
        "dist/**"
      ]
    },
    "dev": {
      "cache": false,
      "persistent": true
    },
    "lint": {
      "dependsOn": [
        "^build"
      ]
    },
    "typecheck": {
      "dependsOn": [
        "^build"
      ]
    },
    "test": {
      "dependsOn": [
        "^build"
      ],
      "env": [
        "TEST_DATABASE_URL"
      ]
    },
    "db:generate": {
      "cache": false
    },
    "db:migrate": {
      "cache": false
    },
    "db:studio": {
      "cache": false,
      "persistent": true
    }
  }
}
"
`;

exports[`template snapshots > scripts templates > generateBuildSandbox 1`] = `
"#!/usr/bin/env bash
set -e

SCRIPT_DIR="$(cd "$(dirname "\${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
PROJECT_NAME="$(basename "$PROJECT_ROOT")"
IMAGE_NAME="\${PROJECT_NAME}-claude-sandbox"

echo "Building custom Claude sandbox image for project: $PROJECT_NAME"
docker build -t "$IMAGE_NAME" "$SCRIPT_DIR"
echo "Done! Image tagged as: $IMAGE_NAME"
"
`;

exports[`template snapshots > scripts templates > generateSandboxDockerfile 1`] = `
"FROM docker/sandbox-templates:claude-code

# Clear NPM_CONFIG_PREFIX from base image - it conflicts with nvm
# Setting to empty string effectively unsets it at the image level
ENV NPM_CONFIG_PREFIX=""

# Set proper terminal type for color support
# Note: Shift+Enter doesn't work in Docker sandbox - use \\ + Enter for newlines
ENV TERM=xterm-256color

# Use relative cache dir to avoid absolute path issues across worktrees
ENV TURBO_CACHE_DIR=".turbo/cache"

# Install nvm and Node.js LTS (jod = v22)
ENV NVM_DIR="/home/agent/.nvm"
RUN unset NPM_CONFIG_PREFIX \\
    && curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.40.1/install.sh | bash \\
    && . "$NVM_DIR/nvm.sh" \\
    && nvm install lts/jod \\
    && nvm alias default lts/jod \\
    && nvm use default

# Enable Corepack and install pnpm
RUN unset NPM_CONFIG_PREFIX \\
    && . "$NVM_DIR/nvm.sh" \\
    && corepack enable \\
    && corepack prepare pnpm@latest --activate

# Add nvm to shell startup
RUN echo 'export NVM_DIR="$HOME/.nvm"' >> ~/.bashrc \\
    && echo '[ -s "$NVM_DIR/nvm.sh" ] && \\. "$NVM_DIR/nvm.sh"' >> ~/.bashrc \\
    && echo '[ -s "$NVM_DIR/bash_completion" ] && \\. "$NVM_DIR/bash_completion"' >> ~/.bashrc
"
`;

exports[`template snapshots > scripts templates > generateSandboxSettings 1`] = `
"{
	"permissions": {
		"disableBypassPermissionsMode": "disable",
		"allow": [
			"Bash(pnpm lint:*)",
			"Bash(ls:*)",
			"Bash(pnpm test:*)",
			"Bash(pnpm typecheck:*)",
			"Bash(cat:*)",
			"Bash(pnpm --filter web typecheck:*)",
			"Bash(pnpm --filter web lint:*)",
			"Bash(pnpm --filter web test)",
			"Bash(nvm use)",
			"Bash(pnpm db:generate:*)",
			"Bash(find:*)",
			"Bash(xargs cat:*)",
			"Bash(pnpm --filter web test:*)",
			"Bash(node -e:*)",
			"Bash(node --input-type=module -e:*)",
			"Bash(grep:*)",
			"Bash(wc:*)",
			"Bash(paste:*)",
			"Bash(sort:*)",
			"Bash(git status:*)",
			"Bash(git diff:*)",
			"Bash(git log:*)",
			"Bash(git branch:*)",
			"Bash(git show:*)",
			"WebSearch"
		],
		"deny": [],
		"ask": [
			"Bash(git commit:*)",
			"Bash(git push:*)",
			"Bash(git merge:*)",
			"Bash(git rebase:*)",
			"Bash(git reset:*)",
			"Bash(git stash:*)",
			"Bash(git cherry-pick:*)"
		]
	}
}"
`;

exports[`template snapshots > scripts templates > generateWorktreeInclude 1`] = `
"apps/web/.vercel
apps/web/.env.local
"
`;

exports[`template snapshots > scripts templates > generateWtcsScript 1`] = `
"#!/usr/bin/env bash
set -e

# Parse arguments
sandbox_mode=false
if [[ "$1" == "--sandbox" ]]; then
  sandbox_mode=true
fi

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
NC='\\033[0m' # No Color

worktree_path=$(pwd)
git_dir=$(git rev-parse --git-dir 2>/dev/null)

if [[ ! "$git_dir" == *".git/worktrees"* ]]; then
  echo -e "\${RED}Error: Not in a git worktree\${NC}"
  exit 1
fi

branch_name=$(git branch --show-current)
safe_branch_name="\${branch_name//\\//-}"
repo_name=$(basename "$(git rev-parse --git-common-dir | sed 's/\\/.git$//')")
compose_project="\${repo_name}-\${safe_branch_name}"
main_repo=$(git rev-parse --git-common-dir | sed 's/\\/.git$//')

# Supabase branch names
supabase_branch_name="\${safe_branch_name}"
supabase_test_branch="\${safe_branch_name}-test"

# Get Supabase project ref
if [[ -f ".supabase/.project-ref" ]]; then
  PROJECT_REF=$(cat .supabase/.project-ref)
else
  PROJECT_REF=""
fi

echo ""
echo "This will:"
if [[ -n "$PROJECT_REF" ]]; then
  echo "  - Delete Supabase branch: $supabase_branch_name"
  echo "  - Delete Supabase branch: $supabase_test_branch"
fi
if [[ "$sandbox_mode" == "true" ]]; then
  echo "  - Stop Docker Sandbox: $compose_project (if exists)"
  echo "  - Remove sandbox node_modules volumes"
fi
echo "  - Remove worktree at: $worktree_path"
echo "  - Delete local branch: $branch_name"
echo ""
read -p "Are you sure? (y/N) " confirm

if [[ "$confirm" != [yY] ]]; then
  echo "Aborted"
  exit 0
fi

# Delete Supabase branches (must disable persistence first)
if [[ -n "$PROJECT_REF" ]]; then
  echo ""
  echo "Deleting Supabase branches..."

  echo "  Deleting '$supabase_branch_name'..."
  # Disable persistence first (required before deletion)
  supabase branches update "$supabase_branch_name" --persistent=false --project-ref "$PROJECT_REF" 2>/dev/null || true
  if supabase branches delete "$supabase_branch_name" --project-ref "$PROJECT_REF" --yes 2>/dev/null; then
    echo -e "  \${GREEN}✓ $supabase_branch_name deleted\${NC}"
  else
    echo -e "  \${YELLOW}⚠ $supabase_branch_name may not exist or failed to delete\${NC}"
  fi

  echo "  Deleting '$supabase_test_branch'..."
  # Disable persistence first (required before deletion)
  supabase branches update "$supabase_test_branch" --persistent=false --project-ref "$PROJECT_REF" 2>/dev/null || true
  if supabase branches delete "$supabase_test_branch" --project-ref "$PROJECT_REF" --yes 2>/dev/null; then
    echo -e "  \${GREEN}✓ $supabase_test_branch deleted\${NC}"
  else
    echo -e "  \${YELLOW}⚠ $supabase_test_branch may not exist or failed to delete\${NC}"
  fi
else
  echo -e "\${YELLOW}No Supabase project configured - skipping branch deletion\${NC}"
fi

if [[ "$sandbox_mode" == "true" ]]; then
  # Remove Docker Sandbox if it exists (search by name)
  # Note: docker sandbox ls doesn't support --format, so we parse the table output
  # Format: SANDBOX ID | TEMPLATE | NAME | WORKSPACE | STATUS | CREATED
  echo ""
  echo "Cleaning up Docker Sandbox..."
  sandbox_id=$(docker sandbox ls --no-trunc 2>/dev/null | awk -v name="$compose_project" '$3 == name {print $1}')
  if [[ -n "$sandbox_id" ]]; then
    echo "  Removing Docker Sandbox: $sandbox_id ($compose_project)"
    docker sandbox rm "$sandbox_id" 2>/dev/null || true
    echo -e "  \${GREEN}✓ Docker Sandbox removed\${NC}"
  else
    echo "  No Docker Sandbox found with name: $compose_project"
  fi

  # Remove sandbox node_modules volumes
  echo ""
  echo "🗑️  Removing sandbox node_modules volumes..."
  docker volume rm "\${compose_project}_node_modules" 2>/dev/null || true
  docker volume rm "\${compose_project}_web_node_modules" 2>/dev/null || true
  docker volume rm "\${compose_project}_ui_node_modules" 2>/dev/null || true
  echo -e "  \${GREEN}✓ Node_modules volumes removed\${NC}"

  # Clean up node_modules directories (may have root ownership from Docker)
  echo "🧹 Cleaning up node_modules directories..."
  docker run --rm -v "$worktree_path:/workspace" alpine rm -rf /workspace/node_modules /workspace/apps/web/node_modules /workspace/packages/ui/node_modules 2>/dev/null || true
fi

# Remove worktree and branch
echo ""
echo "Removing git worktree and branch..."
cd "$main_repo"
git worktree remove "$worktree_path" --force
echo -e "  \${GREEN}✓ Worktree removed\${NC}"

git branch -D "$branch_name"
echo -e "  \${GREEN}✓ Branch deleted\${NC}"

cd ..

echo ""
echo "========================================"
echo -e "\${GREEN}       Cleanup Complete!\${NC}"
echo "========================================"
echo ""
echo "Cleaned up:"
if [[ -n "$PROJECT_REF" ]]; then
  echo "  - Supabase branches: $supabase_branch_name, $supabase_test_branch"
fi
if [[ "$sandbox_mode" == "true" ]]; then
  echo "  - Docker Sandbox: $compose_project"
  echo "  - Node_modules volumes"
fi
echo "  - Worktree: $worktree_path"
echo "  - Branch: $branch_name"
"
`;

exports[`template snapshots > scripts templates > generateWtsScript 1`] = `
"#!/usr/bin/env bash
set -e

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
NC='\\033[0m' # No Color

# Load nvm if available
export NVM_DIR="$HOME/.nvm"
[ -s "$NVM_DIR/nvm.sh" ] && \\. "$NVM_DIR/nvm.sh"

# Parse arguments
sandbox_mode=false
branch_name=""
while [[ $# -gt 0 ]]; do
  case $1 in
    --sandbox) sandbox_mode=true; shift ;;
    *) branch_name="$1"; shift ;;
  esac
done

if [[ -z "$branch_name" ]]; then
  echo "Usage: ./scripts/wts [--sandbox] <branch-name>"
  exit 1
fi
original_dir=$(pwd)
repo_name=$(basename "$original_dir")
safe_branch_name="\${branch_name//\\//-}"
worktree_path="../\${repo_name}-\${safe_branch_name}"
sandbox_image="\${repo_name}-claude-sandbox"

# Check if custom sandbox image exists, build if not (only for sandbox mode)
if [[ "$sandbox_mode" == "true" ]]; then
  if ! docker image inspect "$sandbox_image" &>/dev/null; then
    echo "Custom sandbox image not found. Building $sandbox_image..."
    "$original_dir/scripts/sandbox/build-sandbox"
  fi
fi

# Check for Supabase project configuration
if [[ ! -f ".supabase/.project-ref" ]]; then
  echo -e "\${RED}Error: Supabase project not configured.\${NC}"
  echo "Run: pnpm supabase:setup"
  exit 1
fi

PROJECT_REF=$(cat .supabase/.project-ref)
env_file="apps/web/.env.local"

# Supabase branch names
supabase_branch_name="\${safe_branch_name}"
supabase_test_branch="\${safe_branch_name}-test"

echo ""
echo "Creating worktree sandbox: $branch_name"
echo "  Supabase branches: $supabase_branch_name, $supabase_test_branch"
echo ""

git fetch origin main

if ! git worktree add -b "$branch_name" "$worktree_path" origin/main; then
  echo -e "\${RED}Failed to create worktree\${NC}"
  exit 1
fi

# Copy .worktreeinclude files
if [[ -f ".worktreeinclude" ]]; then
  while IFS= read -r file || [[ -n "$file" ]]; do
    [[ -z "$file" || "$file" =~ ^# ]] && continue
    file="\${file%/}"
    if [[ -e "$file" ]]; then
      mkdir -p "$worktree_path/$(dirname "$file")"
      cp -r "$file" "$worktree_path/$file"
      echo "Copied: $file"
    fi
  done < ".worktreeinclude"
fi

# Copy Claude settings for non-sandbox mode (disables dangerous mode, blocks destructive git commands)
# In sandbox mode, Claude runs in dangerous mode since the Docker sandbox provides isolation
if [[ "$sandbox_mode" != "true" ]] && [[ -f ".claude/sandbox.settings.local.json" ]]; then
  mkdir -p "$worktree_path/.claude"
  cp ".claude/sandbox.settings.local.json" "$worktree_path/.claude/settings.local.json"
  echo "Copied: Claude settings -> .claude/settings.local.json"
fi

# Copy Supabase project reference
mkdir -p "$worktree_path/.supabase"
cp ".supabase/.project-ref" "$worktree_path/.supabase/.project-ref"
echo "Copied: Supabase project reference"

cd "$worktree_path"

echo ""
echo "Creating Supabase branches..."

# Create main dev branch
echo "  Creating '$supabase_branch_name' branch..."
if supabase branches create "$supabase_branch_name" --persistent --project-ref "$PROJECT_REF" 2>/dev/null; then
  echo -e "  \${GREEN}✓ $supabase_branch_name created\${NC}"
else
  echo -e "  \${YELLOW}⚠ Branch may already exist or creation failed\${NC}"
fi

# Create test branch
echo "  Creating '$supabase_test_branch' branch..."
if supabase branches create "$supabase_test_branch" --persistent --project-ref "$PROJECT_REF" 2>/dev/null; then
  echo -e "  \${GREEN}✓ $supabase_test_branch created\${NC}"
else
  echo -e "  \${YELLOW}⚠ Branch may already exist or creation failed\${NC}"
fi

echo ""
echo "Waiting for branches to be provisioned (this may take 30-60 seconds)..."

# Poll for branch readiness with retries
max_attempts=12
attempt=0
branches_ready=false

while [[ $attempt -lt $max_attempts ]]; do
  attempt=$((attempt + 1))
  echo "  Checking branch status (attempt $attempt/$max_attempts)..."

  # Try to get credentials
  if eval "$(supabase branches get "$supabase_branch_name" --project-ref "$PROJECT_REF" -o env 2>/dev/null)"; then
    if [[ -n "$POSTGRES_URL" ]]; then
      branches_ready=true
      break
    fi
  fi

  sleep 10
done

if [[ "$branches_ready" != "true" ]]; then
  echo -e "\${YELLOW}Warning: Could not verify branch readiness. Continuing anyway...\${NC}"
fi

echo ""
echo "Installing dependencies..."
if command -v nvm &> /dev/null; then
  nvm use 2>/dev/null || true
fi
pnpm i

# Fetch credentials and update .env.local
echo ""
echo "Fetching branch credentials..."

# Get main branch URL
eval "$(supabase branches get "$supabase_branch_name" --project-ref "$PROJECT_REF" -o env 2>/dev/null)" || true
new_db_url="$POSTGRES_URL"

# Get test branch URL
eval "$(supabase branches get "$supabase_test_branch" --project-ref "$PROJECT_REF" -o env 2>/dev/null)" || true
new_test_db_url="$POSTGRES_URL"

# Update .env.local
worktree_env_file="$env_file"

if [[ -n "$new_db_url" ]]; then
  if [[ -f "$worktree_env_file" ]]; then
    if grep -q "^DATABASE_URL=" "$worktree_env_file"; then
      sed -i '' "s|^DATABASE_URL=.*|DATABASE_URL=\\"$new_db_url\\"|" "$worktree_env_file"
    else
      echo "DATABASE_URL=\\"$new_db_url\\"" >> "$worktree_env_file"
    fi
  else
    mkdir -p "$(dirname "$worktree_env_file")"
    echo "DATABASE_URL=\\"$new_db_url\\"" > "$worktree_env_file"
  fi
  echo -e "  \${GREEN}✓ DATABASE_URL configured\${NC}"
else
  echo -e "  \${YELLOW}⚠ Could not get DATABASE_URL - branch may still be provisioning\${NC}"
fi

if [[ -n "$new_test_db_url" ]]; then
  if grep -q "^TEST_DATABASE_URL=" "$worktree_env_file" 2>/dev/null; then
    sed -i '' "s|^TEST_DATABASE_URL=.*|TEST_DATABASE_URL=\\"$new_test_db_url\\"|" "$worktree_env_file"
  else
    echo "TEST_DATABASE_URL=\\"$new_test_db_url\\"" >> "$worktree_env_file"
  fi
  echo -e "  \${GREEN}✓ TEST_DATABASE_URL configured\${NC}"
else
  echo -e "  \${YELLOW}⚠ Could not get TEST_DATABASE_URL - branch may still be provisioning\${NC}"
fi

# Apply migrations to branches
echo ""
echo "Applying migrations to branches..."

if [[ -n "$new_db_url" ]]; then
  echo "  Pushing to $supabase_branch_name..."
  cd apps/web
  DATABASE_URL="$new_db_url" pnpm db:push 2>/dev/null && echo -e "  \${GREEN}✓ Migrations applied\${NC}" || echo -e "  \${YELLOW}⚠ Migration push failed\${NC}"
  cd ../..
fi

if [[ -n "$new_test_db_url" ]]; then
  echo "  Pushing to $supabase_test_branch..."
  cd apps/web
  DATABASE_URL="$new_test_db_url" pnpm db:push 2>/dev/null && echo -e "  \${GREEN}✓ Migrations applied\${NC}" || echo -e "  \${YELLOW}⚠ Migration push failed\${NC}"
  cd ../..
fi

# Push branch to remote
git push -u origin "$branch_name"

echo ""
echo "========================================"
echo -e "\${GREEN}       Worktree Ready!\${NC}"
echo "========================================"
echo ""
echo "   Path:           $(pwd)"
echo "   Branch:         $branch_name"
echo "   Supabase Dev:   $supabase_branch_name"
echo "   Supabase Test:  $supabase_test_branch"
echo ""

# Launch iTerm2 with 3-pane layout
compose_project="\${repo_name}-\${safe_branch_name}"
sandbox_name="\${compose_project}"
worktree_dir="$(pwd)"

if [[ "$sandbox_mode" == "true" ]]; then
  echo "Launching iTerm2 with Claude Code sandbox and dev terminals..."

  # Pre-create node_modules and turbo volumes with correct ownership (agent user UID=1000)
  echo "📦 Creating node_modules and turbo volumes with correct ownership..."
  docker volume create "\${sandbox_name}_node_modules" >/dev/null 2>&1 || true
  docker volume create "\${sandbox_name}_web_node_modules" >/dev/null 2>&1 || true
  docker volume create "\${sandbox_name}_ui_node_modules" >/dev/null 2>&1 || true
  docker run --rm \\
    -v "\${sandbox_name}_node_modules:/mnt/root" \\
    -v "\${sandbox_name}_web_node_modules:/mnt/web" \\
    -v "\${sandbox_name}_ui_node_modules:/mnt/ui" \\
    alpine chown -R 1000:1000 /mnt/root /mnt/web /mnt/ui

  osascript <<APPLESCRIPT
tell application "iTerm2"
    create window with default profile
    tell current window
        tell current session
            set name to "Claude Sandbox"
            write text "cd '$worktree_dir' && docker sandbox run --template '$sandbox_image' --name '$sandbox_name' --mount-docker-socket -v '$HOME/.claude:/home/agent/.claude' -v '$original_dir/.git:$original_dir/.git' -v '\${sandbox_name}_node_modules:$worktree_dir/node_modules' -v '\${sandbox_name}_web_node_modules:$worktree_dir/apps/web/node_modules' -v '\${sandbox_name}_ui_node_modules:$worktree_dir/packages/ui/node_modules' -w '$worktree_dir' claude"

            -- Split vertically to create right pane
            set rightPane to (split vertically with default profile)
            tell rightPane
                set name to "Dev Terminal"
                write text "cd '$worktree_dir' && nvm use 2>/dev/null; echo 'Dev terminal ready'"

                -- Split horizontally to create bottom-right pane
                set bottomPane to (split horizontally with default profile)
                tell bottomPane
                    set name to "Terminal 2"
                    write text "cd '$worktree_dir' && nvm use 2>/dev/null; echo 'Terminal 2 ready'"
                end tell
            end tell
        end tell
    end tell
end tell
APPLESCRIPT

  echo "iTerm2 launched with 3-pane layout (sandbox mode)"
else
  echo "Launching iTerm2 with Claude Code and dev terminals..."

  osascript <<APPLESCRIPT
tell application "iTerm2"
    create window with default profile
    tell current window
        tell current session
            set name to "Claude"
            write text "cd '$worktree_dir' && claude"

            -- Split vertically to create right pane
            set rightPane to (split vertically with default profile)
            tell rightPane
                set name to "Dev Terminal"
                write text "cd '$worktree_dir' && nvm use 2>/dev/null; echo 'Dev terminal ready'"

                -- Split horizontally to create bottom-right pane
                set bottomPane to (split horizontally with default profile)
                tell bottomPane
                    set name to "Terminal 2"
                    write text "cd '$worktree_dir' && nvm use 2>/dev/null; echo 'Terminal 2 ready'"
                end tell
            end tell
        end tell
    end tell
end tell
APPLESCRIPT

  echo "iTerm2 launched with 3-pane layout"
fi
"
`;

exports[`template snapshots > services templates > generateServicesIndex 1`] = `
"export * from "./user";
"
`;

exports[`template snapshots > services templates > generateUserService 1`] = `
"import { cache } from "react";
import { headers } from "next/headers";
import { auth } from "@/lib/auth";
// Uncomment when adding database queries:
// import { db } from "@/db";
// import { eq } from "drizzle-orm";
// import { users } from "@/db/schema";

/**
 * Get the current session
 *
 * Uses React cache() to deduplicate requests within a single render.
 */
export const getSession = cache(async () => {
	const session = await auth.api.getSession({
		headers: await headers(),
	});
	return session;
});

/**
 * Get the current authenticated user's ID
 *
 * Returns null if not authenticated.
 */
export const getCurrentUserId = cache(async (): Promise<string | null> => {
	const session = await getSession();
	return session?.user?.id ?? null;
});

/**
 * Get the current authenticated user
 *
 * Returns the user object from the session.
 */
export const getCurrentUser = cache(async () => {
	const session = await getSession();
	if (!session?.user) return null;

	return {
		id: session.user.id,
		email: session.user.email,
		name: session.user.name,
		image: session.user.image,
	};
});

/**
 * Example: Get a user by ID from the database
 * Uncomment and modify once you have a users table in your schema
 */
// export async function getUserById(id: string) {
//   const result = await db
//     .select()
//     .from(users)
//     .where(eq(users.id, id))
//     .limit(1);
//   return result[0] || null;
// }
"
`;

exports[`template snapshots > setup templates > generateSetupScript 1`] = `
"#!/usr/bin/env bash
set -e

# =============================================================================
# Project Setup Script
# Generated by Hatch CLI
# =============================================================================

# Colors for output
RED='\\033[0;31m'
GREEN='\\033[0;32m'
YELLOW='\\033[1;33m'
BLUE='\\033[0;34m'
NC='\\033[0m' # No Color

# Project configuration (embedded at generation time)
PROJECT_NAME="undefined"
USE_DOCKER=false
USE_WORKOS=false

# State tracking file for idempotency
STATE_FILE=".setup-state"

# =============================================================================
# Helper Functions
# =============================================================================

print_header() {
  echo ""
  echo "========================================"
  echo -e "\${BLUE}$1\${NC}"
  echo "========================================"
  echo ""
}

print_step() {
  echo -e "\${BLUE}→\${NC} $1"
}

print_success() {
  echo -e "\${GREEN}✓\${NC} $1"
}

print_warning() {
  echo -e "\${YELLOW}⚠\${NC} $1"
}

print_error() {
  echo -e "\${RED}✗\${NC} $1"
}

# State management for idempotency
mark_completed() {
  echo "$1" >> "$STATE_FILE"
}

is_completed() {
  [[ -f "$STATE_FILE" ]] && grep -q "^$1$" "$STATE_FILE"
}

# Construct pooler DATABASE_URL from saved credentials (for runtime/serverless)
# Returns empty string if credentials not available
get_pooler_database_url() {
  local project_ref="$1"

  if [[ ! -f ".supabase/.db-password" ]] || [[ ! -f ".supabase/.region" ]]; then
    echo ""
    return
  fi

  local db_password=$(cat .supabase/.db-password)
  local region=$(cat .supabase/.region)

  # Use transaction pooler (port 6543) for serverless compatibility
  echo "postgresql://postgres.\${project_ref}:\${db_password}@aws-0-\${region}.pooler.supabase.com:6543/postgres"
}

# Construct DATABASE_URL for DDL operations (migrations, schema push)
# Uses session pooler (aws-1, port 5432) which supports DDL and is IPv4 compatible
get_direct_database_url() {
  local project_ref="$1"

  if [[ ! -f ".supabase/.db-password" ]] || [[ ! -f ".supabase/.region" ]]; then
    echo ""
    return
  fi

  local db_password=$(cat .supabase/.db-password)
  local region=$(cat .supabase/.region)

  # Use session pooler (aws-1, port 5432) for DDL operations
  # Note: aws-0 is transaction pooler (port 6543) which doesn't support DDL
  # Note: db.{ref}.supabase.co is IPv6-only and doesn't resolve on many networks
  echo "postgresql://postgres.\${project_ref}:\${db_password}@aws-1-\${region}.pooler.supabase.com:5432/postgres"
}

# =============================================================================
# Step 1: Check Prerequisites
# =============================================================================

check_prerequisites() {
  print_header "Checking Prerequisites"

  local missing=()

  # Check gh CLI
  if command -v gh &> /dev/null; then
    print_success "GitHub CLI (gh) installed"
  else
    missing+=("gh")
    print_error "GitHub CLI (gh) not installed"
  fi

  # Check vercel CLI
  if command -v vercel &> /dev/null; then
    print_success "Vercel CLI installed"
  else
    missing+=("vercel")
    print_error "Vercel CLI not installed"
  fi

  # Check supabase CLI (only if not using Docker)
  if [[ "$USE_DOCKER" != "true" ]]; then
    if command -v supabase &> /dev/null; then
      print_success "Supabase CLI installed"
    else
      missing+=("supabase")
      print_error "Supabase CLI not installed"
    fi
  fi

  # Check jq (needed for parsing JSON)
  if command -v jq &> /dev/null; then
    print_success "jq installed"
  else
    missing+=("jq")
    print_error "jq not installed"
  fi

  if [[ \${#missing[@]} -gt 0 ]]; then
    echo ""
    print_error "Missing required tools: \${missing[*]}"
    echo ""
    echo "Install them with:"
    for tool in "\${missing[@]}"; do
      case $tool in
        gh)
          echo "  brew install gh"
          ;;
        vercel)
          echo "  npm i -g vercel"
          ;;
        supabase)
          echo "  brew install supabase/tap/supabase"
          ;;
        jq)
          echo "  brew install jq"
          ;;
      esac
    done
    exit 1
  fi

  print_success "All prerequisites satisfied"
}

# =============================================================================
# Step 2: Authenticate with GitHub
# =============================================================================

auth_github() {
  print_header "GitHub Authentication"

  # Check if already authenticated
  if gh auth status &> /dev/null; then
    print_success "Already authenticated with GitHub"
    return 0
  fi

  print_step "Opening browser for GitHub authentication..."
  gh auth login --web

  if gh auth status &> /dev/null; then
    print_success "GitHub authentication successful"
  else
    print_error "GitHub authentication failed"
    exit 1
  fi
}

# =============================================================================
# Step 3: Authenticate with Supabase (skipped if USE_DOCKER=true)
# =============================================================================

auth_supabase() {
  if [[ "$USE_DOCKER" == "true" ]]; then
    print_step "Skipping Supabase authentication (Docker mode)"
    return 0
  fi

  print_header "Supabase Authentication"

  # Check if already authenticated
  if supabase projects list &> /dev/null 2>&1; then
    print_success "Already authenticated with Supabase"
    return 0
  fi

  print_step "Opening browser for Supabase authentication..."
  supabase login

  if supabase projects list &> /dev/null 2>&1; then
    print_success "Supabase authentication successful"
  else
    print_error "Supabase authentication failed"
    exit 1
  fi
}

# =============================================================================
# Step 4: Authenticate with Vercel
# =============================================================================

auth_vercel() {
  print_header "Vercel Authentication"

  # Check if already authenticated
  if vercel whoami &> /dev/null 2>&1; then
    print_success "Already authenticated with Vercel"
    return 0
  fi

  print_step "Opening browser for Vercel authentication..."
  vercel login

  if vercel whoami &> /dev/null 2>&1; then
    print_success "Vercel authentication successful"
  else
    print_error "Vercel authentication failed"
    exit 1
  fi
}

# =============================================================================
# Step 4b: Select Vercel Team/Account
# =============================================================================

select_vercel_team() {
  print_header "Vercel Team Selection"

  print_step "Select which Vercel account/team to use for this project..."
  echo ""

  # vercel switch without arguments shows interactive picker
  vercel switch

  # Show which team was selected
  local current_team=$(vercel whoami 2>/dev/null)
  if [[ -n "$current_team" ]]; then
    print_success "Using Vercel account: $current_team"
  fi
}

# =============================================================================
# Step 5: Create GitHub Repository
# =============================================================================

create_github_repo() {
  print_header "GitHub Repository Setup"

  # Check if remote 'origin' already exists locally
  if git remote get-url origin &> /dev/null 2>&1; then
    print_success "Git remote 'origin' already configured"
    # Verify we can access the remote
    if git ls-remote origin &> /dev/null 2>&1; then
      print_success "Remote repository accessible"
    else
      print_warning "Remote exists but may not be accessible"
    fi
    return 0
  fi

  if is_completed "github_repo"; then
    print_success "GitHub repository already created (from previous run)"
    return 0
  fi

  # Check if repo already exists on GitHub before creating
  if gh repo view "$PROJECT_NAME" &> /dev/null 2>&1; then
    print_warning "Repository '$PROJECT_NAME' already exists on GitHub"
    echo ""
    echo "Options:"
    echo "  1. Link to existing repository (won't push - manual sync required)"
    echo "  2. Use a different name"
    echo "  3. Cancel setup"
    echo ""
    read -p "Choice (1/2/3): " choice

    case $choice in
      1)
        # Link to existing - DO NOT push (would overwrite remote)
        local gh_user=$(gh api user -q .login 2>/dev/null)
        git remote add origin "https://github.com/$gh_user/$PROJECT_NAME.git"
        print_success "Linked to existing repository"
        print_warning "Code NOT pushed to avoid overwriting. Sync manually if needed."
        mark_completed "github_repo"
        return 0
        ;;
      2)
        read -p "Enter new repository name: " new_name
        print_step "Creating private GitHub repository: $new_name"
        if gh repo create "$new_name" --private --source=. --push; then
          print_success "GitHub repository '$new_name' created and code pushed"
          mark_completed "github_repo"
        else
          print_error "Failed to create repository"
          exit 1
        fi
        return 0
        ;;
      3)
        print_step "Cancelled by user"
        exit 1
        ;;
      *)
        print_error "Invalid choice"
        exit 1
        ;;
    esac
  fi

  # Safe to create - repo doesn't exist
  print_step "Creating private GitHub repository: $PROJECT_NAME"

  if gh repo create "$PROJECT_NAME" --private --source=. --push; then
    print_success "GitHub repository created and code pushed"
    mark_completed "github_repo"
  else
    print_error "Failed to create repository"
    echo ""
    echo "This might happen if:"
    echo "  - Repository name is already taken"
    echo "  - You don't have permission to create repos"
    echo ""
    read -p "Would you like to link to an existing repository? (y/N) " link_existing
    if [[ "$link_existing" == [yY] ]]; then
      read -p "Enter repository (e.g., username/repo): " repo_name
      git remote add origin "https://github.com/$repo_name.git"
      git push -u origin main 2>/dev/null || git push -u origin master 2>/dev/null || true
      mark_completed "github_repo"
    else
      exit 1
    fi
  fi
}

# =============================================================================
# Step 6: Create Supabase Project (skipped if USE_DOCKER=true)
# =============================================================================

create_supabase_project() {
  if [[ "$USE_DOCKER" == "true" ]]; then
    print_step "Skipping Supabase project creation (Docker mode)"
    return 0
  fi

  print_header "Supabase Project Setup"

  # Check if already linked
  if [[ -f ".supabase/.project-ref" ]]; then
    local existing_ref=$(cat .supabase/.project-ref)
    print_success "Already linked to Supabase project: $existing_ref"
    return 0
  fi

  if is_completed "supabase_project"; then
    print_success "Supabase project already configured (from previous run)"
    return 0
  fi

  # Check if project name already exists in user's Supabase account
  local existing_project=$(supabase projects list -o json 2>/dev/null | jq -r ".[] | select(.name == \\"$PROJECT_NAME\\") | .id" 2>/dev/null || echo "")

  if [[ -n "$existing_project" ]]; then
    print_warning "Supabase project '$PROJECT_NAME' already exists (ref: $existing_project)"
    echo ""
    echo "Options:"
    echo "  1. Link to existing project"
    echo "  2. Create with different name"
    echo "  3. Cancel setup"
    echo ""
    read -p "Choice (1/2/3): " choice

    case $choice in
      1)
        PROJECT_REF="$existing_project"
        print_success "Using existing project: $PROJECT_REF"
        ;;
      2)
        read -p "Enter new project name: " new_name
        create_new_supabase_project "$new_name"
        return $?
        ;;
      3)
        print_step "Cancelled by user"
        exit 1
        ;;
      *)
        print_error "Invalid choice"
        exit 1
        ;;
    esac
  else
    create_new_supabase_project "$PROJECT_NAME"
  fi

  # Save project ref
  mkdir -p .supabase
  echo "$PROJECT_REF" > .supabase/.project-ref

  mark_completed "supabase_project"
  print_success "Supabase project configured: $PROJECT_REF"
}

create_new_supabase_project() {
  local proj_name="$1"

  print_step "Creating new Supabase project: $proj_name"

  # Get organization
  echo ""
  echo "Available organizations:"
  supabase orgs list
  echo ""
  read -p "Enter organization ID: " org_id

  if [[ -z "$org_id" ]]; then
    print_error "Organization ID is required"
    exit 1
  fi

  # Generate a secure database password
  local db_password=$(openssl rand -base64 24 | tr -dc 'a-zA-Z0-9' | head -c 20)

  # Create project interactively (lets CLI handle region selection)
  echo ""
  print_step "Creating project (this may take 1-2 minutes)..."
  echo ""
  echo "  The Supabase CLI will prompt you to select a region."
  echo ""

  # Run interactively - do NOT capture output or it breaks the region prompt
  if supabase projects create "$proj_name" --org-id "$org_id" --db-password "$db_password"; then
    print_success "Supabase project created"

    # Save password for later DATABASE_URL construction
    mkdir -p .supabase
    echo "$db_password" > .supabase/.db-password
    chmod 600 .supabase/.db-password

    # Look up the project reference and region
    print_step "Fetching project details..."

    local max_attempts=10
    local attempt=0

    while [[ $attempt -lt $max_attempts ]]; do
      attempt=$((attempt + 1))
      echo "  Checking project status (attempt $attempt/$max_attempts)..."

      local project_info=$(supabase projects list -o json 2>/dev/null | jq -r ".[] | select(.name == \\"$proj_name\\")" 2>/dev/null || echo "")

      if [[ -n "$project_info" ]]; then
        PROJECT_REF=$(echo "$project_info" | jq -r '.id')
        local region=$(echo "$project_info" | jq -r '.region')
        echo "$region" > .supabase/.region
        print_success "Project is ready: $PROJECT_REF (region: $region)"
        break
      fi

      sleep 10
    done

    if [[ -z "$PROJECT_REF" ]]; then
      print_warning "Could not automatically get project reference"
      echo ""
      echo "Please find your project reference at: https://supabase.com/dashboard"
      echo "Go to: Project Settings > General > Reference ID"
      read -p "Enter project reference: " PROJECT_REF
    fi
  else
    print_warning "Could not create project via CLI"
    echo ""
    echo "This can happen if:"
    echo "  - The Supabase CLI has issues fetching available regions"
    echo "  - Project name contains invalid characters (use lowercase, numbers, hyphens only)"
    echo "  - You've reached your project limit for this organization"
    echo ""
    echo "Let's create the project via the dashboard instead."
    echo ""

    # Open dashboard
    open "https://supabase.com/dashboard/new/$org_id" 2>/dev/null || echo "  Open: https://supabase.com/dashboard/new/$org_id"

    echo ""
    echo "  Steps:"
    echo "    1. Name your project: $proj_name"
    echo "    2. Generate or enter a database password"
    echo "    3. Select a region close to your users"
    echo "    4. Click 'Create new project'"
    echo "    5. Wait for the project to be ready"
    echo "    6. Go to Project Settings > General > Reference ID"
    echo ""
    read -p "Enter the project reference ID: " PROJECT_REF
  fi

  if [[ -z "$PROJECT_REF" ]]; then
    print_error "Project reference is required"
    exit 1
  fi
}

# =============================================================================
# Step 7: Link Supabase Project (skipped if USE_DOCKER=true)
# =============================================================================

link_supabase() {
  if [[ "$USE_DOCKER" == "true" ]]; then
    print_step "Skipping Supabase linking (Docker mode)"
    return 0
  fi

  print_header "Linking Supabase Project"

  if [[ ! -f ".supabase/.project-ref" ]]; then
    print_error "No Supabase project configured"
    exit 1
  fi

  local project_ref=$(cat .supabase/.project-ref)

  print_step "Linking to project: $project_ref"
  supabase link --project-ref "$project_ref"

  print_success "Supabase project linked"
}

# =============================================================================
# Step 8: Run Database Migrations (skipped if USE_DOCKER=true)
# =============================================================================

run_migrations() {
  if [[ "$USE_DOCKER" == "true" ]]; then
    print_step "Skipping Supabase migrations (Docker mode)"
    return 0
  fi

  print_header "Running Database Migrations"

  if [[ ! -f ".supabase/.project-ref" ]]; then
    print_error "No Supabase project configured"
    exit 1
  fi

  local project_ref=$(cat .supabase/.project-ref)

  # Wait for database to be ACTIVE_HEALTHY
  print_step "Waiting for database to be ready..."
  local max_attempts=12
  local attempt=0

  while [[ $attempt -lt $max_attempts ]]; do
    attempt=$((attempt + 1))
    local status=$(supabase projects list -o json 2>/dev/null | jq -r ".[] | select(.id == \\"$project_ref\\") | .status" 2>/dev/null || echo "")

    if [[ "$status" == "ACTIVE_HEALTHY" ]]; then
      print_success "Database is ready"
      # Give pooler endpoints a moment to become fully available
      sleep 5
      break
    fi

    echo "  Waiting for database to provision (attempt $attempt/$max_attempts, status: \${status:-unknown})..."
    sleep 60
  done

  # Get direct DATABASE_URL for migrations (DDL requires direct connection, not pooler)
  local prod_db_url=$(get_direct_database_url "$project_ref")

  if [[ -z "$prod_db_url" ]]; then
    print_warning "Database credentials not available - skipping migrations"
    echo "  You can run migrations manually later:"
    echo "  cd apps/web && pnpm db:generate"
    echo "  DATABASE_URL='your-url' pnpm db:migrate"
    echo ""
    echo "  Get your DATABASE_URL from:"
    echo "  https://supabase.com/dashboard/project/$project_ref/settings/database"
    return 0
  fi

  cd apps/web

  # Generate initial migration files first
  print_step "Generating initial database migration..."
  if pnpm db:generate; then
    print_success "Initial migration generated"
  else
    echo ""
    print_warning "Could not generate migration - you may need to run: pnpm db:generate"
  fi

  # Run migrations (not db:push) so Drizzle records them as applied
  # This prevents "relation already exists" errors on deploy
  print_step "Applying migrations to production database..."
  if DATABASE_URL="$prod_db_url" pnpm db:migrate; then
    print_success "Database migrations applied successfully"
  else
    echo ""
    print_warning "Migration failed - you may need to run it manually"
    echo "  cd apps/web && DATABASE_URL='<your-url>' pnpm db:migrate"
  fi
  cd ../..
}

# =============================================================================
# Step 9: Create/Link Vercel Project
# =============================================================================

setup_vercel() {
  print_header "Vercel Project Setup"

  # Check if already linked (in apps/web where the Next.js app lives)
  if [[ -f "apps/web/.vercel/project.json" ]]; then
    print_success "Already linked to a Vercel project"
    return 0
  fi

  if is_completed "vercel_project"; then
    print_success "Vercel project already configured (from previous run)"
    return 0
  fi

  print_step "Setting up Vercel project..."

  # Run vercel link with project name and --yes to skip prompts
  cd apps/web
  vercel link --project "$PROJECT_NAME" --yes

  if [[ -f ".vercel/project.json" ]]; then
    print_success "Vercel project linked: $PROJECT_NAME"

    # Set root directory via Vercel API (CLI doesn't support this)
    local project_id=$(jq -r '.projectId' .vercel/project.json)
    local vercel_token=""

    # Try to get token from Vercel CLI config
    # macOS stores in Application Support, Linux in .config
    if [[ -f "$HOME/Library/Application Support/com.vercel.cli/auth.json" ]]; then
      vercel_token=$(jq -r '.token // empty' "$HOME/Library/Application Support/com.vercel.cli/auth.json" 2>/dev/null)
    elif [[ -f "$HOME/.config/com.vercel.cli/auth.json" ]]; then
      vercel_token=$(jq -r '.token // empty' "$HOME/.config/com.vercel.cli/auth.json" 2>/dev/null)
    fi

    if [[ -n "$vercel_token" && -n "$project_id" ]]; then
      print_step "Setting Vercel root directory to apps/web..."
      if curl -s -X PATCH "https://api.vercel.com/v9/projects/$project_id" \\
        -H "Authorization: Bearer $vercel_token" \\
        -H "Content-Type: application/json" \\
        -d '{"rootDirectory": "apps/web"}' > /dev/null; then
        print_success "Root directory configured"
      else
        print_warning "Could not set root directory - set it manually in Vercel dashboard"
      fi
    else
      print_warning "Could not set root directory automatically - set apps/web in Vercel dashboard"
    fi

    # Connect Git repository for automatic deployments
    local git_url=$(git remote get-url origin 2>/dev/null || echo "")
    if [[ -n "$git_url" ]]; then
      print_step "Connecting Git repository..."
      if vercel git connect "$git_url" 2>/dev/null; then
        print_success "Git repository connected"
      else
        print_warning "Could not auto-connect Git - connect manually in Vercel dashboard"
      fi
    fi
    mark_completed "vercel_project"
  else
    print_warning "Vercel project may not be fully configured"
  fi

  cd ../..
}

# =============================================================================
# Step 9b: Configure Auth Secrets
# =============================================================================

setup_auth_secrets() {
  if [[ ! -f "apps/web/.vercel/project.json" ]]; then
    print_warning "Vercel project not linked, skipping auth secret setup"
    return 0
  fi

  if [[ "$USE_WORKOS" == "true" ]]; then
    setup_workos_secrets
  else
    setup_better_auth_secrets
  fi
}

setup_better_auth_secrets() {
  print_header "Configuring Better Auth Secrets"

  print_step "Generating unique secrets for each environment..."

  local prod_secret=$(openssl rand -base64 32)
  local preview_secret=$(openssl rand -base64 32)
  local dev_secret=$(openssl rand -base64 32)

  # Helper to set env var (removes existing first to avoid --force issues)
  set_vercel_env() {
    local name="$1"
    local env="$2"
    local value="$3"
    local sensitive="$4"

    # Remove existing var if present (ignore errors)
    vercel env rm "$name" "$env" --cwd apps/web --yes 2>/dev/null || true

    # Add the new value
    local flags=""
    if [[ "$sensitive" == "true" ]]; then
      flags="--sensitive"
    fi
    printf '%s' "$value" | vercel env add "$name" "$env" $flags --cwd apps/web
  }

  # Add BETTER_AUTH_SECRET to each environment
  print_step "Adding BETTER_AUTH_SECRET to production..."
  if set_vercel_env "BETTER_AUTH_SECRET" "production" "$prod_secret" "true" 2>/dev/null; then
    print_success "Production BETTER_AUTH_SECRET configured"
  else
    print_warning "Could not set production secret"
  fi

  print_step "Adding BETTER_AUTH_SECRET to preview..."
  if set_vercel_env "BETTER_AUTH_SECRET" "preview" "$preview_secret" "true" 2>/dev/null; then
    print_success "Preview BETTER_AUTH_SECRET configured"
  else
    print_warning "Could not set preview secret"
  fi

  print_step "Adding BETTER_AUTH_SECRET to development..."
  # Note: Don't use --sensitive for development env vars
  if set_vercel_env "BETTER_AUTH_SECRET" "development" "$dev_secret" "false" 2>/dev/null; then
    print_success "Development BETTER_AUTH_SECRET configured"
  else
    print_warning "Could not set development secret"
  fi

  # Add BETTER_AUTH_URL for production
  print_step "Adding BETTER_AUTH_URL to production..."
  if set_vercel_env "BETTER_AUTH_URL" "production" "https://$PROJECT_NAME.vercel.app" "false" 2>/dev/null; then
    print_success "Production BETTER_AUTH_URL configured"
  else
    print_warning "Could not set production BETTER_AUTH_URL"
  fi

  print_step "Adding BETTER_AUTH_URL to development..."
  if set_vercel_env "BETTER_AUTH_URL" "development" "http://localhost:3000" "false" 2>/dev/null; then
    print_success "Development BETTER_AUTH_URL configured"
  else
    print_warning "Could not set development BETTER_AUTH_URL"
  fi

  # Note: Development secrets will be pulled to .env.local by pull_vercel_env()
  print_success "Better Auth secrets configured for all environments!"
}

setup_workos_secrets() {
  print_header "Configuring WorkOS Cookie Password"

  print_step "Generating unique secrets for each environment..."

  local prod_secret=$(openssl rand -base64 32)
  local preview_secret=$(openssl rand -base64 32)
  local dev_secret=$(openssl rand -base64 32)

  # Helper to set env var (removes existing first to avoid --force issues)
  set_vercel_env() {
    local name="$1"
    local env="$2"
    local value="$3"
    local sensitive="$4"

    # Remove existing var if present (ignore errors)
    vercel env rm "$name" "$env" --cwd apps/web --yes 2>/dev/null || true

    # Add the new value
    local flags=""
    if [[ "$sensitive" == "true" ]]; then
      flags="--sensitive"
    fi
    printf '%s' "$value" | vercel env add "$name" "$env" $flags --cwd apps/web
  }

  # Add WORKOS_COOKIE_PASSWORD to each environment
  print_step "Adding WORKOS_COOKIE_PASSWORD to production..."
  if set_vercel_env "WORKOS_COOKIE_PASSWORD" "production" "$prod_secret" "true" 2>/dev/null; then
    print_success "Production WORKOS_COOKIE_PASSWORD configured"
  else
    print_warning "Could not set production secret"
  fi

  print_step "Adding WORKOS_COOKIE_PASSWORD to preview..."
  if set_vercel_env "WORKOS_COOKIE_PASSWORD" "preview" "$preview_secret" "true" 2>/dev/null; then
    print_success "Preview WORKOS_COOKIE_PASSWORD configured"
  else
    print_warning "Could not set preview secret"
  fi

  print_step "Adding WORKOS_COOKIE_PASSWORD to development..."
  # Note: Don't use --sensitive for development env vars
  if set_vercel_env "WORKOS_COOKIE_PASSWORD" "development" "$dev_secret" "false" 2>/dev/null; then
    print_success "Development WORKOS_COOKIE_PASSWORD configured"
  else
    print_warning "Could not set development secret"
  fi

  # Note: Development secrets will be pulled to .env.local by pull_vercel_env()
  print_success "WorkOS cookie password configured for all environments!"
}

# =============================================================================
# Step 10: Pull Vercel Environment Variables
# =============================================================================

pull_vercel_env() {
  print_header "Pulling Vercel Environment Variables"

  if [[ ! -f "apps/web/.vercel/project.json" ]]; then
    print_warning "Vercel project not linked, skipping env pull"
    return 0
  fi

  print_step "Pulling environment variables from Vercel..."

  # Pull to apps/web/.env.local (run from apps/web where .vercel config lives)
  cd apps/web
  vercel env pull .env.local --yes 2>/dev/null || true
  cd ../..

  print_success "Environment variables pulled (if any were configured in Vercel)"
}

# =============================================================================
# Step 11: Configure Database Environments (skipped if USE_DOCKER=true)
# =============================================================================

configure_database_environments() {
  if [[ "$USE_DOCKER" == "true" ]]; then
    print_step "Skipping Supabase env configuration (Docker mode)"
    return 0
  fi

  print_header "Configuring Database Environments"

  if [[ ! -f ".supabase/.project-ref" ]]; then
    print_warning "No Supabase project configured, skipping"
    return 0
  fi

  local project_ref=$(cat .supabase/.project-ref)
  local env_file="apps/web/.env.local"

  # Create env file from example if it doesn't exist
  if [[ ! -f "$env_file" ]]; then
    if [[ -f "apps/web/.env.local.example" ]]; then
      cp "apps/web/.env.local.example" "$env_file"
      print_step "Created .env.local from example"
    else
      touch "$env_file"
    fi
  fi

  # -------------------------------------------------------------------------
  # Step 11a: Create Development Branches (dev, dev-test)
  # -------------------------------------------------------------------------
  print_step "Setting up database branches..."
  echo ""

  # Check if branching is enabled
  local branching_enabled=true
  if ! supabase branches list --project-ref "$project_ref" &> /dev/null 2>&1; then
    print_warning "Branching not enabled for this project"
    echo ""
    echo "To enable branching (requires Pro plan):"
    echo "  https://supabase.com/dashboard/project/$project_ref/settings/branching"
    echo ""
    read -p "Continue without branching? (y/N) " skip_branching
    if [[ "$skip_branching" != [yY] ]]; then
      exit 1
    fi
    branching_enabled=false
  fi

  local dev_db_url=""
  local dev_test_db_url=""
  local preview_db_url=""

  if [[ "$branching_enabled" == "true" ]]; then
    # Create dev branch
    echo "  Creating 'dev' branch (for local development)..."
    if supabase branches create dev --persistent --project-ref "$project_ref" 2>/dev/null; then
      print_success "  dev branch created"
    else
      echo -e "    \${YELLOW}⚠ dev branch may already exist\${NC}"
    fi

    # Create dev-test branch
    echo "  Creating 'dev-test' branch (for local tests)..."
    if supabase branches create dev-test --persistent --project-ref "$project_ref" 2>/dev/null; then
      print_success "  dev-test branch created"
    else
      echo -e "    \${YELLOW}⚠ dev-test branch may already exist\${NC}"
    fi

    # Note: Preview branches are created dynamically per PR via Supabase Vercel Integration

    # Wait for branches to provision
    echo ""
    print_step "Waiting for branches to provision (30 seconds)..."
    sleep 30

    # Fetch branch credentials
    print_step "Fetching branch credentials..."

    # Get dev branch URL
    if eval "$(supabase branches get dev --project-ref "$project_ref" -o env 2>/dev/null)"; then
      if [[ -n "$POSTGRES_URL" ]]; then
        dev_db_url="$POSTGRES_URL"
        print_success "  Got dev branch URL"
      fi
    fi

    # Get dev-test branch URL
    if eval "$(supabase branches get dev-test --project-ref "$project_ref" -o env 2>/dev/null)"; then
      if [[ -n "$POSTGRES_URL" ]]; then
        dev_test_db_url="$POSTGRES_URL"
        print_success "  Got dev-test branch URL"
      fi
    fi

    # Note: No need to run migrations on branches - they're copies of main
    # and already have the schema + migration history
    print_success "Branches created (schema inherited from main)"
  fi

  # -------------------------------------------------------------------------
  # Step 11b: Configure Vercel Production DATABASE_URL
  # -------------------------------------------------------------------------
  if [[ -f "apps/web/.vercel/project.json" ]]; then
    echo ""
    print_step "Configuring Vercel production DATABASE_URL..."

    # Use session pooler (aws-1, port 5432) which supports both:
    # - DDL operations (migrations) during build
    # - Runtime queries from serverless functions
    local prod_db_url=$(get_direct_database_url "$project_ref")

    if [[ -n "$prod_db_url" ]]; then
      if printf '%s' "$prod_db_url" | vercel env add DATABASE_URL production --cwd apps/web 2>/dev/null; then
        print_success "Production DATABASE_URL set in Vercel"
      else
        echo -e "  \${YELLOW}⚠ Could not set DATABASE_URL (may already exist)\${NC}"
      fi
    else
      print_warning "Could not get DATABASE_URL - add it manually in Vercel dashboard"
    fi
  fi

  # -------------------------------------------------------------------------
  # Step 11c: Setup Supabase Vercel Integration (optional)
  # -------------------------------------------------------------------------
  # The integration can handle preview branch DATABASE_URLs automatically
  if [[ "$branching_enabled" == "true" ]] && [[ -f "apps/web/.vercel/project.json" ]]; then
    echo ""
    print_step "Supabase Vercel Integration Setup"
    echo ""
    echo "  The Supabase Vercel Integration automatically creates database branches"
    echo "  for each Vercel preview deployment. This requires connecting your"
    echo "  Supabase and Vercel accounts via the dashboard."
    echo ""
    read -p "  Open browser to set up integration now? (Y/n) " setup_integration
    setup_integration=\${setup_integration:-Y}

    if [[ "$setup_integration" == [yY] ]]; then
      echo ""
      echo "  Opening Vercel projects page..."
      echo ""
      echo "  Steps to complete:"
      echo "    1. Select your project: $PROJECT_NAME"
      echo "    2. Go to Settings → Integrations → Browse Marketplace"
      echo "    3. Find and add 'Supabase' integration"
      echo "    4. Connect to your existing Supabase project: $project_ref"
      echo "    5. Enable 'Preview Branches' option"
      echo ""

      open "https://vercel.com/" 2>/dev/null || echo "  URL: https://vercel.com/"

      echo ""
      read -p "  Press Enter when you've completed the integration setup..." _
      print_success "Integration setup acknowledged"
    else
      echo ""
      print_step "Skipping integration setup"
      echo ""
      echo "  To set up later, go to your Vercel project:"
      echo "    Settings → Integrations → Browse Marketplace → Supabase"
      echo ""
      echo "  Or visit: https://vercel.com/integrations/supabase"
    fi
  fi

  # -------------------------------------------------------------------------
  # Step 11d: Configure Local .env.local
  # -------------------------------------------------------------------------
  echo ""
  print_step "Configuring local .env.local..."

  # Use dev branch URL for local development
  local local_db_url="$dev_db_url"
  local local_test_db_url="$dev_test_db_url"

  # If no branches, fall back to production URL for local dev
  if [[ -z "$local_db_url" ]] && [[ "$branching_enabled" != "true" ]]; then
    print_step "Using production database URL for local development..."
    # Use direct connection for local dev (allows running migrations locally)
    local_db_url=$(get_direct_database_url "$project_ref")
  fi

  if [[ -n "$local_db_url" ]]; then
    update_env_var "$env_file" "DATABASE_URL" "$local_db_url"
    print_success "DATABASE_URL configured for local development"
  else
    print_warning "Could not configure DATABASE_URL - add it manually to .env.local"
  fi

  if [[ -n "$local_test_db_url" ]]; then
    update_env_var "$env_file" "TEST_DATABASE_URL" "$local_test_db_url"
    print_success "TEST_DATABASE_URL configured for local tests"
  fi

  echo ""
  print_success "Database environments configured!"
  echo ""
  echo "  Production (Vercel):  DATABASE_URL configured"
  if [[ "$branching_enabled" == "true" ]]; then
    echo "  Preview (Vercel):     Via Supabase Integration (if configured)"
    echo "  Development (local):  'dev' branch"
    echo "  Tests (local):        'dev-test' branch"
  else
    echo "  Development (local):  Main database (no branching)"
  fi
}

# Helper to update or add env var
update_env_var() {
  local file="$1"
  local key="$2"
  local value="$3"

  if grep -q "^$key=" "$file" 2>/dev/null; then
    sed -i '' "s|^$key=.*|$key=\\"$value\\"|" "$file"
  else
    echo "$key=\\"$value\\"" >> "$file"
  fi
}

# =============================================================================
# Step 12: Final Summary
# =============================================================================

print_summary() {
  print_header "Setup Complete!"

  echo "Your project has been configured with:"
  echo ""

  # GitHub status
  if git remote get-url origin &> /dev/null 2>&1; then
    local repo_url=$(git remote get-url origin)
    echo -e "  \${GREEN}✓\${NC} GitHub: $repo_url"
  fi

  # Vercel status
  if [[ -f "apps/web/.vercel/project.json" ]]; then
    echo -e "  \${GREEN}✓\${NC} Vercel: Project linked (apps/web)"
  fi

  # Supabase status (only if not Docker mode)
  if [[ "$USE_DOCKER" != "true" ]]; then
    if [[ -f ".supabase/.project-ref" ]]; then
      local project_ref=$(cat .supabase/.project-ref)
      echo -e "  \${GREEN}✓\${NC} Supabase: $project_ref"
    fi
  else
    echo -e "  \${BLUE}→\${NC} Database: Docker (local)"
  fi

  echo ""
  echo "Next steps:"
  echo ""
  echo "  1. Review apps/web/.env.local and fill in any missing values:"
  if [[ "$USE_DOCKER" != "true" ]]; then
    echo "     - RESEND_API_KEY (for email authentication)"
  fi
  echo "     - OPENAI_API_KEY or AI_GATEWAY_API_KEY"
  echo "     - NEXT_PUBLIC_POSTHOG_KEY (optional)"
  echo ""

  if [[ "$USE_DOCKER" == "true" ]]; then
    echo "  2. Start the local database:"
    echo "     pnpm docker:up"
    echo ""
    echo "  3. Run migrations:"
    echo "     pnpm db:generate && pnpm db:migrate"
    echo ""
    echo "  4. Start development:"
    echo "     pnpm dev"
  else
    echo "  2. Start development:"
    echo "     pnpm dev"
  fi

  echo ""
  if [[ "$USE_DOCKER" != "true" ]]; then
    echo "  Vercel production DATABASE_URL has been configured."
    echo "  Add any additional env vars in the Vercel dashboard."
  else
    echo "  For deployment, configure DATABASE_URL in Vercel:"
    echo "     vercel env add DATABASE_URL production"
  fi
  echo ""
}

# =============================================================================
# Step 12: Commit Setup Changes and Deploy
# =============================================================================

commit_and_deploy() {
  print_header "Deploying to Production"

  # Check if there are any changes to commit (including untracked files)
  if [[ -z $(git status --porcelain) ]]; then
    print_step "No setup changes to commit"
  else
    print_step "Committing setup changes..."
    git add -A
    git commit -m "chore: configure project setup

- Add Vercel project configuration
- Update .gitignore with Vercel entries
- Configure environment files" 2>/dev/null || true
  fi

  # Check if we need to push
  local unpushed=$(git log origin/main..HEAD --oneline 2>/dev/null | wc -l | tr -d ' ')

  if [[ "$unpushed" -gt 0 ]]; then
    echo ""
    read -p "Push to main and trigger production deploy? (Y/n) " do_deploy
    do_deploy=\${do_deploy:-Y}

    if [[ "$do_deploy" == [yY] ]]; then
      print_step "Pushing to main..."
      if git push origin main; then
        print_success "Pushed to main - deployment triggered!"
      else
        print_warning "Push failed - deploy manually with: git push origin main"
      fi
    else
      print_step "Skipping deploy - deploy later with: git push origin main"
    fi
  else
    print_success "Already up to date with remote"
    echo ""
    echo "  No new commits to push, but you may still want to deploy."
    read -p "Trigger a production deployment via Vercel CLI? (Y/n) " do_cli_deploy
    do_cli_deploy=\${do_cli_deploy:-Y}

    if [[ "$do_cli_deploy" == [yY] ]]; then
      print_step "Deploying to production..."
      if vercel --prod --yes; then
        print_success "Production deployment complete!"
      else
        print_warning "Deployment failed - try manually with: vercel --prod"
      fi
    else
      print_step "Skipping deploy - deploy later with: vercel --prod"
    fi
  fi
}

# =============================================================================
# Main Execution
# =============================================================================

main() {
  echo ""
  echo "========================================"
  echo "       $PROJECT_NAME Setup"
  echo "========================================"
  echo ""

  if [[ "$USE_DOCKER" == "true" ]]; then
    echo "Mode: Docker (local PostgreSQL)"
  else
    echo "Mode: Supabase (cloud PostgreSQL)"
  fi
  echo ""

  check_prerequisites
  auth_github
  auth_supabase
  auth_vercel
  select_vercel_team
  create_github_repo
  create_supabase_project
  link_supabase
  run_migrations
  setup_vercel
  setup_auth_secrets
  pull_vercel_env
  configure_database_environments
  commit_and_deploy
  print_summary
}

# Run main function
main "$@"
"
`;

exports[`template snapshots > test factories templates > generateFactoriesIndex (Better Auth) 1`] = `
"export * from "./user";
"
`;

exports[`template snapshots > test factories templates > generateFactoriesIndex (WorkOS) 1`] = `
"export * from "./user";
export * from "./organization";
"
`;

exports[`template snapshots > test factories templates > generateOrganizationFactory 1`] = `
"import { faker } from "@faker-js/faker";

interface Organization {
	id: string;
	workosId: string;
	name: string;
	allowProfilesOutsideOrganization: boolean;
	createdAt: Date;
	updatedAt: Date;
	deletedAt: Date | null;
}

/**
 * Create a fake organization for testing
 * @param overrides - Optional fields to override the generated values
 */
export function createOrganization(
	overrides?: Partial<Organization>,
): Organization {
	return {
		id: faker.string.uuid(),
		workosId: \`org_\${faker.string.alphanumeric(24)}\`,
		name: faker.company.name(),
		allowProfilesOutsideOrganization: false,
		createdAt: faker.date.past(),
		updatedAt: faker.date.recent(),
		deletedAt: null,
		...overrides,
	};
}

/**
 * Create organization input data (for insert operations)
 */
export function createOrganizationInput(overrides?: {
	workosId?: string;
	name?: string;
	allowProfilesOutsideOrganization?: boolean;
}) {
	return {
		workosId: \`org_\${faker.string.alphanumeric(24)}\`,
		name: faker.company.name(),
		allowProfilesOutsideOrganization: false,
		...overrides,
	};
}

interface OrganizationMembership {
	id: string;
	workosId: string;
	organizationId: string;
	userId: string;
	roleSlug: string | null;
	roleName: string | null;
	status: string | null;
	createdAt: Date;
	updatedAt: Date;
	deletedAt: Date | null;
}

/**
 * Create a fake organization membership for testing
 * @param overrides - Optional fields to override the generated values
 */
export function createOrganizationMembership(
	overrides?: Partial<OrganizationMembership>,
): OrganizationMembership {
	return {
		id: faker.string.uuid(),
		workosId: \`om_\${faker.string.alphanumeric(24)}\`,
		organizationId: faker.string.uuid(),
		userId: faker.string.uuid(),
		roleSlug: "member",
		roleName: "Member",
		status: "active",
		createdAt: faker.date.past(),
		updatedAt: faker.date.recent(),
		deletedAt: null,
		...overrides,
	};
}

/**
 * Create organization membership input data (for insert operations)
 */
export function createOrganizationMembershipInput(overrides?: {
	workosId?: string;
	organizationId?: string;
	userId?: string;
	roleSlug?: string;
	roleName?: string;
	status?: string;
}) {
	return {
		workosId: \`om_\${faker.string.alphanumeric(24)}\`,
		organizationId: overrides?.organizationId ?? faker.string.uuid(),
		userId: overrides?.userId ?? faker.string.uuid(),
		roleSlug: "member",
		roleName: "Member",
		status: "active",
		...overrides,
	};
}
"
`;

exports[`template snapshots > test factories templates > generateWorkOSDbTest 1`] = `
"import {
	describe,
	it,
	expect,
	beforeAll,
	beforeEach,
	afterAll,
} from "vitest";
import { eq } from "drizzle-orm";
import {
	getTestDb,
	resetTestDb,
	seedTestDb,
	closeTestDb,
} from "../utils/test-db";
import * as schema from "@/db/schema";

describe("WorkOS Schema Integration Tests", () => {
	beforeAll(async () => {
		await getTestDb();
	});

	beforeEach(async () => {
		await resetTestDb();
	});

	afterAll(async () => {
		await closeTestDb();
	});

	describe("users table", () => {
		it("can create a user with WorkOS fields", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.users)
				.values({
					workosId: "user_test123",
					email: "test@example.com",
					firstName: "Test",
					lastName: "User",
					emailVerified: true,
				})
				.returning();

			expect(user.id).toBeDefined();
			expect(user.workosId).toBe("user_test123");
			expect(user.email).toBe("test@example.com");
			expect(user.firstName).toBe("Test");
			expect(user.lastName).toBe("User");
			expect(user.emailVerified).toBe(true);
			expect(user.createdAt).toBeInstanceOf(Date);
		});

		it("enforces unique workosId constraint", async () => {
			const db = await getTestDb();

			await db.insert(schema.users).values({
				workosId: "user_duplicate",
				email: "user1@example.com",
			});

			await expect(
				db.insert(schema.users).values({
					workosId: "user_duplicate",
					email: "user2@example.com",
				}),
			).rejects.toThrow();
		});

		it("enforces unique email constraint", async () => {
			const db = await getTestDb();

			await db.insert(schema.users).values({
				workosId: "user_1",
				email: "duplicate@example.com",
			});

			await expect(
				db.insert(schema.users).values({
					workosId: "user_2",
					email: "duplicate@example.com",
				}),
			).rejects.toThrow();
		});

		it("can update user fields", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.users)
				.values({
					workosId: "user_update",
					email: "update@example.com",
				})
				.returning();

			const [updated] = await db
				.update(schema.users)
				.set({ firstName: "Updated", lastName: "Name" })
				.where(eq(schema.users.id, user.id))
				.returning();

			expect(updated.firstName).toBe("Updated");
			expect(updated.lastName).toBe("Name");
		});
	});

	describe("organizations table", () => {
		it("can create an organization", async () => {
			const db = await getTestDb();

			const [org] = await db
				.insert(schema.organizations)
				.values({
					workosId: "org_test123",
					name: "Test Organization",
				})
				.returning();

			expect(org.id).toBeDefined();
			expect(org.workosId).toBe("org_test123");
			expect(org.name).toBe("Test Organization");
			expect(org.allowProfilesOutsideOrganization).toBe(false);
		});

		it("enforces unique workosId constraint", async () => {
			const db = await getTestDb();

			await db.insert(schema.organizations).values({
				workosId: "org_duplicate",
				name: "Org 1",
			});

			await expect(
				db.insert(schema.organizations).values({
					workosId: "org_duplicate",
					name: "Org 2",
				}),
			).rejects.toThrow();
		});

		it("can update organization fields", async () => {
			const db = await getTestDb();

			const [org] = await db
				.insert(schema.organizations)
				.values({
					workosId: "org_update",
					name: "Original Name",
				})
				.returning();

			const [updated] = await db
				.update(schema.organizations)
				.set({ name: "Updated Name", allowProfilesOutsideOrganization: true })
				.where(eq(schema.organizations.id, org.id))
				.returning();

			expect(updated.name).toBe("Updated Name");
			expect(updated.allowProfilesOutsideOrganization).toBe(true);
		});
	});

	describe("organization memberships", () => {
		it("can create membership linking user and org", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.users)
				.values({
					workosId: "user_member1",
					email: "member@example.com",
				})
				.returning();

			const [org] = await db
				.insert(schema.organizations)
				.values({
					workosId: "org_team1",
					name: "Team",
				})
				.returning();

			const [membership] = await db
				.insert(schema.organizationMemberships)
				.values({
					workosId: "om_test123",
					userId: user.id,
					organizationId: org.id,
					roleSlug: "admin",
					roleName: "Admin",
				})
				.returning();

			expect(membership.userId).toBe(user.id);
			expect(membership.organizationId).toBe(org.id);
			expect(membership.roleSlug).toBe("admin");
			expect(membership.roleName).toBe("Admin");
			expect(membership.status).toBe("active");
		});

		it("enforces unique user per organization", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.users)
				.values({
					workosId: "user_unique_test",
					email: "unique@example.com",
				})
				.returning();

			const [org] = await db
				.insert(schema.organizations)
				.values({
					workosId: "org_unique_test",
					name: "Unique Org",
				})
				.returning();

			await db.insert(schema.organizationMemberships).values({
				workosId: "om_first",
				userId: user.id,
				organizationId: org.id,
			});

			await expect(
				db.insert(schema.organizationMemberships).values({
					workosId: "om_second",
					userId: user.id,
					organizationId: org.id,
				}),
			).rejects.toThrow();
		});

		it("cascades delete when user is deleted", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.users)
				.values({
					workosId: "user_cascade",
					email: "cascade@example.com",
				})
				.returning();

			const [org] = await db
				.insert(schema.organizations)
				.values({
					workosId: "org_cascade",
					name: "Cascade Org",
				})
				.returning();

			await db.insert(schema.organizationMemberships).values({
				workosId: "om_cascade",
				userId: user.id,
				organizationId: org.id,
				roleSlug: "member",
				roleName: "Member",
			});

			await db.delete(schema.users).where(eq(schema.users.id, user.id));

			const memberships = await db
				.select()
				.from(schema.organizationMemberships);
			expect(memberships).toHaveLength(0);
		});

		it("cascades delete when organization is deleted", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.users)
				.values({
					workosId: "user_org_cascade",
					email: "orgcascade@example.com",
				})
				.returning();

			const [org] = await db
				.insert(schema.organizations)
				.values({
					workosId: "org_to_delete",
					name: "Delete Me Org",
				})
				.returning();

			await db.insert(schema.organizationMemberships).values({
				workosId: "om_org_cascade",
				userId: user.id,
				organizationId: org.id,
			});

			await db
				.delete(schema.organizations)
				.where(eq(schema.organizations.id, org.id));

			const memberships = await db
				.select()
				.from(schema.organizationMemberships);
			expect(memberships).toHaveLength(0);

			// User should still exist
			const users = await db.select().from(schema.users);
			expect(users).toHaveLength(1);
		});
	});

	describe("seed data", () => {
		it("seeds test data correctly", async () => {
			await seedTestDb();

			const db = await getTestDb();
			const users = await db.select().from(schema.users);
			const orgs = await db.select().from(schema.organizations);
			const memberships = await db
				.select()
				.from(schema.organizationMemberships);

			expect(users).toHaveLength(1);
			expect(users[0].workosId).toBe("user_test1");
			expect(orgs).toHaveLength(1);
			expect(orgs[0].workosId).toBe("org_test1");
			expect(memberships).toHaveLength(1);
			expect(memberships[0].roleSlug).toBe("admin");
		});
	});
});
"
`;

exports[`template snapshots > test templates > generateAiTriggerTest 1`] = `
"import { describe, it, expect, vi, beforeEach, afterEach } from "vitest";
import { screen, waitFor } from "@testing-library/react";
import userEvent from "@testing-library/user-event";
import { render } from "../utils/render";
import { mockFetch, resetFetchMock } from "../utils/mocks";
import { AITriggerButton } from "@/app/(app)/dashboard/_components/ai-trigger";

describe("AITriggerButton", () => {
	beforeEach(() => {
		mockFetch({
			"/api/workflow": { runId: "test-run-123" },
		});
	});

	afterEach(() => {
		resetFetchMock();
	});

	it("renders with default prompt", () => {
		render(<AITriggerButton />);

		expect(screen.getByLabelText("Prompt")).toHaveValue(
			"What are 3 interesting facts about TypeScript?",
		);
		expect(
			screen.getByRole("button", { name: /trigger ai workflow/i }),
		).toBeInTheDocument();
	});

	it("allows editing the prompt", async () => {
		const user = userEvent.setup();
		render(<AITriggerButton />);

		const input = screen.getByLabelText("Prompt");
		await user.clear(input);
		await user.type(input, "New prompt");

		expect(input).toHaveValue("New prompt");
	});

	it("disables button when prompt is empty", async () => {
		const user = userEvent.setup();
		render(<AITriggerButton />);

		const input = screen.getByLabelText("Prompt");
		await user.clear(input);

		expect(
			screen.getByRole("button", { name: /trigger ai workflow/i }),
		).toBeDisabled();
	});

	it("shows loading state when triggered", async () => {
		// Create a mock readable stream body for SSE endpoints
		const mockBody = {
			getReader: () => ({
				read: () => Promise.resolve({ done: true, value: undefined }),
			}),
		};

		// Use a delayed mock to capture the loading state
		global.fetch = vi.fn<typeof fetch>(() =>
			new Promise((resolve) =>
				setTimeout(
					() =>
						resolve({
							ok: true,
							json: () => Promise.resolve({ runId: "test-run-123" }),
							body: mockBody,
						} as Response),
					100,
				),
			),
		);

		const user = userEvent.setup();
		render(<AITriggerButton />);

		const button = screen.getByRole("button", { name: /trigger ai workflow/i });
		await user.click(button);

		// Check loading state appears
		expect(screen.getByRole("button", { name: /running/i })).toBeInTheDocument();

		// Wait for completion
		await waitFor(() => {
			expect(
				screen.getByText(/workflow started.*test-run-123/i),
			).toBeInTheDocument();
		});
	});

	it("displays success message with run ID", async () => {
		const user = userEvent.setup();
		render(<AITriggerButton />);

		const button = screen.getByRole("button", { name: /trigger ai workflow/i });
		await user.click(button);

		await waitFor(() => {
			expect(
				screen.getByText(/workflow started.*test-run-123/i),
			).toBeInTheDocument();
		});
	});

	it("displays error message on failure", async () => {
		mockFetch({
			"/api/workflow": { ok: false, error: "Something went wrong" },
		});

		const user = userEvent.setup();
		render(<AITriggerButton />);

		await user.click(
			screen.getByRole("button", { name: /trigger ai workflow/i }),
		);

		await waitFor(() => {
			expect(screen.getByText(/something went wrong/i)).toBeInTheDocument();
		});
	});
});
"
`;

exports[`template snapshots > test templates > generateDbTest 1`] = `
"import {
	describe,
	it,
	expect,
	beforeAll,
	beforeEach,
	afterAll,
} from "vitest";
import { eq } from "drizzle-orm";
import {
	getTestDb,
	resetTestDb,
	seedTestDb,
	closeTestDb,
} from "../utils/test-db";
import * as schema from "@/db/schema";

describe("Better Auth Schema Integration Tests", () => {
	beforeAll(async () => {
		await getTestDb();
	});

	beforeEach(async () => {
		await resetTestDb();
	});

	afterAll(async () => {
		await closeTestDb();
	});

	describe("user table", () => {
		it("can create a user", async () => {
			const db = await getTestDb();

			const [user] = await db
				.insert(schema.user)
				.values({
					id: "user-123",
					name: "John Doe",
					email: "john@example.com",
				})
				.returning();

			expect(user.id).toBe("user-123");
			expect(user.name).toBe("John Doe");
			expect(user.email).toBe("john@example.com");
			expect(user.emailVerified).toBe(false); // default value
			expect(user.createdAt).toBeInstanceOf(Date);
		});

		it("enforces unique email constraint", async () => {
			const db = await getTestDb();

			await db.insert(schema.user).values({
				id: "user-1",
				name: "User 1",
				email: "duplicate@example.com",
			});

			await expect(
				db.insert(schema.user).values({
					id: "user-2",
					name: "User 2",
					email: "duplicate@example.com",
				}),
			).rejects.toThrow();
		});

		it("can update user fields", async () => {
			const db = await getTestDb();

			await db.insert(schema.user).values({
				id: "user-update",
				name: "Original Name",
				email: "update@example.com",
			});

			const [updated] = await db
				.update(schema.user)
				.set({ name: "Updated Name", emailVerified: true })
				.where(eq(schema.user.id, "user-update"))
				.returning();

			expect(updated.name).toBe("Updated Name");
			expect(updated.emailVerified).toBe(true);
		});
	});

	describe("session table", () => {
		it("can create a session for a user", async () => {
			const db = await getTestDb();

			// Create user first
			await db.insert(schema.user).values({
				id: "user-session",
				name: "Session User",
				email: "session@example.com",
			});

			const [session] = await db
				.insert(schema.session)
				.values({
					id: "session-123",
					userId: "user-session",
					token: "token-abc",
					expiresAt: new Date(Date.now() + 3600000),
				})
				.returning();

			expect(session.id).toBe("session-123");
			expect(session.userId).toBe("user-session");
			expect(session.token).toBe("token-abc");
		});

		it("cascades delete when user is deleted", async () => {
			const db = await getTestDb();

			await db.insert(schema.user).values({
				id: "user-cascade",
				name: "Cascade User",
				email: "cascade@example.com",
			});

			await db.insert(schema.session).values({
				id: "session-cascade",
				userId: "user-cascade",
				token: "token-cascade",
				expiresAt: new Date(Date.now() + 3600000),
			});

			await db.delete(schema.user).where(eq(schema.user.id, "user-cascade"));

			const sessions = await db.select().from(schema.session);
			expect(sessions).toHaveLength(0);
		});
	});

	describe("account table", () => {
		it("can create an account for a user", async () => {
			const db = await getTestDb();

			await db.insert(schema.user).values({
				id: "user-account",
				name: "Account User",
				email: "account@example.com",
			});

			const [account] = await db
				.insert(schema.account)
				.values({
					id: "account-123",
					userId: "user-account",
					accountId: "oauth-id-123",
					providerId: "google",
				})
				.returning();

			expect(account.id).toBe("account-123");
			expect(account.providerId).toBe("google");
			expect(account.userId).toBe("user-account");
		});
	});

	describe("verification table", () => {
		it("can create a verification entry", async () => {
			const db = await getTestDb();

			const [verification] = await db
				.insert(schema.verification)
				.values({
					id: "verification-123",
					identifier: "test@example.com",
					value: "otp-code-123456",
					expiresAt: new Date(Date.now() + 600000), // 10 minutes
				})
				.returning();

			expect(verification.id).toBe("verification-123");
			expect(verification.identifier).toBe("test@example.com");
			expect(verification.value).toBe("otp-code-123456");
		});
	});

	describe("seed data", () => {
		it("seeds test data correctly", async () => {
			await seedTestDb();

			const db = await getTestDb();
			const users = await db.select().from(schema.user);
			const sessions = await db.select().from(schema.session);

			expect(users).toHaveLength(1);
			expect(users[0].email).toBe("test@example.com");
			expect(sessions).toHaveLength(1);
			expect(sessions[0].userId).toBe("test-user-1");
		});
	});
});
"
`;

exports[`template snapshots > test templates > generateTestDbUtils 1`] = `
"import { drizzle } from "drizzle-orm/node-postgres";
import { Pool } from "pg";
import * as schema from "../../db/schema";
import { sql } from "drizzle-orm";

const TEST_DATABASE_URL =
	process.env.TEST_DATABASE_URL ||
	"postgresql://postgres:postgres@localhost:5434/undefined_test";

let pool: Pool | null = null;
let testDb: ReturnType<typeof drizzle<typeof schema>> | null = null;

export async function getTestDb() {
	if (!testDb) {
		pool = new Pool({ connectionString: TEST_DATABASE_URL });
		testDb = drizzle(pool, { schema });
	}
	return testDb;
}

export async function resetTestDb() {
	const db = await getTestDb();
	// Truncate all tables in correct order (respecting foreign key constraints)
	await db.execute(
		sql\`TRUNCATE TABLE verification, account, session, "user" RESTART IDENTITY CASCADE\`,
	);
}

export async function seedTestDb() {
	const db = await getTestDb();

	// Create test user
	await db.insert(schema.user).values({
		id: "test-user-1",
		name: "Test User",
		email: "test@example.com",
		emailVerified: true,
	});

	// Create test session
	await db.insert(schema.session).values({
		id: "test-session-1",
		userId: "test-user-1",
		token: "test-token-123",
		expiresAt: new Date(Date.now() + 24 * 60 * 60 * 1000), // 24 hours from now
	});
}

export async function closeTestDb() {
	if (pool) {
		await pool.end();
		pool = null;
		testDb = null;
	}
}
"
`;

exports[`template snapshots > test templates > generateTestMocks 1`] = `
"import { vi } from "vitest";

export function createFetchMock(responses: Record<string, unknown> = {}) {
	return vi.fn((url: string) => {
		const response = responses[url] || { ok: true, data: {} };

		// Create a mock readable stream body for SSE endpoints
		const mockBody = {
			getReader: () => ({
				read: () => Promise.resolve({ done: true, value: undefined }),
			}),
		};

		return Promise.resolve({
			ok: true,
			json: () => Promise.resolve(response),
			text: () => Promise.resolve(JSON.stringify(response)),
			body: mockBody,
			...response,
		});
	});
}

export function mockFetch(responses: Record<string, unknown> = {}) {
	const mock = createFetchMock(responses);
	global.fetch = mock as unknown as typeof fetch;
	return mock;
}

export function resetFetchMock() {
	vi.restoreAllMocks();
}
"
`;

exports[`template snapshots > test templates > generateTestRenderUtils 1`] = `
"import { render, type RenderOptions } from "@testing-library/react";
import type { ReactElement, ReactNode } from "react";

interface ProvidersProps {
	children: ReactNode;
}

function Providers({ children }: ProvidersProps) {
	// Add providers here as needed (e.g., ThemeProvider, QueryClientProvider)
	return <>{children}</>;
}

function customRender(
	ui: ReactElement,
	options?: Omit<RenderOptions, "wrapper">,
) {
	return render(ui, { wrapper: Providers, ...options });
}

export * from "@testing-library/react";
export { customRender as render };
"
`;

exports[`template snapshots > test templates > generateUserFactory 1`] = `
"import { faker } from "@faker-js/faker";

// User type matching Better Auth schema
interface User {
	id: string;
	name: string;
	email: string;
	emailVerified: boolean;
	image: string | null;
	createdAt: Date;
	updatedAt: Date;
}

/**
 * Create a fake user for testing (Better Auth schema)
 * @param overrides - Optional fields to override the generated values
 */
export function createUser(overrides?: Partial<User>): User {
	return {
		id: faker.string.alphanumeric(24),
		name: faker.person.fullName(),
		email: faker.internet.email(),
		emailVerified: true,
		image: faker.image.avatar(),
		createdAt: faker.date.past(),
		updatedAt: faker.date.recent(),
		...overrides,
	};
}

/**
 * Create user input data (for insert operations)
 * Note: createdAt/updatedAt have defaults in the schema
 */
export function createUserInput(overrides?: {
	id?: string;
	name?: string;
	email?: string;
	emailVerified?: boolean;
	image?: string | null;
}) {
	return {
		id: faker.string.alphanumeric(24),
		name: faker.person.fullName(),
		email: faker.internet.email(),
		emailVerified: true,
		image: faker.image.avatar(),
		...overrides,
	};
}
"
`;

exports[`template snapshots > test templates > generateUtilsTest 1`] = `
"import { describe, it, expect } from "vitest";
import { cn } from "@workspace/ui/lib/utils";

describe("cn utility", () => {
	it("merges class names correctly", () => {
		expect(cn("foo", "bar")).toBe("foo bar");
	});

	it("handles conditional classes", () => {
		expect(cn("base", false && "hidden", true && "visible")).toBe(
			"base visible",
		);
	});

	it("merges tailwind classes correctly", () => {
		expect(cn("px-4 py-2", "px-6")).toBe("py-2 px-6");
	});

	it("handles undefined and null", () => {
		expect(cn("base", undefined, null, "end")).toBe("base end");
	});
});
"
`;

exports[`template snapshots > test templates > generateVitestConfig 1`] = `
"import { defineConfig } from "vitest/config";
import react from "@vitejs/plugin-react";
import tsconfigPaths from "vite-tsconfig-paths";

export default defineConfig({
	plugins: [tsconfigPaths(), react()],
	test: {
		environment: "jsdom",
		setupFiles: ["./vitest.setup.ts"],
		include: ["__tests__/**/*.{test,spec}.{ts,tsx}"],
		globals: true,
		coverage: {
			provider: "v8",
			reporter: ["text", "json", "html"],
			exclude: ["node_modules/", "__tests__/", "*.config.*"],
		},
	},
});
"
`;

exports[`template snapshots > test templates > generateVitestSetup 1`] = `
"import { config } from "dotenv";
config({ path: ".env.local" });

import "@testing-library/jest-dom/vitest";
import { cleanup } from "@testing-library/react";
import { afterEach, vi } from "vitest";

// Cleanup after each test
afterEach(() => {
	cleanup();
});

// Mock Next.js router
vi.mock("next/navigation", () => ({
	useRouter: () => ({
		push: vi.fn(),
		replace: vi.fn(),
		back: vi.fn(),
		forward: vi.fn(),
		refresh: vi.fn(),
		prefetch: vi.fn(),
	}),
	usePathname: () => "/",
	useSearchParams: () => new URLSearchParams(),
}));
"
`;

exports[`template snapshots > web templates > generateEnvExample 1`] = `
"# Database (Supabase)
# Production: Set in Vercel environment variables from Supabase dashboard
# Development: Run 'pnpm supabase:env dev' to populate these
DATABASE_URL=

# Test Database (Supabase branch)
TEST_DATABASE_URL=

# Local Docker fallback (optional - run 'pnpm docker:up' first)
# DATABASE_URL=postgresql://postgres:postgres@localhost:5432/undefined
# TEST_DATABASE_URL=postgresql://postgres:postgres@localhost:5434/undefined_test

# Better Auth
BETTER_AUTH_SECRET=  # Generate with: openssl rand -base64 32
BETTER_AUTH_URL=http://localhost:3000

# Resend (for email OTP)
RESEND_API_KEY=

# Vercel AI Gateway (https://vercel.com/docs/ai-gateway)
AI_GATEWAY_API_KEY=

# PostHog Analytics
NEXT_PUBLIC_POSTHOG_KEY=  # PostHog project API key
NEXT_PUBLIC_POSTHOG_HOST=https://us.i.posthog.com  # or https://eu.i.posthog.com
POSTHOG_API_KEY=  # Server-side key (often same as NEXT_PUBLIC_POSTHOG_KEY)

# App
NEXT_PUBLIC_APP_URL=http://localhost:3000
"
`;

exports[`template snapshots > web templates > generateGlobalsCss 1`] = `
"@import "tailwindcss";
@import "tw-animate-css";

@source "../../packages/ui/src/**/*.{ts,tsx}";

@custom-variant dark (&:is(.dark *));

:root {
	--background: hsl(0 0% 100%);
	--foreground: hsl(222.2 84% 4.9%);
	--card: hsl(0 0% 100%);
	--card-foreground: hsl(222.2 84% 4.9%);
	--popover: hsl(0 0% 100%);
	--popover-foreground: hsl(222.2 84% 4.9%);
	--primary: hsl(222.2 47.4% 11.2%);
	--primary-foreground: hsl(210 40% 98%);
	--secondary: hsl(210 40% 96.1%);
	--secondary-foreground: hsl(222.2 47.4% 11.2%);
	--muted: hsl(210 40% 96.1%);
	--muted-foreground: hsl(215.4 16.3% 46.9%);
	--accent: hsl(210 40% 96.1%);
	--accent-foreground: hsl(222.2 47.4% 11.2%);
	--destructive: hsl(0 84.2% 60.2%);
	--destructive-foreground: hsl(210 40% 98%);
	--border: hsl(214.3 31.8% 91.4%);
	--input: hsl(214.3 31.8% 91.4%);
	--ring: hsl(222.2 84% 4.9%);
	--radius: 0.5rem;
}

.dark {
	--background: hsl(222.2 84% 4.9%);
	--foreground: hsl(210 40% 98%);
	--card: hsl(222.2 84% 4.9%);
	--card-foreground: hsl(210 40% 98%);
	--popover: hsl(222.2 84% 4.9%);
	--popover-foreground: hsl(210 40% 98%);
	--primary: hsl(210 40% 98%);
	--primary-foreground: hsl(222.2 47.4% 11.2%);
	--secondary: hsl(217.2 32.6% 17.5%);
	--secondary-foreground: hsl(210 40% 98%);
	--muted: hsl(217.2 32.6% 17.5%);
	--muted-foreground: hsl(215 20.2% 65.1%);
	--accent: hsl(217.2 32.6% 17.5%);
	--accent-foreground: hsl(210 40% 98%);
	--destructive: hsl(0 62.8% 30.6%);
	--destructive-foreground: hsl(210 40% 98%);
	--border: hsl(217.2 32.6% 17.5%);
	--input: hsl(217.2 32.6% 17.5%);
	--ring: hsl(212.7 26.8% 83.9%);
}

@theme inline {
	--color-background: var(--background);
	--color-foreground: var(--foreground);
	--color-card: var(--card);
	--color-card-foreground: var(--card-foreground);
	--color-popover: var(--popover);
	--color-popover-foreground: var(--popover-foreground);
	--color-primary: var(--primary);
	--color-primary-foreground: var(--primary-foreground);
	--color-secondary: var(--secondary);
	--color-secondary-foreground: var(--secondary-foreground);
	--color-muted: var(--muted);
	--color-muted-foreground: var(--muted-foreground);
	--color-accent: var(--accent);
	--color-accent-foreground: var(--accent-foreground);
	--color-destructive: var(--destructive);
	--color-destructive-foreground: var(--destructive-foreground);
	--color-border: var(--border);
	--color-input: var(--input);
	--color-ring: var(--ring);
	--radius-sm: calc(var(--radius) - 4px);
	--radius-md: calc(var(--radius) - 2px);
	--radius-lg: var(--radius);
	--radius-xl: calc(var(--radius) + 4px);
	--radius-2xl: calc(var(--radius) + 8px);
	--radius-3xl: calc(var(--radius) + 12px);
	--radius-4xl: calc(var(--radius) + 16px);
}

@layer base {
	* {
		@apply border-border outline-ring/50;
	}
	body {
		@apply bg-background text-foreground;
	}
}

@layer utilities {
	/* Defer rendering of off-screen content for performance */
	.defer-render {
		content-visibility: auto;
		contain-intrinsic-size: 0 200px;
	}
}
"
`;

exports[`template snapshots > web templates > generateHomePage 1`] = `
"import Link from "next/link";
import { Button } from "@workspace/ui/components/button";

export default function HomePage() {
	return (
		<div className="flex min-h-screen flex-col items-center justify-center gap-8 p-8">
			<div className="text-center">
				<h1 className="text-4xl font-bold mb-4">Welcome to Your App</h1>
				<p className="text-muted-foreground text-lg">
					Built with Next.js, Turborepo, and Hatch
				</p>
			</div>

			<div className="flex gap-4">
				<Button asChild>
					<Link href="/login">Get Started</Link>
				</Button>
				<Button variant="outline" asChild>
					<Link href="/dashboard">Dashboard</Link>
				</Button>
			</div>
		</div>
	);
}
"
`;

exports[`template snapshots > web templates > generateNextConfig 1`] = `
"import { withWorkflow } from "workflow/next";
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
	transpilePackages: ["@workspace/ui"],
	experimental: {
		optimizePackageImports: ["lucide-react"],
		useCache: true,
	},
};

export default withWorkflow(nextConfig);
"
`;

exports[`template snapshots > web templates > generatePostcssConfig 1`] = `
"/** @type {import('postcss-load-config').Config} */
const config = {
	plugins: {
		"@tailwindcss/postcss": {},
	},
};

export default config;
"
`;

exports[`template snapshots > web templates > generateRootLayout 1`] = `
"import type { Metadata } from "next";
import { PostHogProvider } from "@/components/providers/posthog";
import "./globals.css";

export const metadata: Metadata = {
	metadataBase: new URL(
		process.env.NEXT_PUBLIC_APP_URL || "http://localhost:3000",
	),
	title: {
		default: "undefined",
		template: "%s | undefined",
	},
	description: "undefined - Built with Hatch",
	keywords: ["undefined", "web app"],
	authors: [{ name: "undefined" }],
	openGraph: {
		type: "website",
		locale: "en_US",
		siteName: "undefined",
		title: "undefined",
		description: "undefined - Built with Hatch",
	},
	twitter: {
		card: "summary_large_image",
		title: "undefined",
		description: "undefined - Built with Hatch",
	},
	robots: {
		index: true,
		follow: true,
	},
};

export default function RootLayout({
	children,
}: {
	children: React.ReactNode;
}) {
	return (
		<html lang="en">
			<body>
				<PostHogProvider>{children}</PostHogProvider>
			</body>
		</html>
	);
}
"
`;

exports[`template snapshots > web templates > generateTailwindConfig 1`] = `
"import type { Config } from "tailwindcss";

const config: Config = {
	darkMode: "class",
	content: [
		"./pages/**/*.{js,ts,jsx,tsx,mdx}",
		"./components/**/*.{js,ts,jsx,tsx,mdx}",
		"./app/**/*.{js,ts,jsx,tsx,mdx}",
	],
	theme: {
		extend: {
			colors: {
				background: "hsl(var(--background))",
				foreground: "hsl(var(--foreground))",
				card: {
					DEFAULT: "hsl(var(--card))",
					foreground: "hsl(var(--card-foreground))",
				},
				popover: {
					DEFAULT: "hsl(var(--popover))",
					foreground: "hsl(var(--popover-foreground))",
				},
				primary: {
					DEFAULT: "hsl(var(--primary))",
					foreground: "hsl(var(--primary-foreground))",
				},
				secondary: {
					DEFAULT: "hsl(var(--secondary))",
					foreground: "hsl(var(--secondary-foreground))",
				},
				muted: {
					DEFAULT: "hsl(var(--muted))",
					foreground: "hsl(var(--muted-foreground))",
				},
				accent: {
					DEFAULT: "hsl(var(--accent))",
					foreground: "hsl(var(--accent-foreground))",
				},
				destructive: {
					DEFAULT: "hsl(var(--destructive))",
					foreground: "hsl(var(--destructive-foreground))",
				},
				border: "hsl(var(--border))",
				input: "hsl(var(--input))",
				ring: "hsl(var(--ring))",
			},
			borderRadius: {
				lg: "var(--radius)",
				md: "calc(var(--radius) - 2px)",
				sm: "calc(var(--radius) - 4px)",
			},
		},
	},
	plugins: [],
};

export default config;
"
`;

exports[`template snapshots > web templates > generateWebPackageJson (Better Auth) 1`] = `
"{
  "name": "web",
  "version": "0.0.1",
  "private": true,
  "scripts": {
    "dev": "next dev --turbopack",
    "build": "next build",
    "start": "next start",
    "lint": "biome check .",
    "typecheck": "tsc --noEmit",
    "test": "vitest run",
    "test:watch": "vitest",
    "test:ui": "vitest --ui",
    "test:coverage": "vitest run --coverage",
    "db:generate": "drizzle-kit generate",
    "db:migrate": "drizzle-kit migrate",
    "db:migrate:deploy": "drizzle-kit migrate",
    "db:push": "drizzle-kit push",
    "db:studio": "drizzle-kit studio"
  },
  "dependencies": {
    "@ai-sdk/react": "^3.0.35",
    "@posthog/ai": "^7.4.2",
    "@workspace/ui": "workspace:*",
    "ai": "^6.0.33",
    "better-auth": "^1.4.12",
    "drizzle-orm": "^0.45.1",
    "next": "^16.1.1",
    "next-safe-action": "^8.0.11",
    "pg": "^8.16.3",
    "posthog-js": "^1.320.0",
    "posthog-node": "^5.20.0",
    "react": "^19.2.3",
    "react-dom": "^19.2.3",
    "resend": "^6.7.0",
    "server-only": "^0.0.1",
    "swr": "^2.3.3",
    "tw-animate-css": "^1.4.0",
    "workflow": "^4.0.1-beta.45",
    "zod": "^4.3.5"
  },
  "devDependencies": {
    "@faker-js/faker": "^10.2.0",
    "@tailwindcss/postcss": "^4.1.18",
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.1",
    "@testing-library/user-event": "^14.6.1",
    "@types/node": "^25.0.8",
    "@types/pg": "^8.16.0",
    "@types/react": "^19.2.8",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.2",
    "dotenv": "^17.2.3",
    "drizzle-kit": "^0.31.8",
    "jsdom": "^27.4.0",
    "postcss": "^8.5.6",
    "tailwindcss": "^4.1.18",
    "typescript": "^5.9.3",
    "vite-tsconfig-paths": "^6.0.4",
    "vitest": "^4.0.17"
  }
}
"
`;

exports[`template snapshots > web templates > generateWebPackageJson (WorkOS) 1`] = `
"{
  "name": "web",
  "version": "0.0.1",
  "private": true,
  "scripts": {
    "dev": "next dev --turbopack",
    "build": "next build",
    "start": "next start",
    "lint": "biome check .",
    "typecheck": "tsc --noEmit",
    "test": "vitest run",
    "test:watch": "vitest",
    "test:ui": "vitest --ui",
    "test:coverage": "vitest run --coverage",
    "db:generate": "drizzle-kit generate",
    "db:migrate": "drizzle-kit migrate",
    "db:migrate:deploy": "drizzle-kit migrate",
    "db:push": "drizzle-kit push",
    "db:studio": "drizzle-kit studio"
  },
  "dependencies": {
    "@ai-sdk/react": "^3.0.35",
    "@posthog/ai": "^7.4.2",
    "@workos-inc/authkit-nextjs": "^2.13.0",
    "@workspace/ui": "workspace:*",
    "ai": "^6.0.33",
    "drizzle-orm": "^0.45.1",
    "next": "^16.1.1",
    "next-safe-action": "^8.0.11",
    "pg": "^8.16.3",
    "posthog-js": "^1.320.0",
    "posthog-node": "^5.20.0",
    "react": "^19.2.3",
    "react-dom": "^19.2.3",
    "server-only": "^0.0.1",
    "swr": "^2.3.3",
    "tw-animate-css": "^1.4.0",
    "workflow": "^4.0.1-beta.45",
    "zod": "^4.3.5"
  },
  "devDependencies": {
    "@faker-js/faker": "^10.2.0",
    "@tailwindcss/postcss": "^4.1.18",
    "@testing-library/dom": "^10.4.1",
    "@testing-library/jest-dom": "^6.9.1",
    "@testing-library/react": "^16.3.1",
    "@testing-library/user-event": "^14.6.1",
    "@types/node": "^25.0.8",
    "@types/pg": "^8.16.0",
    "@types/react": "^19.2.8",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.2",
    "dotenv": "^17.2.3",
    "drizzle-kit": "^0.31.8",
    "jsdom": "^27.4.0",
    "postcss": "^8.5.6",
    "tailwindcss": "^4.1.18",
    "typescript": "^5.9.3",
    "vite-tsconfig-paths": "^6.0.4",
    "vitest": "^4.0.17"
  }
}
"
`;

exports[`template snapshots > web templates > generateWebTsconfig 1`] = `
"{
  "compilerOptions": {
    "target": "ES2017",
    "lib": [
      "dom",
      "dom.iterable",
      "esnext"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "react-jsx",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "paths": {
      "@/*": [
        "./*"
      ],
      "@workspace/ui/*": [
        "../../packages/ui/src/*"
      ]
    }
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts",
    ".next/dev/types/**/*.ts"
  ],
  "exclude": [
    "node_modules"
  ]
}
"
`;

exports[`template snapshots > workflow templates > generateExampleWorkflow 1`] = `
"import { generateText } from "ai";
import { fetch, getWritable, sleep } from "workflow";
import {
	WORKFLOW_PROGRESS_MESSAGES,
	WORKFLOW_STREAM_NAMESPACE,
	WORKFLOW_TOTAL_STEPS,
	type WorkflowProgressEvent,
	type WorkflowStep,
} from "@/lib/workflow-progress/types";

// Step keys in order - used to derive step number from position
const STEP_KEYS = Object.keys(WORKFLOW_PROGRESS_MESSAGES) as WorkflowStep[];

/**
 * Emit a progress event to the workflow stream.
 * This is a step function because getWritable() must be called from within a step.
 */
async function emitProgress(
	step: WorkflowStep | "error",
	type: WorkflowProgressEvent["type"] = "progress",
	data?: WorkflowProgressEvent["data"],
): Promise<void> {
	"use step";

	const writable = getWritable<WorkflowProgressEvent>({
		namespace: WORKFLOW_STREAM_NAMESPACE,
	});
	const writer = writable.getWriter();

	const stepNumber = step === "error" ? 0 : STEP_KEYS.indexOf(step) + 1;
	const message =
		step === "error"
			? "An error occurred"
			: WORKFLOW_PROGRESS_MESSAGES[step];

	await writer.write({
		type,
		step: stepNumber,
		totalSteps: WORKFLOW_TOTAL_STEPS,
		message,
		timestamp: new Date().toISOString(),
		data,
	});

	writer.releaseLock();
}

/**
 * AI Agent Workflow
 *
 * This workflow demonstrates streaming progress events to the frontend
 * while processing an AI request. Each step emits a progress event
 * that the UI can consume in real-time.
 *
 * @param prompt - The user's prompt for the AI agent
 * @returns The AI-generated response
 */
export async function aiAgentWorkflow(prompt: string): Promise<string> {
	"use workflow";

	globalThis.fetch = fetch;

	try {
		// Step 1: Initialize
		await emitProgress("initializing");
		await sleep("500ms");

		// Step 2: Analyze prompt
		await emitProgress("analyzing");
		await sleep("500ms");
		const processedPrompt = await analyzePrompt(prompt);

		// Step 3: Generate AI response
		await emitProgress("generating");
		const response = await generateAIResponse(processedPrompt);

		// Step 4: Process results
		await emitProgress("processing");
		await sleep("500ms");
		const processedResponse = await processResults(response);

		// Step 5: Finalize
		await emitProgress("finalizing", "completed", { result: processedResponse });

		return processedResponse;
	} catch (error) {
		await emitProgress("error", "error", {
			error: error instanceof Error ? error.message : "Unknown error",
		});
		throw error;
	}
}

async function analyzePrompt(prompt: string): Promise<string> {
	"use step";
	// Add any preprocessing logic here
	return \`User request: \${prompt}\`;
}

async function generateAIResponse(prompt: string): Promise<string> {
	"use step";

	const result = await generateText({
		model: "openai/gpt-4o-mini",
		prompt,
		system: "You are a helpful AI assistant. Be concise and helpful.",
	});

	return result.text;
}

async function processResults(response: string): Promise<string> {
	"use step";
	// Add any post-processing logic here
	return response;
}
"
`;

exports[`template snapshots > workflow templates > generateWorkflowProgressRoute (Better Auth) 1`] = `
"import { NextResponse } from "next/server";
import { getRun } from "workflow/api";
import { getSession } from "@/lib/auth";
import { WORKFLOW_STREAM_NAMESPACE } from "@/lib/workflow-progress/types";

/**
 * GET /api/workflow-progress/[runId]
 *
 * Streams workflow progress events as Server-Sent Events (SSE).
 * Supports reconnection via \`startIndex\` query parameter.
 *
 * Query parameters:
 * - startIndex: Event index to start from (for reconnection)
 */
export async function GET(
	request: Request,
	{ params }: { params: Promise<{ runId: string }> },
) {
	try {
		// Authenticate user
		const session = await getSession();
		if (!session?.user) {
			return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
		}

		const { runId } = await params;

		if (!runId) {
			return NextResponse.json(
				{ error: "Missing runId parameter" },
				{ status: 400 },
			);
		}

		// Parse query parameters
		const url = new URL(request.url);
		const startIndex = Number.parseInt(
			url.searchParams.get("startIndex") || "0",
			10,
		);

		// Get the workflow run
		const run = getRun(runId);

		// Get the readable stream with namespace and optional startIndex for reconnection
		const readable = run.getReadable<Record<string, unknown>>({
			namespace: WORKFLOW_STREAM_NAMESPACE,
			startIndex,
		});

		// Transform the stream to SSE format
		const encoder = new TextEncoder();
		let eventIndex = startIndex;

		const transformStream = new TransformStream<
			Record<string, unknown>,
			Uint8Array
		>({
			transform(chunk, controller) {
				// Format as SSE event with index for reconnection tracking
				const sseData = JSON.stringify({
					...chunk,
					_index: eventIndex++,
				});
				controller.enqueue(encoder.encode(\`data: \${sseData}\\n\\n\`));
			},
		});

		const sseStream = readable.pipeThrough(transformStream);

		return new Response(sseStream, {
			headers: {
				"Content-Type": "text/event-stream",
				"Cache-Control": "no-cache, no-transform",
				Connection: "keep-alive",
				"X-Accel-Buffering": "no", // Disable nginx buffering
			},
		});
	} catch (error) {
		console.error("Workflow progress stream error:", error);

		// Check if it's a "run not found" error
		if (
			error instanceof Error &&
			error.message.toLowerCase().includes("not found")
		) {
			return NextResponse.json(
				{ error: "Workflow run not found" },
				{ status: 404 },
			);
		}

		return NextResponse.json(
			{
				error: "Failed to stream workflow progress",
				message: error instanceof Error ? error.message : "Unknown error",
			},
			{ status: 500 },
		);
	}
}
"
`;

exports[`template snapshots > workflow templates > generateWorkflowProgressRoute (WorkOS) 1`] = `
"import { NextResponse } from "next/server";
import { getRun } from "workflow/api";
import { withAuth } from "@workos-inc/authkit-nextjs";
import { WORKFLOW_STREAM_NAMESPACE } from "@/lib/workflow-progress/types";

/**
 * GET /api/workflow-progress/[runId]
 *
 * Streams workflow progress events as Server-Sent Events (SSE).
 * Supports reconnection via \`startIndex\` query parameter.
 *
 * Query parameters:
 * - startIndex: Event index to start from (for reconnection)
 */
export async function GET(
	request: Request,
	{ params }: { params: Promise<{ runId: string }> },
) {
	try {
		// Authenticate user
		const { user } = await withAuth();
		if (!user) {
			return NextResponse.json({ error: "Unauthorized" }, { status: 401 });
		}

		const { runId } = await params;

		if (!runId) {
			return NextResponse.json(
				{ error: "Missing runId parameter" },
				{ status: 400 },
			);
		}

		// Parse query parameters
		const url = new URL(request.url);
		const startIndex = Number.parseInt(
			url.searchParams.get("startIndex") || "0",
			10,
		);

		// Get the workflow run
		const run = getRun(runId);

		// Get the readable stream with namespace and optional startIndex for reconnection
		const readable = run.getReadable<Record<string, unknown>>({
			namespace: WORKFLOW_STREAM_NAMESPACE,
			startIndex,
		});

		// Transform the stream to SSE format
		const encoder = new TextEncoder();
		let eventIndex = startIndex;

		const transformStream = new TransformStream<
			Record<string, unknown>,
			Uint8Array
		>({
			transform(chunk, controller) {
				// Format as SSE event with index for reconnection tracking
				const sseData = JSON.stringify({
					...chunk,
					_index: eventIndex++,
				});
				controller.enqueue(encoder.encode(\`data: \${sseData}\\n\\n\`));
			},
		});

		const sseStream = readable.pipeThrough(transformStream);

		return new Response(sseStream, {
			headers: {
				"Content-Type": "text/event-stream",
				"Cache-Control": "no-cache, no-transform",
				Connection: "keep-alive",
				"X-Accel-Buffering": "no", // Disable nginx buffering
			},
		});
	} catch (error) {
		console.error("Workflow progress stream error:", error);

		// Check if it's a "run not found" error
		if (
			error instanceof Error &&
			error.message.toLowerCase().includes("not found")
		) {
			return NextResponse.json(
				{ error: "Workflow run not found" },
				{ status: 404 },
			);
		}

		return NextResponse.json(
			{
				error: "Failed to stream workflow progress",
				message: error instanceof Error ? error.message : "Unknown error",
			},
			{ status: 500 },
		);
	}
}
"
`;

exports[`template snapshots > workflow templates > generateWorkflowProgressTypes 1`] = `
"/**
 * Types for workflow progress streaming
 *
 * Used to emit and consume real-time progress events from workflows
 * to display status updates in the UI.
 */

/**
 * Progress event emitted by workflow steps
 */
export interface WorkflowProgressEvent {
	type: "progress" | "completed" | "error";
	step: number;
	totalSteps: number;
	message: string;
	timestamp: string;
	data?: {
		result?: string;
		error?: string;
	};
}

/**
 * High-level progress messages for each workflow step.
 * Uses semantic keys for better readability and easier maintenance.
 */
export const WORKFLOW_PROGRESS_MESSAGES = {
	initializing: "Initializing workflow...",
	analyzing: "Analyzing your prompt...",
	generating: "Generating AI response...",
	processing: "Processing results...",
	finalizing: "Finalizing...",
} as const;

export type WorkflowStep = keyof typeof WORKFLOW_PROGRESS_MESSAGES;

/**
 * Total number of steps in the workflow
 */
export const WORKFLOW_TOTAL_STEPS = 5;

/**
 * Namespace used for progress stream
 */
export const WORKFLOW_STREAM_NAMESPACE = "ai-workflow-progress";
"
`;

exports[`template snapshots > workflow templates > generateWorkflowRoute 1`] = `
"import { NextResponse } from "next/server";
import { start } from "workflow/api";
import { aiAgentWorkflow } from "@/workflows/ai-agent";

export async function POST(req: Request) {
	try {
		const { prompt } = await req.json();

		if (!prompt) {
			return NextResponse.json(
				{ error: "Prompt is required" },
				{ status: 400 },
			);
		}

		const run = await start(aiAgentWorkflow, [prompt]);

		return NextResponse.json({
			runId: run.runId,
			status: "started",
		});
	} catch (error) {
		console.error("Workflow error:", error);
		return NextResponse.json(
			{ error: "Failed to start workflow" },
			{ status: 500 },
		);
	}
}
"
`;
